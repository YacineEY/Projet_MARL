{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ec3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_multigrid.envs import MARL_env \n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8780b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for CustomMultiAgentEnv that distinguishes between training and testing phases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, is_testing=False, width=10, height=10, max_steps=100, seed=None,\n",
    "                 agent_positions=None, goal_positions=None, walls=None):\n",
    "        self.is_testing = is_testing\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.max_steps = max_steps\n",
    "        self.seed = seed\n",
    "        self.envs = []  # stores all environments created\n",
    "        self.initial_states = []\n",
    "        \n",
    "        if agent_positions is None or goal_positions is None:\n",
    "            raise ValueError(\"agent_positions and goal_positions must be provided\")\n",
    "        \n",
    "        # Training mode: one env per agent/goal pair\n",
    "        if not is_testing:\n",
    "            for agent_pos, goal_pos in zip(agent_positions, goal_positions):\n",
    "                agents = [{\"start\": agent_pos, \"goal\": goal_pos}]\n",
    "                env = MARL_env.CustomMultiAgentEnv(\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    max_steps=max_steps,\n",
    "                    seed=seed,\n",
    "                    agents=agents\n",
    "                )\n",
    "                \n",
    "                # Add walls if specified\n",
    "                if walls:\n",
    "                    for wall_x, wall_y in walls:\n",
    "                        env.add_wall(wall_x, wall_y)\n",
    "                \n",
    "                self.envs.append(env)\n",
    "                self.initial_states.append(env.reset()[0])  # Get first agent's observation\n",
    "        \n",
    "        # Test mode: one env with all agents and goals\n",
    "        else:\n",
    "            agents = [{\"start\": start, \"goal\": goal} for start, goal in zip(agent_positions, goal_positions)]\n",
    "            env = MARL_env.CustomMultiAgentEnv(\n",
    "                width=width,\n",
    "                height=height,\n",
    "                max_steps=max_steps,\n",
    "                seed=seed,\n",
    "                agents=agents\n",
    "            )\n",
    "            \n",
    "            # Add walls if specified\n",
    "            if walls:\n",
    "                for wall_x, wall_y in walls:\n",
    "                    env.add_wall(wall_x, wall_y)\n",
    "            \n",
    "            self.envs.append(env)\n",
    "            self.initial_states.append(env.reset())\n",
    "    \n",
    "    def reset(self, idx=0):\n",
    "        return self.envs[idx].reset(), self.get_agent_position(idx=idx)\n",
    "    def step(self, actions, idx=0):\n",
    "        \"\"\"\n",
    "        Execute a step in the environment with the given actions for each agent.\n",
    "        \n",
    "        Args:\n",
    "            actions: List of actions, one per agent or single action\n",
    "            idx: Index of the environment to use\n",
    "            \n",
    "        Returns:\n",
    "            observations: List of observations for each agent\n",
    "            rewards: List of rewards for each agent\n",
    "            terminated: If the episode is terminated by reaching goals\n",
    "            truncated: If the episode is truncated by exceeding max steps\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        # Make sure we're using the right environment\n",
    "        env = self.envs[idx]\n",
    "        \n",
    "        # Different handling for training vs testing\n",
    "        if not self.is_testing:\n",
    "            # Training mode - one action for one agent\n",
    "            if not isinstance(actions, list):\n",
    "                obs, rewards, terminated, truncated, info = env.step([actions])\n",
    "                return obs[0], rewards[0], terminated, truncated, info  # Return just the first agent's results\n",
    "            else:\n",
    "                obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "                return obs[0], rewards[0], terminated, truncated, info  # Return just the first agent's results\n",
    "        else:\n",
    "            # Testing mode - list of actions for multiple agents\n",
    "            if not isinstance(actions, list):\n",
    "                # Convert single action to list if needed\n",
    "                actions = [actions]\n",
    "            \n",
    "            # Make sure we have enough actions for all agents\n",
    "            if len(actions) < len(env.agents):\n",
    "                actions = actions + [0] * (len(env.agents) - len(actions))\n",
    "            obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "            # Call the environment's step method\n",
    "            return obs, rewards, terminated, truncated, info\n",
    "\n",
    "    def visualize(self, idx=0, highlight_masks=None, show=True):\n",
    "        \"\"\"\n",
    "        Visualize the environment at the specified index\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the environment to visualize\n",
    "            highlight_masks: Optional mask to highlight specific cells\n",
    "            show: Whether to display the image (True) or just return it (False)\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The rendered grid image\n",
    "        \"\"\"\n",
    "        grid_image = self.envs[idx].visualize(highlight_masks=highlight_masks, show=show)\n",
    "        return grid_image\n",
    "    \n",
    "    def get_action_space(self):\n",
    "        # 1: left, 2: right, 3: up, 4: down, 0: stay\n",
    "        return list(range(5))\n",
    "    \n",
    "    def all_agents_at_goals(self, idx=0):\n",
    "        return self.envs[idx].all_agents_at_goals()\n",
    "    \n",
    "    def run_random_agents(self, max_steps=100, idx=0):\n",
    "        self.envs[idx].run_random_agents(max_steps)\n",
    "    \n",
    "    def get_agent_position(self, idx=0):\n",
    "        \"\"\"Get the current position of the agent.\"\"\"\n",
    "        if self.is_testing:\n",
    "            return tuple(self.envs[0].agents[idx].pos)\n",
    "        else:\n",
    "            return tuple(self.envs[idx].agents[0].pos)\n",
    "    def collision():\n",
    "        return MARL_env.CustomMultiAgentEnv.are_agents_colliding()\n",
    "    def get_agent_goal(self, idx=0):\n",
    "        \"\"\"\n",
    "        Get the goal position of the agent.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the agent whose goal position to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (x, y) coordinates of the agent's goal position\n",
    "        \"\"\"\n",
    "        if self.is_testing:\n",
    "            # In testing mode, get the goal for the specified agent\n",
    "            env = self.envs[0]\n",
    "            return tuple(env.agent_goals[idx])\n",
    "        else:\n",
    "            # In training mode, get the goal for the single agent in the environment\n",
    "            env = self.envs[idx]\n",
    "            return tuple(env.agent_goals[0])\n",
    "    def get_lava_pos(self):\n",
    "        \"\"\"\n",
    "        Get the positions of lava cells in the environment.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of (x, y) coordinates of lava positions\n",
    "        \"\"\"\n",
    "        return self.envs[0].lava_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0355cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQLearningAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_dim, \n",
    "                 action_dim, \n",
    "                 lr=0.1, \n",
    "                 gamma=0.99, \n",
    "                 epsilon=0.1,\n",
    "                 n_planning=10):\n",
    "        \n",
    "        # environment\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_planning = n_planning\n",
    "\n",
    "        # model\n",
    "        self.q_table = dict()\n",
    "        self.model = dict()\n",
    "\n",
    "    def action_space(self):\n",
    "        return [0, 1, 2, 3, 4]  # UP, DOWN, LEFT, RIGHT, STOP\n",
    "\n",
    "    def best_action(self, array):\n",
    "\n",
    "        max_value = np.max(array)\n",
    "        max_indices = np.where(array == max_value)[0]\n",
    "        action = np.random.choice(max_indices)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def select_action(self, state, eval=False):\n",
    "        \n",
    "        state_index = self.state_to_position(state)\n",
    "\n",
    "        if state_index not in self.q_table:\n",
    "            self.q_table[state_index] = np.zeros(self.action_dim)\n",
    "        \n",
    "        if not eval and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "\n",
    "        action = np.argmax(self.q_table[state_index])\n",
    "        # action = self.best_action(self.q_table[state_index])\n",
    "        # if eval:\n",
    "        #     return np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def q_learning(self, state, action, reward, next_state, done): \n",
    "\n",
    "        td_target = reward + self.gamma * (np.max(self.q_table[next_state])) * (1 - done)\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "\n",
    "    def q_planning(self):\n",
    "        for _ in range(self.n_planning):\n",
    "            (state, action), (reward, state_next, done) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(state, action, reward, state_next, done)\n",
    "\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "\n",
    "        state_index = (self.state_to_position(state))\n",
    "        next_state_index = (self.state_to_position(next_state))\n",
    "        \n",
    "        if state_index not in self.q_table:\n",
    "            self.q_table[state_index] = np.zeros(self.action_dim)\n",
    "\n",
    "        if next_state_index not in self.q_table:\n",
    "            self.q_table[next_state_index] = np.zeros(self.action_dim)\n",
    "\n",
    "        self.q_learning(state_index, action, reward, next_state_index, done)\n",
    "        self.model[(state_index, action)] = reward, next_state_index, done \n",
    "        self.q_planning()\n",
    "        \n",
    "    def state_to_position(self, state):\n",
    "        \"\"\"\n",
    "        Convert state to a hashable position representation.\n",
    "        \n",
    "        Args:\n",
    "            state: Can be:\n",
    "                - A tuple (x, y)\n",
    "                - A tuple containing (observation, position)\n",
    "                - A list [x, y]\n",
    "                - A numpy array observation\n",
    "            \n",
    "        Returns:\n",
    "            A hashable tuple representing the agent's position\n",
    "        \"\"\"\n",
    "        if isinstance(state, tuple):\n",
    "            # Check if this is a tuple of (observation, position)\n",
    "            if len(state) == 2 and isinstance(state[1], tuple):\n",
    "                return state[1]  # Return the position part\n",
    "            return state  # It's already a position tuple\n",
    "        elif isinstance(state, list):\n",
    "            return tuple(state)  # Convert list to tuple\n",
    "        elif isinstance(state, np.ndarray):\n",
    "            # If state is a full observation array and we have env reference\n",
    "            if hasattr(self, 'env_wrapper') and hasattr(self, 'agent_idx'):\n",
    "                return self.env_wrapper.get_agent_position(self.agent_idx)\n",
    "            else:\n",
    "                print(\"Cannot extract position from numpy array without env_wrapper reference\")\n",
    "                return (0, 0)  # Default fallback\n",
    "        else:\n",
    "            print(f\"Error: Unexpected state type: {type(state)}\")\n",
    "            return (0, 0)  # Default fallback\n",
    "\n",
    "    def print_tables(self, name):\n",
    "        if name == 'qtable':\n",
    "            table = self.q_table\n",
    "            print(f\"{len(table)} out of {(SHAPE[0])*(SHAPE[1])} grids have been reached.\")\n",
    "            col_names=['UP', 'DOWN', 'LEFT', 'RIGHT', 'STOP']\n",
    "            # col_names=['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        elif name == 'model':\n",
    "            table = self.model\n",
    "            print(f\"{len(self.model)} out of {(SHAPE[0])*(SHAPE[1])*self.action_dim} possibilities have been memorized.\")\n",
    "            col_names=['Reward', 'NextState', 'Done']\n",
    "        else:\n",
    "            print(\"Invalid table name.\")\n",
    "            return\n",
    "\n",
    "        ordered_coords_dict =  OrderedDict(sorted(table.items())).copy()\n",
    "        df = pd.DataFrame.from_dict(ordered_coords_dict, orient='index', columns=col_names)\n",
    "        df = df.round(2)\n",
    "        print(df)\n",
    "\n",
    "        return [len(table), (SHAPE[0]-2)*(SHAPE[1]-2), len(self.model), (SHAPE[0]-2)*(SHAPE[1]-2)*self.action_dim]\n",
    "    \n",
    "    def tables_2(self):\n",
    "        return [len(self.q_table), len(self.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f780a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env_wrapper, agent: DynaQLearningAgent, num_episodes=1000, max_steps=1000, print_process=True, agent_idx=0):\n",
    "    \"\"\"\n",
    "    Entraîne un agent dans un environnement donné (env_wrapper) sur plusieurs épisodes.\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment and get initial state (which now includes position)\n",
    "        state = env_wrapper.reset(agent_idx)  # This now returns (observation, position)\n",
    "        total_reward = 0\n",
    "        \n",
    "        if print_process and episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Agent {agent_idx+1}\")\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            action = agent.select_action(state, eval=False)\n",
    "            next_state, reward, terminated, truncated, info = env_wrapper.step(action, agent_idx)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        if print_process and episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward for Agent {agent_idx+1}: {total_reward}\")\n",
    "    \n",
    "    return all_rewards\n",
    "def smooth(data, weight=0.9):  \n",
    "    \"\"\"\n",
    "    Lisse les données en utilisant un facteur de pondération (smooth).\n",
    "    :param data: La liste des données à lisser.\n",
    "    :param weight: Le facteur de pondération utilisé pour le lissage.\n",
    "    :return: La liste des données lissées.\n",
    "    \"\"\"\n",
    "    last = data[0]\n",
    "    smoothed = list()\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards_for_all_agents(all_rewards, filename='rewards.png'):\n",
    "    \"\"\"\n",
    "    Trace les récompenses pour tous les agents après l'entraînement.\n",
    "    \n",
    "    :param all_rewards: Une liste des récompenses pour chaque agent.\n",
    "    :param filename: Le nom du fichier pour enregistrer le graphique.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for idx, rewards in enumerate(all_rewards):\n",
    "        smoothed_rewards = smooth(rewards)  # Lissage des récompenses\n",
    "        plt.plot(smoothed_rewards, label=f\"Agent {idx+1}\")  # Tracer les récompenses lissées\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Q-learning for Multiple Agents')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "\n",
    "def train_all_agents(env_wrapper, agents, num_episodes=1000, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Entraîne plusieurs agents indépendamment dans un environnement multi-agent.\n",
    "    \n",
    "    :param env_wrapper: Le wrapper de l'environnement avec plusieurs agents.\n",
    "    :param agents: Une liste d'agents à entraîner.\n",
    "    :param num_episodes: Nombre d'épisodes pour l'entraînement de chaque agent.\n",
    "    :param max_steps: Nombre maximal d'étapes par épisode.\n",
    "    :return: Une liste des récompenses pour chaque agent.\n",
    "    \"\"\"\n",
    "    all_agents_rewards = []\n",
    "    \n",
    "    for idx, agent in enumerate(agents):\n",
    "        print(f\"\\nTraining Agent {idx+1}...\")\n",
    "        agent_rewards = train_agent(env_wrapper, agent, num_episodes, max_steps, agent_idx=idx)\n",
    "        all_agents_rewards.append(agent_rewards)\n",
    "    \n",
    "    return all_agents_rewards\n",
    "# Après l'entraînement de tous les agents, vous pouvez afficher leur Q-table comme suit :\n",
    "\n",
    "def print_qtables(agents):\n",
    "    for idx, agent in enumerate(agents):\n",
    "        print(f\"\\nQ-table for Agent {idx+1}:\")\n",
    "        agent.print_tables('qtable')  # Utilise la fonction print_tables pour afficher la Q-table\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e37984",
   "metadata": {},
   "source": [
    "### Movement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ff3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_state(state,action):\n",
    "\n",
    "    pos= state\n",
    "    next_pos_array = np.array(pos)\n",
    "    \n",
    "    # Traitement explicite pour l'action \"stay\"\n",
    "    if action == 0:\n",
    "        next_pos = pos  # Rester sur place\n",
    "    elif action == 1:  # west/left\n",
    "        next_pos_array[0] -= 1\n",
    "        next_pos = tuple(next_pos_array)\n",
    "    elif action == 2:  # east/right\n",
    "        next_pos_array[0] += 1\n",
    "        next_pos = tuple(next_pos_array)\n",
    "    elif action == 3:  # north/up\n",
    "        next_pos_array[1] -= 1\n",
    "        next_pos = tuple(next_pos_array)\n",
    "    elif action == 4:  # south/down\n",
    "        next_pos_array[1] += 1\n",
    "        next_pos = tuple(next_pos_array)\n",
    "    return next_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a1593",
   "metadata": {},
   "source": [
    "### SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2136620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reachable_states(env, agent_idx=0, start_pos=None, n_steps=5):\n",
    "    \"\"\"\n",
    "    Computes all states reachable for an agent in n_steps.\n",
    "    \n",
    "    Args:\n",
    "        env: The MultiAgentEnvWrapper environment\n",
    "        agent_idx: Index of the agent to compute reachable states for\n",
    "        start_pos: Optional starting position, defaults to agent's current position\n",
    "        n_steps: Number of steps to look ahead\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (reachable_by_step, all_reachable)\n",
    "            - reachable_by_step: Dict mapping step number to set of positions {step: {positions}}\n",
    "            - all_reachable: Set of all reachable positions\n",
    "    \"\"\"\n",
    "    start_pos = start_pos or env.get_agent_position(agent_idx)\n",
    "    queue = deque([(start_pos, 0)])  \n",
    "    visited = {start_pos} \n",
    "    reachable = {0: {start_pos}}  \n",
    "\n",
    "    actions = [0, 1, 2, 3, 4]  \n",
    "\n",
    "    while queue:\n",
    "        pos, steps = queue.popleft()\n",
    "        \n",
    "        if steps >= n_steps:\n",
    "            continue\n",
    "            \n",
    "        for action in actions:\n",
    "            next_pos= model_state(pos, action)\n",
    "\n",
    "            if action == 0:\n",
    "                if next_pos not in visited:\n",
    "                    visited.add(next_pos)\n",
    "                    queue.append((next_pos, steps + 1))\n",
    "                    if steps + 1 not in reachable:\n",
    "                        reachable[steps + 1] = set()\n",
    "                    reachable[steps + 1].add(next_pos)\n",
    "                continue\n",
    "            \n",
    "            # Check boundaries\n",
    "            if (0 <= next_pos[0] < env.width and \n",
    "                0 <= next_pos[1] < env.height):\n",
    "                if env.is_testing:\n",
    "                    grid = env.envs[0].grid\n",
    "                else:\n",
    "                    if agent_idx < len(env.envs):\n",
    "                        grid = env.envs[agent_idx].grid\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if next_pos not in visited:\n",
    "                    visited.add(next_pos)\n",
    "                    queue.append((next_pos, steps + 1))\n",
    "\n",
    "                    if steps + 1 not in reachable:\n",
    "                        reachable[steps + 1] = set()\n",
    "                    reachable[steps + 1].add(next_pos)\n",
    "    all_reachable = set()\n",
    "    for step in reachable:\n",
    "        all_reachable.update(reachable[step])\n",
    "    \n",
    "    return reachable, all_reachable\n",
    "\n",
    "def n_unsafe_set(env, agent_idx1=0, agent_idx2=1, n=5):\n",
    "    \"\"\"\n",
    "    Computes the unsafe set for two agents in n steps.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment wrapper\n",
    "        agent_idx1: Index of the first agent\n",
    "        agent_idx2: Index of the second agent\n",
    "        n: Number of steps to look ahead\n",
    "        \n",
    "    Returns:\n",
    "        dict: Unsafe states where both agents could potentially collide, by step\n",
    "    \"\"\"\n",
    "    if env.is_testing:\n",
    "        num_agents = len(env.envs[0].agents) if env.envs else 0\n",
    "    else:\n",
    "        num_agents = len(env.envs)\n",
    "        \n",
    "    if agent_idx1 >= num_agents or agent_idx2 >= num_agents:\n",
    "        \n",
    "        return {}  \n",
    "    \n",
    "    _, all_reachable_agent_1 = compute_reachable_states(env, agent_idx=agent_idx1, n_steps=n)\n",
    "    _, all_reachable_agent_2 = compute_reachable_states(env, agent_idx=agent_idx2, n_steps=n)\n",
    "    \n",
    "    intersections = all_reachable_agent_1.intersection(all_reachable_agent_2)\n",
    "    \n",
    "    unsafe_set = {0: intersections} if intersections else {}\n",
    "    \n",
    "    return unsafe_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def K_agents_n_unsafe_set(env, agent_idx=0, n_steps=4, exclude_indices=None):\n",
    "    \"\"\"\n",
    "    Computes the unsafe set for a specific agent by considering its interaction\n",
    "    with all other agents in the environment. Also counts how many times each position\n",
    "    is shared among different agents.\n",
    "    \n",
    "    Args:1safety_aware_pipeline_with_deadlock\n",
    "        env: The MultiAgentEnvWrapper environment\n",
    "        agent_idx: Index of the agent for which to compute the unsafe set\n",
    "        n_steps: Number of steps to look ahead\n",
    "        exclude_indices: Optional list of agent indices to exclude from the computation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Unsafe set for the specified agent with counts of how many agents share each position\n",
    "    \"\"\"\n",
    "    lava_positions = env.get_lava_pos()\n",
    "    if env.is_testing:\n",
    "        num_agents = len(env.envs[0].agents) if env.envs else 0\n",
    "    else:\n",
    "        num_agents = len(env.envs)\n",
    "    \n",
    "    combined_unsafe_set = {}\n",
    "    position_counts = {}  \n",
    "\n",
    "    if exclude_indices is None:\n",
    "        exclude_indices = []\n",
    "\n",
    "    if agent_idx not in exclude_indices:\n",
    "        exclude_indices.append(agent_idx)\n",
    "\n",
    "    for other_idx in range(num_agents):\n",
    "        if other_idx in exclude_indices:\n",
    "            continue\n",
    "\n",
    "        pairwise_unsafe = n_unsafe_set(env, agent_idx1=agent_idx, agent_idx2=other_idx, n=n_steps)\n",
    "\n",
    "        for step, positions in pairwise_unsafe.items():\n",
    "            if step not in combined_unsafe_set:\n",
    "                combined_unsafe_set[step] = set()\n",
    "                position_counts[step] = {}\n",
    "\n",
    "            combined_unsafe_set[step].update(positions)\n",
    "\n",
    "            for pos in positions:\n",
    "                if pos not in position_counts[step]:\n",
    "                    position_counts[step][pos] = 1\n",
    "                else:\n",
    "                    position_counts[step][pos] += 1\n",
    "    \n",
    "    # Add lava positions to the unsafe set with a high count\n",
    "    for step in range(n_steps + 1):\n",
    "        if step not in combined_unsafe_set:\n",
    "            combined_unsafe_set[step] = set()\n",
    "            position_counts[step] = {}\n",
    "        \n",
    "        for lava_pos in lava_positions:\n",
    "            combined_unsafe_set[step].add(lava_pos)\n",
    "            position_counts[step][lava_pos] = 1\n",
    "    \n",
    "    unsafe_with_counts = {}\n",
    "    for step, positions in combined_unsafe_set.items():\n",
    "        unsafe_with_counts[step] = {pos: position_counts[step][pos] for pos in positions}\n",
    "    \n",
    "    return unsafe_with_counts\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba9c0c",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ec80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_reachable_to_mask(reachable_by_step, env_width, env_height, env, fixed_color=None):\n",
    "    \"\"\"\n",
    "    Convert reachable states to a mask for visualization with better color control.\n",
    "\n",
    "    Args:\n",
    "        reachable_by_step: Dict mapping step number to set of positions {step: {positions}}\n",
    "        env_width: Width of the environment\n",
    "        env_height: Height of the environment\n",
    "        env: Reference to environment wrapper\n",
    "        fixed_color: If provided, use this color index for all positions\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Mask with reachable states\n",
    "    \"\"\"\n",
    "    mask = np.empty((env_width, env_height), dtype=object)\n",
    "    for i in range(env_width):\n",
    "        for j in range(env_height):\n",
    "            mask[i, j] = []\n",
    "\n",
    "    env_world = env.envs[0].world\n",
    "    color_count = len(env_world.IDX_TO_COLOR)\n",
    "\n",
    "    for step, positions in reachable_by_step.items():\n",
    "     \n",
    "        for pos in positions:\n",
    "            x, y = pos\n",
    "            if 0 <= x < env_width and 0 <= y < env_height:\n",
    "                color = fixed_color if fixed_color is not None else (step % color_count)\n",
    "                mask[x, y].append(color)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def visualize_all_agents_unsafe_sets(env_wrapper, n_steps=4):\n",
    "    \"\"\"\n",
    "    Visualizes the unsafe sets for all agents in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env_wrapper: The MultiAgentEnvWrapper environment\n",
    "        n_steps: Number of steps to look ahead for computing unsafe sets\n",
    "    \"\"\"\n",
    "    # Determine the number of agents\n",
    "    if env_wrapper.is_testing:\n",
    "        num_agents = len(env_wrapper.envs[0].agents) if env_wrapper.envs else 0\n",
    "    else:\n",
    "        num_agents = len(env_wrapper.envs)\n",
    "    \n",
    "    env_width = env_wrapper.width\n",
    "    env_height = env_wrapper.height\n",
    "\n",
    "    for agent_idx in range(num_agents):\n",
    "\n",
    "        unsafe_set = K_agents_n_unsafe_set(\n",
    "            env_wrapper, \n",
    "            agent_idx=agent_idx, \n",
    "            n_steps=n_steps\n",
    "        )\n",
    "\n",
    "        unsafe_mask = from_reachable_to_mask(\n",
    "            unsafe_set, \n",
    "            env_width, \n",
    "            env_height, \n",
    "            env_wrapper,\n",
    "            fixed_color=0  \n",
    "        )\n",
    "        \n",
    "        env_wrapper.visualize(highlight_masks=unsafe_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa182179",
   "metadata": {},
   "source": [
    "### Safety Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fbc4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_status(state,action,unsafe_set_dict):\n",
    "    \"\"\"\n",
    "    Check if the next state is safe or unsafe based on the unsafe set.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state (position)\n",
    "        action: Action taken\n",
    "        unsafe_set_dict: Dictionary of unsafe states\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the next state is safe, False if unsafe\n",
    "    \"\"\"\n",
    "    next_state = model_state(state, action)\n",
    "    \n",
    "    for step, positions_with_counts in unsafe_set_dict.items():\n",
    "        if next_state in positions_with_counts:\n",
    "            count= positions_with_counts[next_state]\n",
    "            return False,count\n",
    "    \n",
    "    return True,0\n",
    "def select_safe_action(state,action, unsafe_set_dict=None,flag=False):\n",
    "    \"\"\"\n",
    "    Sélectionne une action sûre pour l'agent.\n",
    "    Si l'action proposée mène à un état dangereux, remplace par l'action \"stay\".\n",
    "    \n",
    "    Args:\n",
    "        agent_idx: Indice de l'agent\n",
    "        env: L'environnement wrapper\n",
    "        action: L'action proposée à évaluer (0=stay, 1=west, 2=east, 3=north, 4=south)\n",
    "        unsafe_set_dict: Dictionnaire des états dangereux\n",
    "        \n",
    "    Returns:\n",
    "        int: Action sûre (0=stay si danger détecté, sinon l'action originale)\n",
    "    \"\"\"\n",
    "    safety,_=safety_status(state, action, unsafe_set_dict)\n",
    "    if not safety:\n",
    "        if flag:\n",
    "            return 0 ,safety\n",
    "        else :\n",
    "            return 0\n",
    "    else:\n",
    "        if flag:\n",
    "            return action, safety\n",
    "        else:\n",
    "            return action  # Original action if safe\n",
    "\n",
    "def n_steps_safety_judger(n_steps, env, agents):\n",
    "    \"\"\"\n",
    "    Évalue si les trajectoires des agents sur n étapes sont sûres.\n",
    "    \n",
    "    Args:\n",
    "        n_steps: Nombre d'étapes à prévoir\n",
    "        env: L'environnement wrapper\n",
    "        agents: Liste des objets agents\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau de booléens indiquant si chaque agent est en sécurité\n",
    "    \"\"\"\n",
    "    judge = np.ones(len(agents), dtype=bool)  # Initialize all agents as safe\n",
    "    \n",
    "    # Get current positions of all agents\n",
    "    current_positions = [env.get_agent_position(idx=i) for i in range(len(agents))]\n",
    "    \n",
    "    # For each agent, simulate actions for n_steps and check if path is unsafe\n",
    "    for agent_idx in range(len(agents)):\n",
    "        # Get unsafe set dictionary for this agent\n",
    "        unsafe_set_dict = K_agents_n_unsafe_set(env, agent_idx=agent_idx, n_steps=n_steps)\n",
    "        \n",
    "        # Start from current position\n",
    "        current_state = current_positions[agent_idx]\n",
    "        \n",
    "        # Simulate agent's path for n steps\n",
    "        for step in range(n_steps):\n",
    "            # Select action according to the agent's policy\n",
    "            action = agents[agent_idx].select_action(current_state, eval=True)\n",
    "            \n",
    "            # Check if this action would lead to an unsafe state\n",
    "            safety, count = safety_status(current_state, action, unsafe_set_dict)\n",
    "            \n",
    "            if not safety:\n",
    "                judge[agent_idx] = False\n",
    "                break  # No need to continue checking this agent\n",
    "                \n",
    "            # Move to next state for next iteration\n",
    "            current_state = model_state(current_state, action)\n",
    "    \n",
    "    return judge\n",
    "\n",
    "# def n_steps_safety_judger_qtable(n_steps,env,agents=None):\n",
    "#     \"\"\"\n",
    "#     Évalue si les trajectoires des agents sur n étapes sont sûres.\n",
    "    \n",
    "#     Args:\n",
    "#         n_steps: Nombre d'étapes à prévoir\n",
    "#         state: L'état actuel de l'agent\n",
    "        \n",
    "#     Returns:\n",
    "#         numpy.ndarray: Tableau de booléens indiquant si chaque agent est en sécurité\n",
    "#     \"\"\"\n",
    "#     judge = np.ones(len(agents), dtype=bool)  # Initialize all agents as safe\n",
    "    \n",
    "#     current_positions = [env.get_agent_position(idx=i) for i in range(len(agents))]\n",
    "    \n",
    "#     for agent_idx in range(len(agents)):\n",
    "#         # Get unsafe set dictionary for this agent\n",
    "#         unsafe_set_dict = K_agents_n_unsafe_set(env, agent_idx=agent_idx, n_steps=n_steps)\n",
    "        \n",
    "#         # Start from current position\n",
    "#         current_state = current_positions[agent_idx]\n",
    "        \n",
    "#         # Simulate agent's path for n steps\n",
    "#         for step in range(n_steps):\n",
    "#            action=np.argmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f357c56",
   "metadata": {},
   "source": [
    "### Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "607636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reward(state,action,unsafe_set_dict, env, agent_idx=0):\n",
    "\n",
    "    \n",
    "    goal= env.get_agent_goal(agent_idx)\n",
    "    lava= env.get_lava_pos()\n",
    "    safety,_= safety_status(state, action, unsafe_set_dict)\n",
    "    next_state = model_state(state, action)\n",
    "    done= False\n",
    "    reward=-1\n",
    "    if next_state == goal:\n",
    "        done = True\n",
    "        reward =0\n",
    "    elif next_state in lava:\n",
    "        reward = -50\n",
    "    elif not safety:\n",
    "        for step, positions_with_counts in unsafe_set_dict.items():\n",
    "            if next_state in positions_with_counts:\n",
    "                count= positions_with_counts[next_state]\n",
    "                reward=-50-0.1*(count-1)\n",
    "    elif action == 0:\n",
    "        reward = -2\n",
    "\n",
    "    return next_state, reward, done\n",
    "\n",
    "def deadlock_model_reward(state,action, unsafe_set_dict,k):\n",
    "    \"\"\"\n",
    "    Modèle de récompense pour la situation de blocage.\n",
    "    \n",
    "    Args:\n",
    "        action: Action prise\n",
    "        unsafe_set_dict: Dictionnaire des états dangereux\n",
    "        env: L'environnement wrapper\n",
    "        agent_idx: Indice de l'agent\n",
    "        \n",
    "    Returns:\n",
    "        float: Récompense associée à l'action\n",
    "    \"\"\"\n",
    "    safety,_ = safety_status(state, action, unsafe_set_dict)\n",
    "    next_state = model_state(state, action)\n",
    "    reward = 0\n",
    "    if not safety:\n",
    "        reward = -k\n",
    "    if safety:\n",
    "        reward = k\n",
    "    print(f\"reward\", reward)\n",
    "\n",
    "    return next_state,reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c2df3",
   "metadata": {},
   "source": [
    "### Local Q-update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd4e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state_key(state):\n",
    "    \"\"\"Convertit un état en une clé de tuple standard\"\"\"\n",
    "    return tuple(int(x) for x in state)\n",
    "\n",
    "\n",
    "\n",
    "def k_Qlearning(k, L, Qstar, agent_idx, env_wrapper, agent, learning_rate=0.1, discount_factor=0.95):\n",
    "    \"\"\"\n",
    "    Performs temporal Q-learning strictly localized to states reachable in k steps,\n",
    "    with penalization for unsafe states.\n",
    "    \n",
    "    Args:\n",
    "        k: Number of planning steps\n",
    "        L: Number of learning iterations\n",
    "        Qstar: Original Q-table to copy and update locally\n",
    "        agent_idx: Index of the agent\n",
    "        env_wrapper: Environment wrapper\n",
    "        agent: Agent whose Q-table is being updated\n",
    "        learning_rate: Learning rate for Q-value updates\n",
    "        discount_factor: Discount factor for future rewards\n",
    "        kp: Optional alternative horizon for model calls\n",
    "        \n",
    "    Returns:\n",
    "        Updated Q-table (only for reachable states)\n",
    "    \"\"\"\n",
    "    # Get states reachable in k steps and unsafe states\n",
    "    _, all_reachable = compute_reachable_states(\n",
    "        env_wrapper, agent_idx, \n",
    "        start_pos=env_wrapper.get_agent_position(idx=agent_idx), \n",
    "        n_steps=k\n",
    "    )\n",
    "    \n",
    "    unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "        env_wrapper, agent_idx=agent_idx, \n",
    "        n_steps=k, \n",
    "        exclude_indices=None\n",
    "    )\n",
    "\n",
    "    Q_temp = {}\n",
    "    for state in all_reachable:\n",
    "        if state not in Qstar:\n",
    "            Q_temp[state] = np.zeros(agent.action_dim)\n",
    "        else:\n",
    "            Q_temp[state] = Qstar[state].copy()\n",
    "    \n",
    "    for _ in range(L):\n",
    "        if not all_reachable:\n",
    "            continue\n",
    "            \n",
    "        state = random.choice(list(all_reachable))\n",
    "        \n",
    "        for action in range(agent.action_dim):\n",
    "            next_state, reward, done = model_reward(state,action,unsafe_set_dict, env_wrapper, agent_idx)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                if next_state in all_reachable:\n",
    "                    next_state_value = np.max(Q_temp[next_state])\n",
    "                else:\n",
    "                    next_state_value = np.max(Qstar[next_state]) if next_state in Qstar else 0\n",
    "                \n",
    "                target = reward + discount_factor * next_state_value\n",
    "            \n",
    "            # Update Q-value\n",
    "            Q_temp[state][action] += learning_rate * (target - Q_temp[state][action])\n",
    "    \n",
    "    Q_result = copy.deepcopy(Qstar)\n",
    "    for state in all_reachable:\n",
    "        Q_result[state] = Q_temp[state]\n",
    "    \n",
    "    return Q_result\n",
    "def temporal_q_learning_optimized(horizon,k, Qt_plus_1, agent_idx, env_wrapper, agent,L,\n",
    "                               learning_rate=0.1, discount_factor=0.95):\n",
    "    \"\"\"\n",
    "    Optimized version that performs a single update per state/action pair\n",
    "    \n",
    "    Args:\n",
    "        k: Number of planning steps\n",
    "        Qt_plus_1: Q-table from the next time step to update\n",
    "        agent_idx: Index of the agent\n",
    "        env_wrapper: Environment wrapper\n",
    "        agent: Agent whose Q-table is being updated\n",
    "        learning_rate: Learning rate for Q-value updates\n",
    "        discount_factor: Discount factor for future rewards\n",
    "        kp: Optional alternative horizon for model calls\n",
    "        \n",
    "    Returns:\n",
    "        Updated Q-table for current time step\n",
    "    \"\"\"\n",
    "    # Get current agent position\n",
    "    current_pos = env_wrapper.get_agent_position(idx=agent_idx)\n",
    "    \n",
    "    # Calculate reachable states and unsafe sets\n",
    "    _, all_reachable = compute_reachable_states(\n",
    "        env_wrapper, agent_idx, start_pos=current_pos, n_steps=horizon\n",
    "    )\n",
    "    \n",
    "    unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "        env_wrapper, agent_idx=agent_idx, n_steps=max(k, 1), exclude_indices=None\n",
    "    )\n",
    "\n",
    "    # Initialize Qt with values from Qt+1 or zeros for new states\n",
    "    Qt={}\n",
    "    for state in all_reachable:\n",
    "        Qt[state] = np.zeros(agent.action_dim)\n",
    "    \n",
    "    # Single pass update for each state/action\n",
    "    for _ in range(L):\n",
    "        for state in all_reachable:\n",
    "            for action in range(agent.action_dim):\n",
    "                next_state, reward, done = model_reward(state,action,unsafe_set_dict, env_wrapper, agent_idx)\n",
    "                \n",
    "                # Calculate target value based on whether episode is done\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_state_value = np.max(Qt_plus_1[next_state]) if next_state in Qt_plus_1 else 0\n",
    "                    target = reward + discount_factor * next_state_value\n",
    "                \n",
    "                Qt[state][action] += learning_rate * (target - Qt[state][action])\n",
    "        \n",
    "    # Create result by copying Qt+1 and updating reachable states\n",
    "    Q_result = copy.deepcopy(Qt_plus_1)\n",
    "    for state in all_reachable:\n",
    "        if not np.array_equal(Qt[state], np.zeros(agent.action_dim)):\n",
    "            Q_result[state] = Qt[state]\n",
    "        \n",
    "    return Q_result\n",
    "\n",
    "def zero_Qlearning(Qt_plus_1, agent_idx, env_wrapper, agent, k ,\n",
    "                               learning_rate=0.1, discount_factor=0.95):\n",
    "    \n",
    "    # Position actuelle de l'agent\n",
    "    current_pos = env_wrapper.get_agent_position(idx=agent_idx)\n",
    "    \n",
    "    # Calculer les états atteignables et ensemble dangereux\n",
    "    reachable_dict, all_reachable = compute_reachable_states(\n",
    "        env_wrapper, agent_idx, start_pos=current_pos, n_steps=0\n",
    "    )\n",
    "\n",
    "    unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "        env_wrapper, agent_idx=agent_idx, n_steps=1, exclude_indices=None\n",
    "    )\n",
    "\n",
    "    # Initialize Qt with values from Qt+1 or zeros for new states\n",
    "    Qt = {\n",
    "        state: Qt_plus_1[state].copy() if state in Qt_plus_1 \n",
    "        else np.zeros(agent.action_dim)\n",
    "        for state in all_reachable\n",
    "    }\n",
    "    \n",
    "    # Single pass update for each state/action\n",
    "    for state in all_reachable:\n",
    "        for action in range(agent.action_dim):\n",
    "            next_state, reward = deadlock_model_reward(state,action, unsafe_set_dict,k)\n",
    "            # Use Qt+1 for the next state's value\n",
    "            next_state_value = np.max(Qt_plus_1[next_state]) if next_state in Qt_plus_1 else 0\n",
    "            target = reward + discount_factor * next_state_value\n",
    "            \n",
    "            Qt[state][action] += learning_rate * (target - Qt[state][action])\n",
    "    \n",
    "    # Create result by copying Qt+1 and updating reachable states\n",
    "    Q_result = copy.deepcopy(Qt_plus_1)\n",
    "    for state in all_reachable:\n",
    "        Q_result[state] = Qt[state]\n",
    "        \n",
    "    return Q_result\n",
    "def create_rapid_penalties(k):\n",
    "    \"\"\"\n",
    "    Creates a rapidly decreasing sequence of penalties from -1 toward 0\n",
    "    \n",
    "    Args:\n",
    "        k (int): Number of penalty values to generate minus 1\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of k+1 rapidly decreasing values\n",
    "    \"\"\"\n",
    "    # Generate exponentially spaced values\n",
    "    penalties = -np.exp(np.linspace(0, -5, k+1))\n",
    "    \n",
    "    # Normalize so first value is exactly -1\n",
    "    penalties = penalties / abs(penalties[0])\n",
    "    \n",
    "    return penalties\n",
    "\n",
    "def temp_Qlearning(k, L, Qstar, agent_idx, env_wrapper, agent):\n",
    "    \"\"\"Version qui conserve les Q-tables intermédiaires pour chaque étape\"\"\"\n",
    " \n",
    "    Q_steps = {}\n",
    "    figs={}\n",
    "    Q_t_k = k_Qlearning(k, L, Qstar, agent_idx, env_wrapper, agent)\n",
    "    \n",
    "\n",
    "    Reachable_dict, all_reachable = compute_reachable_states(\n",
    "        env_wrapper, agent_idx, start_pos=env_wrapper.get_agent_position(idx=agent_idx), n_steps=1\n",
    "    )\n",
    "\n",
    "    Reachable_dict1, all_reachable1 = compute_reachable_states(\n",
    "        env_wrapper, agent_idx, start_pos=env_wrapper.get_agent_position(idx=agent_idx), n_steps=k\n",
    "    )\n",
    "    unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "        env_wrapper, agent_idx=agent_idx, n_steps=k, exclude_indices=None\n",
    "    )\n",
    "    fig = visualize_qtable_actions_with_unsafe_set(\n",
    "        env=env_wrapper,\n",
    "        agent_idx=agent_idx,\n",
    "        qtable=Q_t_k,\n",
    "        reachable_set=all_reachable1,\n",
    "        unsafe_set_dict=unsafe_set_dict,\n",
    "        show=False\n",
    "    )\n",
    "    figs[k]=fig\n",
    "\n",
    "    Q_steps[k] = {state: Q_t_k[state].copy() for state in all_reachable if state in Q_t_k}\n",
    "    \n",
    "    # Calculer les Q-tables pour les étapes de k-1 à 1\n",
    "    for i in range(k-1, 0, -1):\n",
    "        \n",
    "        L_i = max(25, int(L * i / k))  \n",
    "        unsafe_set_dict=K_agents_n_unsafe_set(env_wrapper,agent_idx=agent_idx,n_steps=i,exclude_indices=None)\n",
    "        Q_t_k = temporal_q_learning_optimized(k,i, Q_t_k, agent_idx, env_wrapper, agent, L=L_i)\n",
    "        fig=visualize_qtable_actions_with_unsafe_set(\n",
    "            env=env_wrapper,\n",
    "            agent_idx=agent_idx,\n",
    "            qtable=Q_t_k,\n",
    "            reachable_set=all_reachable1,\n",
    "            unsafe_set_dict=unsafe_set_dict,\n",
    "            show=False\n",
    "        )\n",
    "        figs[i]=fig\n",
    "        Q_steps[i] = {state: Q_t_k[state].copy() for state in all_reachable if state in Q_t_k}\n",
    "\n",
    "    return Q_t_k, Q_steps ,figs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a598e7",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1562e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Qtable(Q_table):\n",
    "    \"\"\"\n",
    "    Normalise la Q-table pour chaque état en divisant par la somme des Q-valeurs.\n",
    "    \n",
    "    Args:\n",
    "        Q_table: Dictionnaire de Q-valeurs {état: [valeurs d'action]}\n",
    "    \n",
    "    Returns:\n",
    "        Dictionnaire normalisé de Q-valeurs\n",
    "    \"\"\"\n",
    "    normalized_Q_table = {}\n",
    "    \n",
    "    for state, action_values in Q_table.items():\n",
    "        sum_values = np.sum(action_values)\n",
    "        if sum_values != 0:\n",
    "            normalized_Q_table[state] = action_values / sum_values\n",
    "        else:\n",
    "            normalized_Q_table[state] = action_values  \n",
    "    \n",
    "    return normalized_Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53010369",
   "metadata": {},
   "source": [
    "### Safety pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9001ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_aware_pipeline(env, agents, k=5, L=1000):\n",
    "\n",
    "    safety_env = MultiAgentEnvWrapper(\n",
    "        is_testing=True,\n",
    "        width=env.width,\n",
    "        height=env.height,\n",
    "        agent_positions=[env.get_agent_position(idx=i) for i in range(len(agents))],\n",
    "        goal_positions=[env.get_agent_goal(idx=i) for i in range(len(agents))],\n",
    "        walls=[] \n",
    "    )\n",
    "    \n",
    "    if safety_env.all_agents_at_goals():\n",
    "        print(\"All agents have reached their goals!\")\n",
    "        return [0] * len(agents), [True] * len(agents), {}\n",
    "\n",
    "    safety_status = n_steps_safety_judger(k, safety_env, agents)\n",
    "    selected_actions = []\n",
    "    Qsteps = {} \n",
    "    for agent_idx, agent in enumerate(agents):\n",
    "        one_step_safety=K_agents_n_unsafe_set(safety_env, agent_idx, n_steps=1, exclude_indices=None)\n",
    "        all_reachable = compute_reachable_states(\n",
    "            safety_env, agent_idx, start_pos=safety_env.get_agent_position(idx=agent_idx), n_steps=1\n",
    "        )[1]\n",
    "        state= safety_env.get_agent_position(agent_idx)\n",
    "        if agent_idx not in Qsteps:\n",
    "            Qsteps[agent_idx] = {}\n",
    "            \n",
    "        qtable=agent.q_table.copy()\n",
    "        updated_q_table, Qstep = temp_Qlearning(k, L, qtable, agent_idx, safety_env, agent)\n",
    "        Qstep[k+1]={state: qtable[state].copy() for state in all_reachable if state in qtable}\n",
    "        agent.q_table = updated_q_table\n",
    "        current_pos = env.get_agent_position(idx=agent_idx)\n",
    "        action = agent.select_action(current_pos, eval=True)\n",
    "        safe_action = select_safe_action(state,action, unsafe_set_dict=one_step_safety)\n",
    "        selected_actions.append(safe_action)\n",
    "        Qsteps[agent_idx] = Qstep \n",
    "    env.step(selected_actions)\n",
    "    \n",
    "    \n",
    "    return selected_actions, safety_status, Qsteps\n",
    "def safety_aware_pipeline_with_safety_check(env, agents, k=5, L=1000):\n",
    "    \"\"\"\n",
    "    Execute safety-aware multi-agent reinforcement learning pipeline with safety checks.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment wrapper\n",
    "        agents: List of agent objects\n",
    "        k: Safety lookahead parameter (number of steps to check ahead)\n",
    "        L: Learning iterations parameter\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (selected_actions, safety_status, Qsteps, figs)\n",
    "    \"\"\"\n",
    "    safety_env = MultiAgentEnvWrapper(\n",
    "        is_testing=True,\n",
    "        width=env.width,\n",
    "        height=env.height,\n",
    "        agent_positions=[env.get_agent_position(idx=i) for i in range(len(agents))],\n",
    "        goal_positions=[env.get_agent_goal(idx=i) for i in range(len(agents))],\n",
    "        walls=[]\n",
    "    )\n",
    "    \n",
    "    if safety_env.all_agents_at_goals():\n",
    "        print(\"All agents have reached their goals!\")\n",
    "        return [0] * len(agents), [True] * len(agents), {}, {}\n",
    "\n",
    "    safety_status = n_steps_safety_judger(k, safety_env, agents)\n",
    "    selected_actions = []\n",
    "    Qsteps = {} \n",
    "    figs = {}  # Master dictionary to store figures\n",
    "    \n",
    "    for agent_idx, agent in enumerate(agents):\n",
    "        one_step_safety = K_agents_n_unsafe_set(safety_env, agent_idx, n_steps=1, exclude_indices=None)\n",
    "        all_reachable = compute_reachable_states(\n",
    "            safety_env, agent_idx, start_pos=safety_env.get_agent_position(idx=agent_idx), n_steps=1\n",
    "        )[1]\n",
    "        state = safety_env.get_agent_position(agent_idx)\n",
    "        \n",
    "        if agent_idx not in Qsteps:\n",
    "            Qsteps[agent_idx] = {}\n",
    "            figs[agent_idx] = {}  # Initialize figure dictionary for this agent\n",
    "\n",
    "        if not safety_status[agent_idx]:\n",
    "            qtable = agent.q_table.copy()\n",
    "            updated_q_table, Qstep, agent_figs = temp_Qlearning(k, L, qtable, agent_idx, safety_env, agent)\n",
    "            figs[agent_idx] = agent_figs  # Store the returned figures dictionary\n",
    "            Qstep[k+1] = {state: qtable[state].copy() for state in all_reachable if state in qtable}\n",
    "            agent.q_table = updated_q_table\n",
    "            current_pos = env.get_agent_position(idx=agent_idx)\n",
    "            action = agent.select_action(current_pos, eval=True)\n",
    "            safe_action = select_safe_action(state, action, unsafe_set_dict=one_step_safety)\n",
    "            selected_actions.append(safe_action)\n",
    "            Qsteps[agent_idx] = Qstep\n",
    "        else:\n",
    "            updated_q_table = agent.q_table.copy()\n",
    "            current_pos = env.get_agent_position(idx=agent_idx)\n",
    "            action = agent.select_action(current_pos, eval=True)\n",
    "            safe_action = select_safe_action(state, action, unsafe_set_dict=one_step_safety)\n",
    "            selected_actions.append(safe_action)\n",
    "    \n",
    "    env.step(selected_actions)\n",
    "    \n",
    "    return selected_actions, safety_status, Qsteps, figs\n",
    "\n",
    "def safety_aware_pipeline_with_deadlock(env, agents, k=5, L=1000):\n",
    "\n",
    "    safety_env = MultiAgentEnvWrapper(\n",
    "        is_testing=True,\n",
    "        width=env.width,\n",
    "        height=env.height,\n",
    "        agent_positions=[env.get_agent_position(idx=i) for i in range(len(agents))],\n",
    "        goal_positions=[env.get_agent_goal(idx=i) for i in range(len(agents))],\n",
    "        walls=[]\n",
    "    )\n",
    "    \n",
    "    if safety_env.all_agents_at_goals():\n",
    "        print(\"All agents have reached their goals!\")\n",
    "        return [0] * len(agents), [True] * len(agents), {}\n",
    "\n",
    "    safety_status = n_steps_safety_judger(k, safety_env, agents)\n",
    "    selected_actions = []\n",
    "    Qsteps = {} \n",
    "\n",
    "    for agent_idx, agent in enumerate(agents):\n",
    "        one_step_safety=K_agents_n_unsafe_set(safety_env, agent_idx, n_steps=1, exclude_indices=None)\n",
    "        all_reachable = compute_reachable_states(\n",
    "            safety_env, agent_idx, start_pos=safety_env.get_agent_position(idx=agent_idx), n_steps=1\n",
    "        )[1]\n",
    "        state= safety_env.get_agent_position(agent_idx)\n",
    "        if agent_idx not in Qsteps:\n",
    "            Qsteps[agent_idx] = {}\n",
    "            \n",
    "        if not safety_status[agent_idx]:\n",
    "            qtable=agent.q_table.copy()\n",
    "            updated_q_table, Qstep = temp_Qlearning(k, L, qtable, agent_idx, safety_env, agent)\n",
    "            Qstep[k+1]={state: qtable[state].copy() for state in all_reachable if state in qtable}\n",
    "            agent.q_table = updated_q_table\n",
    "            current_pos = env.get_agent_position(idx=agent_idx)\n",
    "            action = agent.select_action(current_pos, eval=True)\n",
    "            safe_action = select_safe_action(state,action, unsafe_set_dict=one_step_safety)\n",
    "            if safe_action == 0:\n",
    "                qtable = zero_Qlearning(qtable, agent_idx, safety_env, agent, 100)\n",
    "                agent.q_table = qtable\n",
    "                action = agent.select_action(current_pos, eval=True)\n",
    "                safe_action = select_safe_action(state,action, unsafe_set_dict=one_step_safety)\n",
    "                agent.q_table = updated_q_table\n",
    "            selected_actions.append(safe_action)\n",
    "            Qsteps[agent_idx] = Qstep \n",
    "        else:\n",
    "            updated_q_table = agent.q_table.copy()\n",
    "            current_pos = env.get_agent_position(idx=agent_idx)\n",
    "            action = agent.select_action(current_pos, eval=True)\n",
    "            safe_action =select_safe_action(state,action, unsafe_set_dict=one_step_safety)\n",
    "            selected_actions.append(safe_action)\n",
    "    \n",
    "    \n",
    "    env.step(selected_actions)\n",
    "    \n",
    "    \n",
    "    return selected_actions, safety_status, Qsteps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4537c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_positions(num_agents, width=20, height=20, min_distance=4):\n",
    "    \"\"\"20\n",
    "        height: Height of the grid\n",
    "        min_distance: Minimum distance between any two agents/goals\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (agent_positions, goal_positions)\n",
    "    \"\"\"\n",
    "    agent_positions = []\n",
    "    goal_positions = []\n",
    "    all_positions = []  # Track all positions (both agents and goals)\n",
    "    \n",
    "    # Helper function to check if a position is valid\n",
    "    def is_valid_position(pos, existing_positions):\n",
    "        x, y = pos\n",
    "        \n",
    "        # Check if within bounds (with margin to avoid edge positions)\n",
    "        if x < 1 or x >= width-1 or y < 1 or y >= height-1:\n",
    "            return False\n",
    "        \n",
    "        # Check minimum distance from all existing positions\n",
    "        for ex_pos in existing_positions:\n",
    "            dist = abs(ex_pos[0] - x) + abs(ex_pos[1] - y)  # Manhattan distance\n",
    "            if dist < min_distance:\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    # Generate agent positions\n",
    "    for _ in range(num_agents):\n",
    "        while True:\n",
    "            # Random position within grid bounds\n",
    "            pos = (random.randint(1, width-2), random.randint(1, height-2))\n",
    "            \n",
    "            if is_valid_position(pos, all_positions):\n",
    "                agent_positions.append(pos)\n",
    "                all_positions.append(pos)\n",
    "                break\n",
    "    \n",
    "    # Generate goal positions\n",
    "    for _ in range(num_agents):\n",
    "        while True:\n",
    "            # Random position within grid bounds\n",
    "            pos = (random.randint(1, width-2), random.randint(1, height-2))\n",
    "            \n",
    "            if is_valid_position(pos, all_positions):\n",
    "                goal_positions.append(pos)\n",
    "                all_positions.append(pos)\n",
    "                break\n",
    "    \n",
    "    return agent_positions, goal_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2f87108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Creating and training Agent 1 =====\n",
      "\n",
      "===== Creating and training Agent 2 =====\n",
      "\n",
      "===== Creating and training Agent 3 =====\n",
      "\n",
      "===== Creating and training Agent 4 =====\n",
      "\n",
      "===== Training Agents =====\n",
      "\n",
      "Training Agent 1...\n",
      "Episode 0, Agent 1\n",
      "Episode 0, Total Reward for Agent 1: -1815.0\n",
      "Episode 10, Agent 1\n",
      "Episode 10, Total Reward for Agent 1: -1029.0\n",
      "Episode 20, Agent 1\n",
      "Episode 20, Total Reward for Agent 1: -237.0\n",
      "Episode 30, Agent 1\n",
      "Episode 30, Total Reward for Agent 1: -183.0\n",
      "Episode 40, Agent 1\n",
      "Episode 40, Total Reward for Agent 1: -4.0\n",
      "Episode 50, Agent 1\n",
      "Episode 50, Total Reward for Agent 1: -4.0\n",
      "Episode 60, Agent 1\n",
      "Episode 60, Total Reward for Agent 1: -5.0\n",
      "Episode 70, Agent 1\n",
      "Episode 70, Total Reward for Agent 1: -6.0\n",
      "Episode 80, Agent 1\n",
      "Episode 80, Total Reward for Agent 1: -83.0\n",
      "Episode 90, Agent 1\n",
      "Episode 90, Total Reward for Agent 1: -4.0\n",
      "Episode 100, Agent 1\n",
      "Episode 100, Total Reward for Agent 1: -4.0\n",
      "Episode 110, Agent 1\n",
      "Episode 110, Total Reward for Agent 1: -4.0\n",
      "Episode 120, Agent 1\n",
      "Episode 120, Total Reward for Agent 1: -6.0\n",
      "Episode 130, Agent 1\n",
      "Episode 130, Total Reward for Agent 1: -6.0\n",
      "Episode 140, Agent 1\n",
      "Episode 140, Total Reward for Agent 1: -4.0\n",
      "Episode 150, Agent 1\n",
      "Episode 150, Total Reward for Agent 1: -6.0\n",
      "Episode 160, Agent 1\n",
      "Episode 160, Total Reward for Agent 1: -6.0\n",
      "Episode 170, Agent 1\n",
      "Episode 170, Total Reward for Agent 1: -4.0\n",
      "Episode 180, Agent 1\n",
      "Episode 180, Total Reward for Agent 1: -4.0\n",
      "Episode 190, Agent 1\n",
      "Episode 190, Total Reward for Agent 1: -5.0\n",
      "Episode 200, Agent 1\n",
      "Episode 200, Total Reward for Agent 1: -4.0\n",
      "Episode 210, Agent 1\n",
      "Episode 210, Total Reward for Agent 1: -5.0\n",
      "Episode 220, Agent 1\n",
      "Episode 220, Total Reward for Agent 1: -4.0\n",
      "Episode 230, Agent 1\n",
      "Episode 230, Total Reward for Agent 1: -4.0\n",
      "Episode 240, Agent 1\n",
      "Episode 240, Total Reward for Agent 1: -5.0\n",
      "Episode 250, Agent 1\n",
      "Episode 250, Total Reward for Agent 1: -4.0\n",
      "Episode 260, Agent 1\n",
      "Episode 260, Total Reward for Agent 1: -4.0\n",
      "Episode 270, Agent 1\n",
      "Episode 270, Total Reward for Agent 1: -4.0\n",
      "Episode 280, Agent 1\n",
      "Episode 280, Total Reward for Agent 1: -4.0\n",
      "Episode 290, Agent 1\n",
      "Episode 290, Total Reward for Agent 1: -4.0\n",
      "Episode 300, Agent 1\n",
      "Episode 300, Total Reward for Agent 1: -4.0\n",
      "Episode 310, Agent 1\n",
      "Episode 310, Total Reward for Agent 1: -4.0\n",
      "Episode 320, Agent 1\n",
      "Episode 320, Total Reward for Agent 1: -4.0\n",
      "Episode 330, Agent 1\n",
      "Episode 330, Total Reward for Agent 1: -4.0\n",
      "Episode 340, Agent 1\n",
      "Episode 340, Total Reward for Agent 1: -4.0\n",
      "Episode 350, Agent 1\n",
      "Episode 350, Total Reward for Agent 1: -4.0\n",
      "Episode 360, Agent 1\n",
      "Episode 360, Total Reward for Agent 1: -4.0\n",
      "Episode 370, Agent 1\n",
      "Episode 370, Total Reward for Agent 1: -4.0\n",
      "Episode 380, Agent 1\n",
      "Episode 380, Total Reward for Agent 1: -4.0\n",
      "Episode 390, Agent 1\n",
      "Episode 390, Total Reward for Agent 1: -4.0\n",
      "Episode 400, Agent 1\n",
      "Episode 400, Total Reward for Agent 1: -4.0\n",
      "Episode 410, Agent 1\n",
      "Episode 410, Total Reward for Agent 1: -6.0\n",
      "Episode 420, Agent 1\n",
      "Episode 420, Total Reward for Agent 1: -6.0\n",
      "Episode 430, Agent 1\n",
      "Episode 430, Total Reward for Agent 1: -4.0\n",
      "Episode 440, Agent 1\n",
      "Episode 440, Total Reward for Agent 1: -4.0\n",
      "Episode 450, Agent 1\n",
      "Episode 450, Total Reward for Agent 1: -5.0\n",
      "Episode 460, Agent 1\n",
      "Episode 460, Total Reward for Agent 1: -8.0\n",
      "Episode 470, Agent 1\n",
      "Episode 470, Total Reward for Agent 1: -6.0\n",
      "Episode 480, Agent 1\n",
      "Episode 480, Total Reward for Agent 1: -4.0\n",
      "Episode 490, Agent 1\n",
      "Episode 490, Total Reward for Agent 1: -5.0\n",
      "Episode 500, Agent 1\n",
      "Episode 500, Total Reward for Agent 1: -4.0\n",
      "Episode 510, Agent 1\n",
      "Episode 510, Total Reward for Agent 1: -4.0\n",
      "Episode 520, Agent 1\n",
      "Episode 520, Total Reward for Agent 1: -4.0\n",
      "Episode 530, Agent 1\n",
      "Episode 530, Total Reward for Agent 1: -5.0\n",
      "Episode 540, Agent 1\n",
      "Episode 540, Total Reward for Agent 1: -4.0\n",
      "Episode 550, Agent 1\n",
      "Episode 550, Total Reward for Agent 1: -4.0\n",
      "Episode 560, Agent 1\n",
      "Episode 560, Total Reward for Agent 1: -4.0\n",
      "Episode 570, Agent 1\n",
      "Episode 570, Total Reward for Agent 1: -4.0\n",
      "Episode 580, Agent 1\n",
      "Episode 580, Total Reward for Agent 1: -4.0\n",
      "Episode 590, Agent 1\n",
      "Episode 590, Total Reward for Agent 1: -111.0\n",
      "Episode 600, Agent 1\n",
      "Episode 600, Total Reward for Agent 1: -6.0\n",
      "Episode 610, Agent 1\n",
      "Episode 610, Total Reward for Agent 1: -4.0\n",
      "Episode 620, Agent 1\n",
      "Episode 620, Total Reward for Agent 1: -4.0\n",
      "Episode 630, Agent 1\n",
      "Episode 630, Total Reward for Agent 1: -4.0\n",
      "Episode 640, Agent 1\n",
      "Episode 640, Total Reward for Agent 1: -4.0\n",
      "Episode 650, Agent 1\n",
      "Episode 650, Total Reward for Agent 1: -4.0\n",
      "Episode 660, Agent 1\n",
      "Episode 660, Total Reward for Agent 1: -4.0\n",
      "Episode 670, Agent 1\n",
      "Episode 670, Total Reward for Agent 1: -4.0\n",
      "Episode 680, Agent 1\n",
      "Episode 680, Total Reward for Agent 1: -8.0\n",
      "Episode 690, Agent 1\n",
      "Episode 690, Total Reward for Agent 1: -4.0\n",
      "Episode 700, Agent 1\n",
      "Episode 700, Total Reward for Agent 1: -4.0\n",
      "Episode 710, Agent 1\n",
      "Episode 710, Total Reward for Agent 1: -8.0\n",
      "Episode 720, Agent 1\n",
      "Episode 720, Total Reward for Agent 1: -4.0\n",
      "Episode 730, Agent 1\n",
      "Episode 730, Total Reward for Agent 1: -4.0\n",
      "Episode 740, Agent 1\n",
      "Episode 740, Total Reward for Agent 1: -4.0\n",
      "Episode 750, Agent 1\n",
      "Episode 750, Total Reward for Agent 1: -4.0\n",
      "Episode 760, Agent 1\n",
      "Episode 760, Total Reward for Agent 1: -12.0\n",
      "Episode 770, Agent 1\n",
      "Episode 770, Total Reward for Agent 1: -4.0\n",
      "Episode 780, Agent 1\n",
      "Episode 780, Total Reward for Agent 1: -4.0\n",
      "Episode 790, Agent 1\n",
      "Episode 790, Total Reward for Agent 1: -5.0\n",
      "Episode 800, Agent 1\n",
      "Episode 800, Total Reward for Agent 1: -4.0\n",
      "Episode 810, Agent 1\n",
      "Episode 810, Total Reward for Agent 1: -4.0\n",
      "Episode 820, Agent 1\n",
      "Episode 820, Total Reward for Agent 1: -4.0\n",
      "Episode 830, Agent 1\n",
      "Episode 830, Total Reward for Agent 1: -6.0\n",
      "Episode 840, Agent 1\n",
      "Episode 840, Total Reward for Agent 1: -4.0\n",
      "Episode 850, Agent 1\n",
      "Episode 850, Total Reward for Agent 1: -4.0\n",
      "Episode 860, Agent 1\n",
      "Episode 860, Total Reward for Agent 1: -4.0\n",
      "Episode 870, Agent 1\n",
      "Episode 870, Total Reward for Agent 1: -4.0\n",
      "Episode 880, Agent 1\n",
      "Episode 880, Total Reward for Agent 1: -4.0\n",
      "Episode 890, Agent 1\n",
      "Episode 890, Total Reward for Agent 1: -6.0\n",
      "Episode 900, Agent 1\n",
      "Episode 900, Total Reward for Agent 1: -6.0\n",
      "Episode 910, Agent 1\n",
      "Episode 910, Total Reward for Agent 1: -6.0\n",
      "Episode 920, Agent 1\n",
      "Episode 920, Total Reward for Agent 1: -8.0\n",
      "Episode 930, Agent 1\n",
      "Episode 930, Total Reward for Agent 1: -6.0\n",
      "Episode 940, Agent 1\n",
      "Episode 940, Total Reward for Agent 1: -4.0\n",
      "Episode 950, Agent 1\n",
      "Episode 950, Total Reward for Agent 1: -6.0\n",
      "Episode 960, Agent 1\n",
      "Episode 960, Total Reward for Agent 1: -4.0\n",
      "Episode 970, Agent 1\n",
      "Episode 970, Total Reward for Agent 1: -7.0\n",
      "Episode 980, Agent 1\n",
      "Episode 980, Total Reward for Agent 1: -4.0\n",
      "Episode 990, Agent 1\n",
      "Episode 990, Total Reward for Agent 1: -4.0\n",
      "Episode 1000, Agent 1\n",
      "Episode 1000, Total Reward for Agent 1: -7.0\n",
      "Episode 1010, Agent 1\n",
      "Episode 1010, Total Reward for Agent 1: -4.0\n",
      "Episode 1020, Agent 1\n",
      "Episode 1020, Total Reward for Agent 1: -5.0\n",
      "Episode 1030, Agent 1\n",
      "Episode 1030, Total Reward for Agent 1: -4.0\n",
      "Episode 1040, Agent 1\n",
      "Episode 1040, Total Reward for Agent 1: -5.0\n",
      "Episode 1050, Agent 1\n",
      "Episode 1050, Total Reward for Agent 1: -4.0\n",
      "Episode 1060, Agent 1\n",
      "Episode 1060, Total Reward for Agent 1: -4.0\n",
      "Episode 1070, Agent 1\n",
      "Episode 1070, Total Reward for Agent 1: -4.0\n",
      "Episode 1080, Agent 1\n",
      "Episode 1080, Total Reward for Agent 1: -4.0\n",
      "Episode 1090, Agent 1\n",
      "Episode 1090, Total Reward for Agent 1: -5.0\n",
      "Episode 1100, Agent 1\n",
      "Episode 1100, Total Reward for Agent 1: -4.0\n",
      "Episode 1110, Agent 1\n",
      "Episode 1110, Total Reward for Agent 1: -6.0\n",
      "Episode 1120, Agent 1\n",
      "Episode 1120, Total Reward for Agent 1: -4.0\n",
      "Episode 1130, Agent 1\n",
      "Episode 1130, Total Reward for Agent 1: -4.0\n",
      "Episode 1140, Agent 1\n",
      "Episode 1140, Total Reward for Agent 1: -4.0\n",
      "Episode 1150, Agent 1\n",
      "Episode 1150, Total Reward for Agent 1: -6.0\n",
      "Episode 1160, Agent 1\n",
      "Episode 1160, Total Reward for Agent 1: -4.0\n",
      "Episode 1170, Agent 1\n",
      "Episode 1170, Total Reward for Agent 1: -4.0\n",
      "Episode 1180, Agent 1\n",
      "Episode 1180, Total Reward for Agent 1: -4.0\n",
      "Episode 1190, Agent 1\n",
      "Episode 1190, Total Reward for Agent 1: -4.0\n",
      "Episode 1200, Agent 1\n",
      "Episode 1200, Total Reward for Agent 1: -4.0\n",
      "Episode 1210, Agent 1\n",
      "Episode 1210, Total Reward for Agent 1: -4.0\n",
      "Episode 1220, Agent 1\n",
      "Episode 1220, Total Reward for Agent 1: -4.0\n",
      "Episode 1230, Agent 1\n",
      "Episode 1230, Total Reward for Agent 1: -4.0\n",
      "Episode 1240, Agent 1\n",
      "Episode 1240, Total Reward for Agent 1: -8.0\n",
      "Episode 1250, Agent 1\n",
      "Episode 1250, Total Reward for Agent 1: -7.0\n",
      "Episode 1260, Agent 1\n",
      "Episode 1260, Total Reward for Agent 1: -4.0\n",
      "Episode 1270, Agent 1\n",
      "Episode 1270, Total Reward for Agent 1: -4.0\n",
      "Episode 1280, Agent 1\n",
      "Episode 1280, Total Reward for Agent 1: -4.0\n",
      "Episode 1290, Agent 1\n",
      "Episode 1290, Total Reward for Agent 1: -7.0\n",
      "Episode 1300, Agent 1\n",
      "Episode 1300, Total Reward for Agent 1: -4.0\n",
      "Episode 1310, Agent 1\n",
      "Episode 1310, Total Reward for Agent 1: -4.0\n",
      "Episode 1320, Agent 1\n",
      "Episode 1320, Total Reward for Agent 1: -4.0\n",
      "Episode 1330, Agent 1\n",
      "Episode 1330, Total Reward for Agent 1: -4.0\n",
      "Episode 1340, Agent 1\n",
      "Episode 1340, Total Reward for Agent 1: -6.0\n",
      "Episode 1350, Agent 1\n",
      "Episode 1350, Total Reward for Agent 1: -4.0\n",
      "Episode 1360, Agent 1\n",
      "Episode 1360, Total Reward for Agent 1: -4.0\n",
      "Episode 1370, Agent 1\n",
      "Episode 1370, Total Reward for Agent 1: -6.0\n",
      "Episode 1380, Agent 1\n",
      "Episode 1380, Total Reward for Agent 1: -4.0\n",
      "Episode 1390, Agent 1\n",
      "Episode 1390, Total Reward for Agent 1: -4.0\n",
      "Episode 1400, Agent 1\n",
      "Episode 1400, Total Reward for Agent 1: -6.0\n",
      "Episode 1410, Agent 1\n",
      "Episode 1410, Total Reward for Agent 1: -4.0\n",
      "Episode 1420, Agent 1\n",
      "Episode 1420, Total Reward for Agent 1: -4.0\n",
      "Episode 1430, Agent 1\n",
      "Episode 1430, Total Reward for Agent 1: -5.0\n",
      "Episode 1440, Agent 1\n",
      "Episode 1440, Total Reward for Agent 1: -6.0\n",
      "Episode 1450, Agent 1\n",
      "Episode 1450, Total Reward for Agent 1: -4.0\n",
      "Episode 1460, Agent 1\n",
      "Episode 1460, Total Reward for Agent 1: -4.0\n",
      "Episode 1470, Agent 1\n",
      "Episode 1470, Total Reward for Agent 1: -4.0\n",
      "Episode 1480, Agent 1\n",
      "Episode 1480, Total Reward for Agent 1: -4.0\n",
      "Episode 1490, Agent 1\n",
      "Episode 1490, Total Reward for Agent 1: -4.0\n",
      "\n",
      "Training Agent 2...\n",
      "Episode 0, Agent 2\n",
      "Episode 0, Total Reward for Agent 2: -600.0\n",
      "Episode 10, Agent 2\n",
      "Episode 10, Total Reward for Agent 2: -5.0\n",
      "Episode 20, Agent 2\n",
      "Episode 20, Total Reward for Agent 2: -5.0\n",
      "Episode 30, Agent 2\n",
      "Episode 30, Total Reward for Agent 2: -198.0\n",
      "Episode 40, Agent 2\n",
      "Episode 40, Total Reward for Agent 2: -7.0\n",
      "Episode 50, Agent 2\n",
      "Episode 50, Total Reward for Agent 2: -5.0\n",
      "Episode 60, Agent 2\n",
      "Episode 60, Total Reward for Agent 2: -11.0\n",
      "Episode 70, Agent 2\n",
      "Episode 70, Total Reward for Agent 2: -5.0\n",
      "Episode 80, Agent 2\n",
      "Episode 80, Total Reward for Agent 2: -5.0\n",
      "Episode 90, Agent 2\n",
      "Episode 90, Total Reward for Agent 2: -9.0\n",
      "Episode 100, Agent 2\n",
      "Episode 100, Total Reward for Agent 2: -9.0\n",
      "Episode 110, Agent 2\n",
      "Episode 110, Total Reward for Agent 2: -6.0\n",
      "Episode 120, Agent 2\n",
      "Episode 120, Total Reward for Agent 2: -5.0\n",
      "Episode 130, Agent 2\n",
      "Episode 130, Total Reward for Agent 2: -5.0\n",
      "Episode 140, Agent 2\n",
      "Episode 140, Total Reward for Agent 2: -10.0\n",
      "Episode 150, Agent 2\n",
      "Episode 150, Total Reward for Agent 2: -5.0\n",
      "Episode 160, Agent 2\n",
      "Episode 160, Total Reward for Agent 2: -5.0\n",
      "Episode 170, Agent 2\n",
      "Episode 170, Total Reward for Agent 2: -5.0\n",
      "Episode 180, Agent 2\n",
      "Episode 180, Total Reward for Agent 2: -5.0\n",
      "Episode 190, Agent 2\n",
      "Episode 190, Total Reward for Agent 2: -7.0\n",
      "Episode 200, Agent 2\n",
      "Episode 200, Total Reward for Agent 2: -7.0\n",
      "Episode 210, Agent 2\n",
      "Episode 210, Total Reward for Agent 2: -5.0\n",
      "Episode 220, Agent 2\n",
      "Episode 220, Total Reward for Agent 2: -6.0\n",
      "Episode 230, Agent 2\n",
      "Episode 230, Total Reward for Agent 2: -5.0\n",
      "Episode 240, Agent 2\n",
      "Episode 240, Total Reward for Agent 2: -5.0\n",
      "Episode 250, Agent 2\n",
      "Episode 250, Total Reward for Agent 2: -5.0\n",
      "Episode 260, Agent 2\n",
      "Episode 260, Total Reward for Agent 2: -6.0\n",
      "Episode 270, Agent 2\n",
      "Episode 270, Total Reward for Agent 2: -7.0\n",
      "Episode 280, Agent 2\n",
      "Episode 280, Total Reward for Agent 2: -5.0\n",
      "Episode 290, Agent 2\n",
      "Episode 290, Total Reward for Agent 2: -5.0\n",
      "Episode 300, Agent 2\n",
      "Episode 300, Total Reward for Agent 2: -5.0\n",
      "Episode 310, Agent 2\n",
      "Episode 310, Total Reward for Agent 2: -7.0\n",
      "Episode 320, Agent 2\n",
      "Episode 320, Total Reward for Agent 2: -6.0\n",
      "Episode 330, Agent 2\n",
      "Episode 330, Total Reward for Agent 2: -5.0\n",
      "Episode 340, Agent 2\n",
      "Episode 340, Total Reward for Agent 2: -5.0\n",
      "Episode 350, Agent 2\n",
      "Episode 350, Total Reward for Agent 2: -7.0\n",
      "Episode 360, Agent 2\n",
      "Episode 360, Total Reward for Agent 2: -5.0\n",
      "Episode 370, Agent 2\n",
      "Episode 370, Total Reward for Agent 2: -5.0\n",
      "Episode 380, Agent 2\n",
      "Episode 380, Total Reward for Agent 2: -5.0\n",
      "Episode 390, Agent 2\n",
      "Episode 390, Total Reward for Agent 2: -5.0\n",
      "Episode 400, Agent 2\n",
      "Episode 400, Total Reward for Agent 2: -56.0\n",
      "Episode 410, Agent 2\n",
      "Episode 410, Total Reward for Agent 2: -5.0\n",
      "Episode 420, Agent 2\n",
      "Episode 420, Total Reward for Agent 2: -9.0\n",
      "Episode 430, Agent 2\n",
      "Episode 430, Total Reward for Agent 2: -5.0\n",
      "Episode 440, Agent 2\n",
      "Episode 440, Total Reward for Agent 2: -6.0\n",
      "Episode 450, Agent 2\n",
      "Episode 450, Total Reward for Agent 2: -7.0\n",
      "Episode 460, Agent 2\n",
      "Episode 460, Total Reward for Agent 2: -17.0\n",
      "Episode 470, Agent 2\n",
      "Episode 470, Total Reward for Agent 2: -5.0\n",
      "Episode 480, Agent 2\n",
      "Episode 480, Total Reward for Agent 2: -5.0\n",
      "Episode 490, Agent 2\n",
      "Episode 490, Total Reward for Agent 2: -5.0\n",
      "Episode 500, Agent 2\n",
      "Episode 500, Total Reward for Agent 2: -8.0\n",
      "Episode 510, Agent 2\n",
      "Episode 510, Total Reward for Agent 2: -7.0\n",
      "Episode 520, Agent 2\n",
      "Episode 520, Total Reward for Agent 2: -5.0\n",
      "Episode 530, Agent 2\n",
      "Episode 530, Total Reward for Agent 2: -5.0\n",
      "Episode 540, Agent 2\n",
      "Episode 540, Total Reward for Agent 2: -5.0\n",
      "Episode 550, Agent 2\n",
      "Episode 550, Total Reward for Agent 2: -5.0\n",
      "Episode 560, Agent 2\n",
      "Episode 560, Total Reward for Agent 2: -5.0\n",
      "Episode 570, Agent 2\n",
      "Episode 570, Total Reward for Agent 2: -10.0\n",
      "Episode 580, Agent 2\n",
      "Episode 580, Total Reward for Agent 2: -13.0\n",
      "Episode 590, Agent 2\n",
      "Episode 590, Total Reward for Agent 2: -5.0\n",
      "Episode 600, Agent 2\n",
      "Episode 600, Total Reward for Agent 2: -5.0\n",
      "Episode 610, Agent 2\n",
      "Episode 610, Total Reward for Agent 2: -5.0\n",
      "Episode 620, Agent 2\n",
      "Episode 620, Total Reward for Agent 2: -5.0\n",
      "Episode 630, Agent 2\n",
      "Episode 630, Total Reward for Agent 2: -5.0\n",
      "Episode 640, Agent 2\n",
      "Episode 640, Total Reward for Agent 2: -7.0\n",
      "Episode 650, Agent 2\n",
      "Episode 650, Total Reward for Agent 2: -5.0\n",
      "Episode 660, Agent 2\n",
      "Episode 660, Total Reward for Agent 2: -5.0\n",
      "Episode 670, Agent 2\n",
      "Episode 670, Total Reward for Agent 2: -5.0\n",
      "Episode 680, Agent 2\n",
      "Episode 680, Total Reward for Agent 2: -5.0\n",
      "Episode 690, Agent 2\n",
      "Episode 690, Total Reward for Agent 2: -5.0\n",
      "Episode 700, Agent 2\n",
      "Episode 700, Total Reward for Agent 2: -5.0\n",
      "Episode 710, Agent 2\n",
      "Episode 710, Total Reward for Agent 2: -6.0\n",
      "Episode 720, Agent 2\n",
      "Episode 720, Total Reward for Agent 2: -5.0\n",
      "Episode 730, Agent 2\n",
      "Episode 730, Total Reward for Agent 2: -7.0\n",
      "Episode 740, Agent 2\n",
      "Episode 740, Total Reward for Agent 2: -5.0\n",
      "Episode 750, Agent 2\n",
      "Episode 750, Total Reward for Agent 2: -7.0\n",
      "Episode 760, Agent 2\n",
      "Episode 760, Total Reward for Agent 2: -5.0\n",
      "Episode 770, Agent 2\n",
      "Episode 770, Total Reward for Agent 2: -8.0\n",
      "Episode 780, Agent 2\n",
      "Episode 780, Total Reward for Agent 2: -5.0\n",
      "Episode 790, Agent 2\n",
      "Episode 790, Total Reward for Agent 2: -5.0\n",
      "Episode 800, Agent 2\n",
      "Episode 800, Total Reward for Agent 2: -5.0\n",
      "Episode 810, Agent 2\n",
      "Episode 810, Total Reward for Agent 2: -56.0\n",
      "Episode 820, Agent 2\n",
      "Episode 820, Total Reward for Agent 2: -5.0\n",
      "Episode 830, Agent 2\n",
      "Episode 830, Total Reward for Agent 2: -6.0\n",
      "Episode 840, Agent 2\n",
      "Episode 840, Total Reward for Agent 2: -5.0\n",
      "Episode 850, Agent 2\n",
      "Episode 850, Total Reward for Agent 2: -7.0\n",
      "Episode 860, Agent 2\n",
      "Episode 860, Total Reward for Agent 2: -5.0\n",
      "Episode 870, Agent 2\n",
      "Episode 870, Total Reward for Agent 2: -5.0\n",
      "Episode 880, Agent 2\n",
      "Episode 880, Total Reward for Agent 2: -5.0\n",
      "Episode 890, Agent 2\n",
      "Episode 890, Total Reward for Agent 2: -7.0\n",
      "Episode 900, Agent 2\n",
      "Episode 900, Total Reward for Agent 2: -5.0\n",
      "Episode 910, Agent 2\n",
      "Episode 910, Total Reward for Agent 2: -7.0\n",
      "Episode 920, Agent 2\n",
      "Episode 920, Total Reward for Agent 2: -5.0\n",
      "Episode 930, Agent 2\n",
      "Episode 930, Total Reward for Agent 2: -5.0\n",
      "Episode 940, Agent 2\n",
      "Episode 940, Total Reward for Agent 2: -7.0\n",
      "Episode 950, Agent 2\n",
      "Episode 950, Total Reward for Agent 2: -5.0\n",
      "Episode 960, Agent 2\n",
      "Episode 960, Total Reward for Agent 2: -5.0\n",
      "Episode 970, Agent 2\n",
      "Episode 970, Total Reward for Agent 2: -56.0\n",
      "Episode 980, Agent 2\n",
      "Episode 980, Total Reward for Agent 2: -5.0\n",
      "Episode 990, Agent 2\n",
      "Episode 990, Total Reward for Agent 2: -5.0\n",
      "Episode 1000, Agent 2\n",
      "Episode 1000, Total Reward for Agent 2: -5.0\n",
      "Episode 1010, Agent 2\n",
      "Episode 1010, Total Reward for Agent 2: -7.0\n",
      "Episode 1020, Agent 2\n",
      "Episode 1020, Total Reward for Agent 2: -5.0\n",
      "Episode 1030, Agent 2\n",
      "Episode 1030, Total Reward for Agent 2: -56.0\n",
      "Episode 1040, Agent 2\n",
      "Episode 1040, Total Reward for Agent 2: -7.0\n",
      "Episode 1050, Agent 2\n",
      "Episode 1050, Total Reward for Agent 2: -5.0\n",
      "Episode 1060, Agent 2\n",
      "Episode 1060, Total Reward for Agent 2: -5.0\n",
      "Episode 1070, Agent 2\n",
      "Episode 1070, Total Reward for Agent 2: -57.0\n",
      "Episode 1080, Agent 2\n",
      "Episode 1080, Total Reward for Agent 2: -5.0\n",
      "Episode 1090, Agent 2\n",
      "Episode 1090, Total Reward for Agent 2: -5.0\n",
      "Episode 1100, Agent 2\n",
      "Episode 1100, Total Reward for Agent 2: -5.0\n",
      "Episode 1110, Agent 2\n",
      "Episode 1110, Total Reward for Agent 2: -5.0\n",
      "Episode 1120, Agent 2\n",
      "Episode 1120, Total Reward for Agent 2: -5.0\n",
      "Episode 1130, Agent 2\n",
      "Episode 1130, Total Reward for Agent 2: -58.0\n",
      "Episode 1140, Agent 2\n",
      "Episode 1140, Total Reward for Agent 2: -5.0\n",
      "Episode 1150, Agent 2\n",
      "Episode 1150, Total Reward for Agent 2: -5.0\n",
      "Episode 1160, Agent 2\n",
      "Episode 1160, Total Reward for Agent 2: -5.0\n",
      "Episode 1170, Agent 2\n",
      "Episode 1170, Total Reward for Agent 2: -5.0\n",
      "Episode 1180, Agent 2\n",
      "Episode 1180, Total Reward for Agent 2: -5.0\n",
      "Episode 1190, Agent 2\n",
      "Episode 1190, Total Reward for Agent 2: -5.0\n",
      "Episode 1200, Agent 2\n",
      "Episode 1200, Total Reward for Agent 2: -5.0\n",
      "Episode 1210, Agent 2\n",
      "Episode 1210, Total Reward for Agent 2: -5.0\n",
      "Episode 1220, Agent 2\n",
      "Episode 1220, Total Reward for Agent 2: -5.0\n",
      "Episode 1230, Agent 2\n",
      "Episode 1230, Total Reward for Agent 2: -7.0\n",
      "Episode 1240, Agent 2\n",
      "Episode 1240, Total Reward for Agent 2: -7.0\n",
      "Episode 1250, Agent 2\n",
      "Episode 1250, Total Reward for Agent 2: -8.0\n",
      "Episode 1260, Agent 2\n",
      "Episode 1260, Total Reward for Agent 2: -7.0\n",
      "Episode 1270, Agent 2\n",
      "Episode 1270, Total Reward for Agent 2: -56.0\n",
      "Episode 1280, Agent 2\n",
      "Episode 1280, Total Reward for Agent 2: -58.0\n",
      "Episode 1290, Agent 2\n",
      "Episode 1290, Total Reward for Agent 2: -5.0\n",
      "Episode 1300, Agent 2\n",
      "Episode 1300, Total Reward for Agent 2: -5.0\n",
      "Episode 1310, Agent 2\n",
      "Episode 1310, Total Reward for Agent 2: -5.0\n",
      "Episode 1320, Agent 2\n",
      "Episode 1320, Total Reward for Agent 2: -5.0\n",
      "Episode 1330, Agent 2\n",
      "Episode 1330, Total Reward for Agent 2: -5.0\n",
      "Episode 1340, Agent 2\n",
      "Episode 1340, Total Reward for Agent 2: -5.0\n",
      "Episode 1350, Agent 2\n",
      "Episode 1350, Total Reward for Agent 2: -58.0\n",
      "Episode 1360, Agent 2\n",
      "Episode 1360, Total Reward for Agent 2: -56.0\n",
      "Episode 1370, Agent 2\n",
      "Episode 1370, Total Reward for Agent 2: -5.0\n",
      "Episode 1380, Agent 2\n",
      "Episode 1380, Total Reward for Agent 2: -5.0\n",
      "Episode 1390, Agent 2\n",
      "Episode 1390, Total Reward for Agent 2: -5.0\n",
      "Episode 1400, Agent 2\n",
      "Episode 1400, Total Reward for Agent 2: -6.0\n",
      "Episode 1410, Agent 2\n",
      "Episode 1410, Total Reward for Agent 2: -5.0\n",
      "Episode 1420, Agent 2\n",
      "Episode 1420, Total Reward for Agent 2: -5.0\n",
      "Episode 1430, Agent 2\n",
      "Episode 1430, Total Reward for Agent 2: -5.0\n",
      "Episode 1440, Agent 2\n",
      "Episode 1440, Total Reward for Agent 2: -8.0\n",
      "Episode 1450, Agent 2\n",
      "Episode 1450, Total Reward for Agent 2: -5.0\n",
      "Episode 1460, Agent 2\n",
      "Episode 1460, Total Reward for Agent 2: -5.0\n",
      "Episode 1470, Agent 2\n",
      "Episode 1470, Total Reward for Agent 2: -5.0\n",
      "Episode 1480, Agent 2\n",
      "Episode 1480, Total Reward for Agent 2: -5.0\n",
      "Episode 1490, Agent 2\n",
      "Episode 1490, Total Reward for Agent 2: -5.0\n",
      "\n",
      "Training Agent 3...\n",
      "Episode 0, Agent 3\n",
      "Episode 0, Total Reward for Agent 3: -316.0\n",
      "Episode 10, Agent 3\n",
      "Episode 10, Total Reward for Agent 3: -2.0\n",
      "Episode 20, Agent 3\n",
      "Episode 20, Total Reward for Agent 3: -2.0\n",
      "Episode 30, Agent 3\n",
      "Episode 30, Total Reward for Agent 3: -4.0\n",
      "Episode 40, Agent 3\n",
      "Episode 40, Total Reward for Agent 3: -2.0\n",
      "Episode 50, Agent 3\n",
      "Episode 50, Total Reward for Agent 3: -2.0\n",
      "Episode 60, Agent 3\n",
      "Episode 60, Total Reward for Agent 3: -3.0\n",
      "Episode 70, Agent 3\n",
      "Episode 70, Total Reward for Agent 3: -2.0\n",
      "Episode 80, Agent 3\n",
      "Episode 80, Total Reward for Agent 3: -2.0\n",
      "Episode 90, Agent 3\n",
      "Episode 90, Total Reward for Agent 3: -2.0\n",
      "Episode 100, Agent 3\n",
      "Episode 100, Total Reward for Agent 3: -2.0\n",
      "Episode 110, Agent 3\n",
      "Episode 110, Total Reward for Agent 3: -2.0\n",
      "Episode 120, Agent 3\n",
      "Episode 120, Total Reward for Agent 3: -2.0\n",
      "Episode 130, Agent 3\n",
      "Episode 130, Total Reward for Agent 3: -2.0\n",
      "Episode 140, Agent 3\n",
      "Episode 140, Total Reward for Agent 3: -3.0\n",
      "Episode 150, Agent 3\n",
      "Episode 150, Total Reward for Agent 3: -2.0\n",
      "Episode 160, Agent 3\n",
      "Episode 160, Total Reward for Agent 3: -2.0\n",
      "Episode 170, Agent 3\n",
      "Episode 170, Total Reward for Agent 3: -2.0\n",
      "Episode 180, Agent 3\n",
      "Episode 180, Total Reward for Agent 3: -2.0\n",
      "Episode 190, Agent 3\n",
      "Episode 190, Total Reward for Agent 3: -2.0\n",
      "Episode 200, Agent 3\n",
      "Episode 200, Total Reward for Agent 3: -2.0\n",
      "Episode 210, Agent 3\n",
      "Episode 210, Total Reward for Agent 3: -2.0\n",
      "Episode 220, Agent 3\n",
      "Episode 220, Total Reward for Agent 3: -2.0\n",
      "Episode 230, Agent 3\n",
      "Episode 230, Total Reward for Agent 3: -2.0\n",
      "Episode 240, Agent 3\n",
      "Episode 240, Total Reward for Agent 3: -2.0\n",
      "Episode 250, Agent 3\n",
      "Episode 250, Total Reward for Agent 3: -2.0\n",
      "Episode 260, Agent 3\n",
      "Episode 260, Total Reward for Agent 3: -2.0\n",
      "Episode 270, Agent 3\n",
      "Episode 270, Total Reward for Agent 3: -2.0\n",
      "Episode 280, Agent 3\n",
      "Episode 280, Total Reward for Agent 3: -2.0\n",
      "Episode 290, Agent 3\n",
      "Episode 290, Total Reward for Agent 3: -5.0\n",
      "Episode 300, Agent 3\n",
      "Episode 300, Total Reward for Agent 3: -2.0\n",
      "Episode 310, Agent 3\n",
      "Episode 310, Total Reward for Agent 3: -2.0\n",
      "Episode 320, Agent 3\n",
      "Episode 320, Total Reward for Agent 3: -2.0\n",
      "Episode 330, Agent 3\n",
      "Episode 330, Total Reward for Agent 3: -2.0\n",
      "Episode 340, Agent 3\n",
      "Episode 340, Total Reward for Agent 3: -7.0\n",
      "Episode 350, Agent 3\n",
      "Episode 350, Total Reward for Agent 3: -2.0\n",
      "Episode 360, Agent 3\n",
      "Episode 360, Total Reward for Agent 3: -2.0\n",
      "Episode 370, Agent 3\n",
      "Episode 370, Total Reward for Agent 3: -2.0\n",
      "Episode 380, Agent 3\n",
      "Episode 380, Total Reward for Agent 3: -5.0\n",
      "Episode 390, Agent 3\n",
      "Episode 390, Total Reward for Agent 3: -2.0\n",
      "Episode 400, Agent 3\n",
      "Episode 400, Total Reward for Agent 3: -2.0\n",
      "Episode 410, Agent 3\n",
      "Episode 410, Total Reward for Agent 3: -2.0\n",
      "Episode 420, Agent 3\n",
      "Episode 420, Total Reward for Agent 3: -2.0\n",
      "Episode 430, Agent 3\n",
      "Episode 430, Total Reward for Agent 3: -4.0\n",
      "Episode 440, Agent 3\n",
      "Episode 440, Total Reward for Agent 3: -2.0\n",
      "Episode 450, Agent 3\n",
      "Episode 450, Total Reward for Agent 3: -2.0\n",
      "Episode 460, Agent 3\n",
      "Episode 460, Total Reward for Agent 3: -2.0\n",
      "Episode 470, Agent 3\n",
      "Episode 470, Total Reward for Agent 3: -2.0\n",
      "Episode 480, Agent 3\n",
      "Episode 480, Total Reward for Agent 3: -2.0\n",
      "Episode 490, Agent 3\n",
      "Episode 490, Total Reward for Agent 3: -2.0\n",
      "Episode 500, Agent 3\n",
      "Episode 500, Total Reward for Agent 3: -2.0\n",
      "Episode 510, Agent 3\n",
      "Episode 510, Total Reward for Agent 3: -3.0\n",
      "Episode 520, Agent 3\n",
      "Episode 520, Total Reward for Agent 3: -2.0\n",
      "Episode 530, Agent 3\n",
      "Episode 530, Total Reward for Agent 3: -2.0\n",
      "Episode 540, Agent 3\n",
      "Episode 540, Total Reward for Agent 3: -2.0\n",
      "Episode 550, Agent 3\n",
      "Episode 550, Total Reward for Agent 3: -4.0\n",
      "Episode 560, Agent 3\n",
      "Episode 560, Total Reward for Agent 3: -2.0\n",
      "Episode 570, Agent 3\n",
      "Episode 570, Total Reward for Agent 3: -2.0\n",
      "Episode 580, Agent 3\n",
      "Episode 580, Total Reward for Agent 3: -53.0\n",
      "Episode 590, Agent 3\n",
      "Episode 590, Total Reward for Agent 3: -2.0\n",
      "Episode 600, Agent 3\n",
      "Episode 600, Total Reward for Agent 3: -2.0\n",
      "Episode 610, Agent 3\n",
      "Episode 610, Total Reward for Agent 3: -3.0\n",
      "Episode 620, Agent 3\n",
      "Episode 620, Total Reward for Agent 3: -2.0\n",
      "Episode 630, Agent 3\n",
      "Episode 630, Total Reward for Agent 3: -2.0\n",
      "Episode 640, Agent 3\n",
      "Episode 640, Total Reward for Agent 3: -2.0\n",
      "Episode 650, Agent 3\n",
      "Episode 650, Total Reward for Agent 3: -2.0\n",
      "Episode 660, Agent 3\n",
      "Episode 660, Total Reward for Agent 3: -2.0\n",
      "Episode 670, Agent 3\n",
      "Episode 670, Total Reward for Agent 3: -3.0\n",
      "Episode 680, Agent 3\n",
      "Episode 680, Total Reward for Agent 3: -3.0\n",
      "Episode 690, Agent 3\n",
      "Episode 690, Total Reward for Agent 3: -2.0\n",
      "Episode 700, Agent 3\n",
      "Episode 700, Total Reward for Agent 3: -2.0\n",
      "Episode 710, Agent 3\n",
      "Episode 710, Total Reward for Agent 3: -3.0\n",
      "Episode 720, Agent 3\n",
      "Episode 720, Total Reward for Agent 3: -2.0\n",
      "Episode 730, Agent 3\n",
      "Episode 730, Total Reward for Agent 3: -2.0\n",
      "Episode 740, Agent 3\n",
      "Episode 740, Total Reward for Agent 3: -2.0\n",
      "Episode 750, Agent 3\n",
      "Episode 750, Total Reward for Agent 3: -2.0\n",
      "Episode 760, Agent 3\n",
      "Episode 760, Total Reward for Agent 3: -2.0\n",
      "Episode 770, Agent 3\n",
      "Episode 770, Total Reward for Agent 3: -6.0\n",
      "Episode 780, Agent 3\n",
      "Episode 780, Total Reward for Agent 3: -2.0\n",
      "Episode 790, Agent 3\n",
      "Episode 790, Total Reward for Agent 3: -2.0\n",
      "Episode 800, Agent 3\n",
      "Episode 800, Total Reward for Agent 3: -2.0\n",
      "Episode 810, Agent 3\n",
      "Episode 810, Total Reward for Agent 3: -2.0\n",
      "Episode 820, Agent 3\n",
      "Episode 820, Total Reward for Agent 3: -2.0\n",
      "Episode 830, Agent 3\n",
      "Episode 830, Total Reward for Agent 3: -2.0\n",
      "Episode 840, Agent 3\n",
      "Episode 840, Total Reward for Agent 3: -2.0\n",
      "Episode 850, Agent 3\n",
      "Episode 850, Total Reward for Agent 3: -2.0\n",
      "Episode 860, Agent 3\n",
      "Episode 860, Total Reward for Agent 3: -4.0\n",
      "Episode 870, Agent 3\n",
      "Episode 870, Total Reward for Agent 3: -2.0\n",
      "Episode 880, Agent 3\n",
      "Episode 880, Total Reward for Agent 3: -2.0\n",
      "Episode 890, Agent 3\n",
      "Episode 890, Total Reward for Agent 3: -2.0\n",
      "Episode 900, Agent 3\n",
      "Episode 900, Total Reward for Agent 3: -4.0\n",
      "Episode 910, Agent 3\n",
      "Episode 910, Total Reward for Agent 3: -2.0\n",
      "Episode 920, Agent 3\n",
      "Episode 920, Total Reward for Agent 3: -2.0\n",
      "Episode 930, Agent 3\n",
      "Episode 930, Total Reward for Agent 3: -4.0\n",
      "Episode 940, Agent 3\n",
      "Episode 940, Total Reward for Agent 3: -2.0\n",
      "Episode 950, Agent 3\n",
      "Episode 950, Total Reward for Agent 3: -5.0\n",
      "Episode 960, Agent 3\n",
      "Episode 960, Total Reward for Agent 3: -4.0\n",
      "Episode 970, Agent 3\n",
      "Episode 970, Total Reward for Agent 3: -2.0\n",
      "Episode 980, Agent 3\n",
      "Episode 980, Total Reward for Agent 3: -2.0\n",
      "Episode 990, Agent 3\n",
      "Episode 990, Total Reward for Agent 3: -4.0\n",
      "Episode 1000, Agent 3\n",
      "Episode 1000, Total Reward for Agent 3: -2.0\n",
      "Episode 1010, Agent 3\n",
      "Episode 1010, Total Reward for Agent 3: -2.0\n",
      "Episode 1020, Agent 3\n",
      "Episode 1020, Total Reward for Agent 3: -2.0\n",
      "Episode 1030, Agent 3\n",
      "Episode 1030, Total Reward for Agent 3: -2.0\n",
      "Episode 1040, Agent 3\n",
      "Episode 1040, Total Reward for Agent 3: -2.0\n",
      "Episode 1050, Agent 3\n",
      "Episode 1050, Total Reward for Agent 3: -2.0\n",
      "Episode 1060, Agent 3\n",
      "Episode 1060, Total Reward for Agent 3: -2.0\n",
      "Episode 1070, Agent 3\n",
      "Episode 1070, Total Reward for Agent 3: -2.0\n",
      "Episode 1080, Agent 3\n",
      "Episode 1080, Total Reward for Agent 3: -2.0\n",
      "Episode 1090, Agent 3\n",
      "Episode 1090, Total Reward for Agent 3: -3.0\n",
      "Episode 1100, Agent 3\n",
      "Episode 1100, Total Reward for Agent 3: -2.0\n",
      "Episode 1110, Agent 3\n",
      "Episode 1110, Total Reward for Agent 3: -2.0\n",
      "Episode 1120, Agent 3\n",
      "Episode 1120, Total Reward for Agent 3: -2.0\n",
      "Episode 1130, Agent 3\n",
      "Episode 1130, Total Reward for Agent 3: -2.0\n",
      "Episode 1140, Agent 3\n",
      "Episode 1140, Total Reward for Agent 3: -2.0\n",
      "Episode 1150, Agent 3\n",
      "Episode 1150, Total Reward for Agent 3: -2.0\n",
      "Episode 1160, Agent 3\n",
      "Episode 1160, Total Reward for Agent 3: -2.0\n",
      "Episode 1170, Agent 3\n",
      "Episode 1170, Total Reward for Agent 3: -2.0\n",
      "Episode 1180, Agent 3\n",
      "Episode 1180, Total Reward for Agent 3: -2.0\n",
      "Episode 1190, Agent 3\n",
      "Episode 1190, Total Reward for Agent 3: -2.0\n",
      "Episode 1200, Agent 3\n",
      "Episode 1200, Total Reward for Agent 3: -2.0\n",
      "Episode 1210, Agent 3\n",
      "Episode 1210, Total Reward for Agent 3: -4.0\n",
      "Episode 1220, Agent 3\n",
      "Episode 1220, Total Reward for Agent 3: -2.0\n",
      "Episode 1230, Agent 3\n",
      "Episode 1230, Total Reward for Agent 3: -2.0\n",
      "Episode 1240, Agent 3\n",
      "Episode 1240, Total Reward for Agent 3: -4.0\n",
      "Episode 1250, Agent 3\n",
      "Episode 1250, Total Reward for Agent 3: -5.0\n",
      "Episode 1260, Agent 3\n",
      "Episode 1260, Total Reward for Agent 3: -2.0\n",
      "Episode 1270, Agent 3\n",
      "Episode 1270, Total Reward for Agent 3: -2.0\n",
      "Episode 1280, Agent 3\n",
      "Episode 1280, Total Reward for Agent 3: -3.0\n",
      "Episode 1290, Agent 3\n",
      "Episode 1290, Total Reward for Agent 3: -2.0\n",
      "Episode 1300, Agent 3\n",
      "Episode 1300, Total Reward for Agent 3: -2.0\n",
      "Episode 1310, Agent 3\n",
      "Episode 1310, Total Reward for Agent 3: -2.0\n",
      "Episode 1320, Agent 3\n",
      "Episode 1320, Total Reward for Agent 3: -2.0\n",
      "Episode 1330, Agent 3\n",
      "Episode 1330, Total Reward for Agent 3: -2.0\n",
      "Episode 1340, Agent 3\n",
      "Episode 1340, Total Reward for Agent 3: -2.0\n",
      "Episode 1350, Agent 3\n",
      "Episode 1350, Total Reward for Agent 3: -2.0\n",
      "Episode 1360, Agent 3\n",
      "Episode 1360, Total Reward for Agent 3: -4.0\n",
      "Episode 1370, Agent 3\n",
      "Episode 1370, Total Reward for Agent 3: -4.0\n",
      "Episode 1380, Agent 3\n",
      "Episode 1380, Total Reward for Agent 3: -2.0\n",
      "Episode 1390, Agent 3\n",
      "Episode 1390, Total Reward for Agent 3: -4.0\n",
      "Episode 1400, Agent 3\n",
      "Episode 1400, Total Reward for Agent 3: -4.0\n",
      "Episode 1410, Agent 3\n",
      "Episode 1410, Total Reward for Agent 3: -2.0\n",
      "Episode 1420, Agent 3\n",
      "Episode 1420, Total Reward for Agent 3: -2.0\n",
      "Episode 1430, Agent 3\n",
      "Episode 1430, Total Reward for Agent 3: -2.0\n",
      "Episode 1440, Agent 3\n",
      "Episode 1440, Total Reward for Agent 3: -2.0\n",
      "Episode 1450, Agent 3\n",
      "Episode 1450, Total Reward for Agent 3: -2.0\n",
      "Episode 1460, Agent 3\n",
      "Episode 1460, Total Reward for Agent 3: -2.0\n",
      "Episode 1470, Agent 3\n",
      "Episode 1470, Total Reward for Agent 3: -2.0\n",
      "Episode 1480, Agent 3\n",
      "Episode 1480, Total Reward for Agent 3: -2.0\n",
      "Episode 1490, Agent 3\n",
      "Episode 1490, Total Reward for Agent 3: -2.0\n",
      "\n",
      "Training Agent 4...\n",
      "Episode 0, Agent 4\n",
      "Episode 0, Total Reward for Agent 4: -158.0\n",
      "Episode 10, Agent 4\n",
      "Episode 10, Total Reward for Agent 4: -3.0\n",
      "Episode 20, Agent 4\n",
      "Episode 20, Total Reward for Agent 4: -3.0\n",
      "Episode 30, Agent 4\n",
      "Episode 30, Total Reward for Agent 4: -3.0\n",
      "Episode 40, Agent 4\n",
      "Episode 40, Total Reward for Agent 4: -3.0\n",
      "Episode 50, Agent 4\n",
      "Episode 50, Total Reward for Agent 4: -3.0\n",
      "Episode 60, Agent 4\n",
      "Episode 60, Total Reward for Agent 4: -5.0\n",
      "Episode 70, Agent 4\n",
      "Episode 70, Total Reward for Agent 4: -4.0\n",
      "Episode 80, Agent 4\n",
      "Episode 80, Total Reward for Agent 4: -3.0\n",
      "Episode 90, Agent 4\n",
      "Episode 90, Total Reward for Agent 4: -3.0\n",
      "Episode 100, Agent 4\n",
      "Episode 100, Total Reward for Agent 4: -3.0\n",
      "Episode 110, Agent 4\n",
      "Episode 110, Total Reward for Agent 4: -3.0\n",
      "Episode 120, Agent 4\n",
      "Episode 120, Total Reward for Agent 4: -5.0\n",
      "Episode 130, Agent 4\n",
      "Episode 130, Total Reward for Agent 4: -5.0\n",
      "Episode 140, Agent 4\n",
      "Episode 140, Total Reward for Agent 4: -3.0\n",
      "Episode 150, Agent 4\n",
      "Episode 150, Total Reward for Agent 4: -3.0\n",
      "Episode 160, Agent 4\n",
      "Episode 160, Total Reward for Agent 4: -3.0\n",
      "Episode 170, Agent 4\n",
      "Episode 170, Total Reward for Agent 4: -3.0\n",
      "Episode 180, Agent 4\n",
      "Episode 180, Total Reward for Agent 4: -3.0\n",
      "Episode 190, Agent 4\n",
      "Episode 190, Total Reward for Agent 4: -3.0\n",
      "Episode 200, Agent 4\n",
      "Episode 200, Total Reward for Agent 4: -3.0\n",
      "Episode 210, Agent 4\n",
      "Episode 210, Total Reward for Agent 4: -3.0\n",
      "Episode 220, Agent 4\n",
      "Episode 220, Total Reward for Agent 4: -3.0\n",
      "Episode 230, Agent 4\n",
      "Episode 230, Total Reward for Agent 4: -3.0\n",
      "Episode 240, Agent 4\n",
      "Episode 240, Total Reward for Agent 4: -3.0\n",
      "Episode 250, Agent 4\n",
      "Episode 250, Total Reward for Agent 4: -3.0\n",
      "Episode 260, Agent 4\n",
      "Episode 260, Total Reward for Agent 4: -3.0\n",
      "Episode 270, Agent 4\n",
      "Episode 270, Total Reward for Agent 4: -3.0\n",
      "Episode 280, Agent 4\n",
      "Episode 280, Total Reward for Agent 4: -3.0\n",
      "Episode 290, Agent 4\n",
      "Episode 290, Total Reward for Agent 4: -3.0\n",
      "Episode 300, Agent 4\n",
      "Episode 300, Total Reward for Agent 4: -4.0\n",
      "Episode 310, Agent 4\n",
      "Episode 310, Total Reward for Agent 4: -3.0\n",
      "Episode 320, Agent 4\n",
      "Episode 320, Total Reward for Agent 4: -4.0\n",
      "Episode 330, Agent 4\n",
      "Episode 330, Total Reward for Agent 4: -3.0\n",
      "Episode 340, Agent 4\n",
      "Episode 340, Total Reward for Agent 4: -3.0\n",
      "Episode 350, Agent 4\n",
      "Episode 350, Total Reward for Agent 4: -5.0\n",
      "Episode 360, Agent 4\n",
      "Episode 360, Total Reward for Agent 4: -3.0\n",
      "Episode 370, Agent 4\n",
      "Episode 370, Total Reward for Agent 4: -3.0\n",
      "Episode 380, Agent 4\n",
      "Episode 380, Total Reward for Agent 4: -3.0\n",
      "Episode 390, Agent 4\n",
      "Episode 390, Total Reward for Agent 4: -5.0\n",
      "Episode 400, Agent 4\n",
      "Episode 400, Total Reward for Agent 4: -7.0\n",
      "Episode 410, Agent 4\n",
      "Episode 410, Total Reward for Agent 4: -3.0\n",
      "Episode 420, Agent 4\n",
      "Episode 420, Total Reward for Agent 4: -3.0\n",
      "Episode 430, Agent 4\n",
      "Episode 430, Total Reward for Agent 4: -3.0\n",
      "Episode 440, Agent 4\n",
      "Episode 440, Total Reward for Agent 4: -3.0\n",
      "Episode 450, Agent 4\n",
      "Episode 450, Total Reward for Agent 4: -3.0\n",
      "Episode 460, Agent 4\n",
      "Episode 460, Total Reward for Agent 4: -4.0\n",
      "Episode 470, Agent 4\n",
      "Episode 470, Total Reward for Agent 4: -5.0\n",
      "Episode 480, Agent 4\n",
      "Episode 480, Total Reward for Agent 4: -3.0\n",
      "Episode 490, Agent 4\n",
      "Episode 490, Total Reward for Agent 4: -3.0\n",
      "Episode 500, Agent 4\n",
      "Episode 500, Total Reward for Agent 4: -3.0\n",
      "Episode 510, Agent 4\n",
      "Episode 510, Total Reward for Agent 4: -5.0\n",
      "Episode 520, Agent 4\n",
      "Episode 520, Total Reward for Agent 4: -3.0\n",
      "Episode 530, Agent 4\n",
      "Episode 530, Total Reward for Agent 4: -3.0\n",
      "Episode 540, Agent 4\n",
      "Episode 540, Total Reward for Agent 4: -3.0\n",
      "Episode 550, Agent 4\n",
      "Episode 550, Total Reward for Agent 4: -5.0\n",
      "Episode 560, Agent 4\n",
      "Episode 560, Total Reward for Agent 4: -3.0\n",
      "Episode 570, Agent 4\n",
      "Episode 570, Total Reward for Agent 4: -7.0\n",
      "Episode 580, Agent 4\n",
      "Episode 580, Total Reward for Agent 4: -3.0\n",
      "Episode 590, Agent 4\n",
      "Episode 590, Total Reward for Agent 4: -3.0\n",
      "Episode 600, Agent 4\n",
      "Episode 600, Total Reward for Agent 4: -3.0\n",
      "Episode 610, Agent 4\n",
      "Episode 610, Total Reward for Agent 4: -4.0\n",
      "Episode 620, Agent 4\n",
      "Episode 620, Total Reward for Agent 4: -3.0\n",
      "Episode 630, Agent 4\n",
      "Episode 630, Total Reward for Agent 4: -3.0\n",
      "Episode 640, Agent 4\n",
      "Episode 640, Total Reward for Agent 4: -3.0\n",
      "Episode 650, Agent 4\n",
      "Episode 650, Total Reward for Agent 4: -3.0\n",
      "Episode 660, Agent 4\n",
      "Episode 660, Total Reward for Agent 4: -3.0\n",
      "Episode 670, Agent 4\n",
      "Episode 670, Total Reward for Agent 4: -3.0\n",
      "Episode 680, Agent 4\n",
      "Episode 680, Total Reward for Agent 4: -3.0\n",
      "Episode 690, Agent 4\n",
      "Episode 690, Total Reward for Agent 4: -3.0\n",
      "Episode 700, Agent 4\n",
      "Episode 700, Total Reward for Agent 4: -5.0\n",
      "Episode 710, Agent 4\n",
      "Episode 710, Total Reward for Agent 4: -3.0\n",
      "Episode 720, Agent 4\n",
      "Episode 720, Total Reward for Agent 4: -7.0\n",
      "Episode 730, Agent 4\n",
      "Episode 730, Total Reward for Agent 4: -3.0\n",
      "Episode 740, Agent 4\n",
      "Episode 740, Total Reward for Agent 4: -3.0\n",
      "Episode 750, Agent 4\n",
      "Episode 750, Total Reward for Agent 4: -3.0\n",
      "Episode 760, Agent 4\n",
      "Episode 760, Total Reward for Agent 4: -5.0\n",
      "Episode 770, Agent 4\n",
      "Episode 770, Total Reward for Agent 4: -3.0\n",
      "Episode 780, Agent 4\n",
      "Episode 780, Total Reward for Agent 4: -4.0\n",
      "Episode 790, Agent 4\n",
      "Episode 790, Total Reward for Agent 4: -5.0\n",
      "Episode 800, Agent 4\n",
      "Episode 800, Total Reward for Agent 4: -3.0\n",
      "Episode 810, Agent 4\n",
      "Episode 810, Total Reward for Agent 4: -3.0\n",
      "Episode 820, Agent 4\n",
      "Episode 820, Total Reward for Agent 4: -4.0\n",
      "Episode 830, Agent 4\n",
      "Episode 830, Total Reward for Agent 4: -3.0\n",
      "Episode 840, Agent 4\n",
      "Episode 840, Total Reward for Agent 4: -3.0\n",
      "Episode 850, Agent 4\n",
      "Episode 850, Total Reward for Agent 4: -3.0\n",
      "Episode 860, Agent 4\n",
      "Episode 860, Total Reward for Agent 4: -3.0\n",
      "Episode 870, Agent 4\n",
      "Episode 870, Total Reward for Agent 4: -3.0\n",
      "Episode 880, Agent 4\n",
      "Episode 880, Total Reward for Agent 4: -5.0\n",
      "Episode 890, Agent 4\n",
      "Episode 890, Total Reward for Agent 4: -3.0\n",
      "Episode 900, Agent 4\n",
      "Episode 900, Total Reward for Agent 4: -3.0\n",
      "Episode 910, Agent 4\n",
      "Episode 910, Total Reward for Agent 4: -3.0\n",
      "Episode 920, Agent 4\n",
      "Episode 920, Total Reward for Agent 4: -3.0\n",
      "Episode 930, Agent 4\n",
      "Episode 930, Total Reward for Agent 4: -3.0\n",
      "Episode 940, Agent 4\n",
      "Episode 940, Total Reward for Agent 4: -3.0\n",
      "Episode 950, Agent 4\n",
      "Episode 950, Total Reward for Agent 4: -3.0\n",
      "Episode 960, Agent 4\n",
      "Episode 960, Total Reward for Agent 4: -3.0\n",
      "Episode 970, Agent 4\n",
      "Episode 970, Total Reward for Agent 4: -3.0\n",
      "Episode 980, Agent 4\n",
      "Episode 980, Total Reward for Agent 4: -6.0\n",
      "Episode 990, Agent 4\n",
      "Episode 990, Total Reward for Agent 4: -3.0\n",
      "Episode 1000, Agent 4\n",
      "Episode 1000, Total Reward for Agent 4: -3.0\n",
      "Episode 1010, Agent 4\n",
      "Episode 1010, Total Reward for Agent 4: -3.0\n",
      "Episode 1020, Agent 4\n",
      "Episode 1020, Total Reward for Agent 4: -5.0\n",
      "Episode 1030, Agent 4\n",
      "Episode 1030, Total Reward for Agent 4: -3.0\n",
      "Episode 1040, Agent 4\n",
      "Episode 1040, Total Reward for Agent 4: -3.0\n",
      "Episode 1050, Agent 4\n",
      "Episode 1050, Total Reward for Agent 4: -3.0\n",
      "Episode 1060, Agent 4\n",
      "Episode 1060, Total Reward for Agent 4: -3.0\n",
      "Episode 1070, Agent 4\n",
      "Episode 1070, Total Reward for Agent 4: -5.0\n",
      "Episode 1080, Agent 4\n",
      "Episode 1080, Total Reward for Agent 4: -4.0\n",
      "Episode 1090, Agent 4\n",
      "Episode 1090, Total Reward for Agent 4: -3.0\n",
      "Episode 1100, Agent 4\n",
      "Episode 1100, Total Reward for Agent 4: -3.0\n",
      "Episode 1110, Agent 4\n",
      "Episode 1110, Total Reward for Agent 4: -3.0\n",
      "Episode 1120, Agent 4\n",
      "Episode 1120, Total Reward for Agent 4: -4.0\n",
      "Episode 1130, Agent 4\n",
      "Episode 1130, Total Reward for Agent 4: -3.0\n",
      "Episode 1140, Agent 4\n",
      "Episode 1140, Total Reward for Agent 4: -54.0\n",
      "Episode 1150, Agent 4\n",
      "Episode 1150, Total Reward for Agent 4: -5.0\n",
      "Episode 1160, Agent 4\n",
      "Episode 1160, Total Reward for Agent 4: -3.0\n",
      "Episode 1170, Agent 4\n",
      "Episode 1170, Total Reward for Agent 4: -3.0\n",
      "Episode 1180, Agent 4\n",
      "Episode 1180, Total Reward for Agent 4: -3.0\n",
      "Episode 1190, Agent 4\n",
      "Episode 1190, Total Reward for Agent 4: -3.0\n",
      "Episode 1200, Agent 4\n",
      "Episode 1200, Total Reward for Agent 4: -3.0\n",
      "Episode 1210, Agent 4\n",
      "Episode 1210, Total Reward for Agent 4: -6.0\n",
      "Episode 1220, Agent 4\n",
      "Episode 1220, Total Reward for Agent 4: -3.0\n",
      "Episode 1230, Agent 4\n",
      "Episode 1230, Total Reward for Agent 4: -3.0\n",
      "Episode 1240, Agent 4\n",
      "Episode 1240, Total Reward for Agent 4: -3.0\n",
      "Episode 1250, Agent 4\n",
      "Episode 1250, Total Reward for Agent 4: -3.0\n",
      "Episode 1260, Agent 4\n",
      "Episode 1260, Total Reward for Agent 4: -3.0\n",
      "Episode 1270, Agent 4\n",
      "Episode 1270, Total Reward for Agent 4: -5.0\n",
      "Episode 1280, Agent 4\n",
      "Episode 1280, Total Reward for Agent 4: -3.0\n",
      "Episode 1290, Agent 4\n",
      "Episode 1290, Total Reward for Agent 4: -3.0\n",
      "Episode 1300, Agent 4\n",
      "Episode 1300, Total Reward for Agent 4: -5.0\n",
      "Episode 1310, Agent 4\n",
      "Episode 1310, Total Reward for Agent 4: -3.0\n",
      "Episode 1320, Agent 4\n",
      "Episode 1320, Total Reward for Agent 4: -3.0\n",
      "Episode 1330, Agent 4\n",
      "Episode 1330, Total Reward for Agent 4: -3.0\n",
      "Episode 1340, Agent 4\n",
      "Episode 1340, Total Reward for Agent 4: -54.0\n",
      "Episode 1350, Agent 4\n",
      "Episode 1350, Total Reward for Agent 4: -5.0\n",
      "Episode 1360, Agent 4\n",
      "Episode 1360, Total Reward for Agent 4: -3.0\n",
      "Episode 1370, Agent 4\n",
      "Episode 1370, Total Reward for Agent 4: -3.0\n",
      "Episode 1380, Agent 4\n",
      "Episode 1380, Total Reward for Agent 4: -3.0\n",
      "Episode 1390, Agent 4\n",
      "Episode 1390, Total Reward for Agent 4: -3.0\n",
      "Episode 1400, Agent 4\n",
      "Episode 1400, Total Reward for Agent 4: -3.0\n",
      "Episode 1410, Agent 4\n",
      "Episode 1410, Total Reward for Agent 4: -5.0\n",
      "Episode 1420, Agent 4\n",
      "Episode 1420, Total Reward for Agent 4: -3.0\n",
      "Episode 1430, Agent 4\n",
      "Episode 1430, Total Reward for Agent 4: -3.0\n",
      "Episode 1440, Agent 4\n",
      "Episode 1440, Total Reward for Agent 4: -3.0\n",
      "Episode 1450, Agent 4\n",
      "Episode 1450, Total Reward for Agent 4: -3.0\n",
      "Episode 1460, Agent 4\n",
      "Episode 1460, Total Reward for Agent 4: -3.0\n",
      "Episode 1470, Agent 4\n",
      "Episode 1470, Total Reward for Agent 4: -4.0\n",
      "Episode 1480, Agent 4\n",
      "Episode 1480, Total Reward for Agent 4: -5.0\n",
      "Episode 1490, Agent 4\n",
      "Episode 1490, Total Reward for Agent 4: -3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as rewards.png\n",
      "Q-table for Agent 1: {(3, 4): array([-4.90099501, -5.85198475, -3.940399  , -3.940399  , -5.85198466]), (2, 4): array([-5.85198441, -6.79339184, -4.90099497, -4.90099498, -6.79344356]), (1, 4): array([ -6.79344444, -56.72535824,  -5.85198438,  -5.85198085,\n",
      "        -7.72536036]), (0, 4): array([-56.72530394, -56.72546093,  -6.79343608, -55.79342078,\n",
      "       -57.64754617]), (0, 5): array([-57.64780924, -57.64743309,  -7.72542518, -56.72534779,\n",
      "       -58.56031906]), (1, 5): array([ -7.72519364, -57.64795426,  -6.79345264,  -6.79343215,\n",
      "        -8.64799951]), (4, 4): array([-3.940399  , -4.90099501, -2.9701    , -2.9701    , -4.90099496]), (3, 3): array([-3.940399  , -4.90099488, -2.9701    , -2.9701    , -4.90099501]), (2, 3): array([-4.90099496, -5.85198063, -3.940399  , -3.940399  , -5.85198265]), (4, 3): array([-2.97009999, -3.940399  , -1.99      , -1.99      , -3.940399  ]), (3, 2): array([-2.9701    , -3.94039899, -1.99      , -1.99      , -3.940399  ]), (2, 2): array([-3.94039892, -4.90099339, -2.9701    , -2.9701    , -4.9009949 ]), (1, 2): array([ -4.90099307, -54.85197752,  -3.94039898,  -3.94039834,\n",
      "        -5.85197911]), (0, 2): array([-54.8519804 , -54.85197834,  -4.90099369, -53.90099322,\n",
      "       -55.79341308]), (4, 2): array([-1.99      , -2.9701    , -1.        , -1.        , -2.97009999]), (3, 1): array([ -1.99  ,  -2.9701,  -1.    , -51.9701,  -2.9701]), (2, 1): array([ -2.9701    ,  -3.94039866,  -1.99      , -52.94039899,\n",
      "        -3.94039897]), (1, 1): array([ -3.94039829, -53.90099363,  -2.97009996, -53.90099257,\n",
      "        -4.90099279]), (0, 1): array([ -53.90099343,  -53.90099391,   -3.94039872, -103.36197161,\n",
      "        -54.8519813 ]), (4, 1): array([ -1.        ,  -1.99      ,   0.        , -50.98999991,\n",
      "        -1.98999999]), (3, 0): array([-51.97009999, -52.94039898, -50.98999981, -51.9701    ,\n",
      "        -1.99      ]), (2, 0): array([-52.94039898, -53.9009927 , -51.97009999, -52.94039895,\n",
      "        -2.9701    ]), (1, 0): array([ -53.90099395, -103.36197165,  -52.94039883,  -53.90099157,\n",
      "         -3.94039856]), (0, 0): array([-103.36196667, -103.36197796,  -53.90099242, -103.36197852,\n",
      "        -53.90099402]), (4, 0): array([-50.98999996, -51.9701    , -49.99999991, -50.98999135,\n",
      "        -1.        ]), (1, 3): array([ -5.85197538, -55.79342883,  -4.9009949 ,  -4.90099383,\n",
      "        -6.79342891]), (0, 3): array([-55.79339156, -55.79339791,  -5.85197029, -54.85198051,\n",
      "       -56.72537682]), (2, 5): array([-6.79345703, -7.72537482, -5.85198481, -5.85198358, -7.72543737]), (3, 5): array([-5.85198418, -6.79343718, -4.90099499, -4.90099499, -6.79344786]), (4, 5): array([-4.90099477, -5.85198445, -3.940399  , -3.940399  , -5.85198437]), (5, 5): array([-3.94039894, -4.90099494, -4.90099021, -2.9701    , -4.90099453]), (3, 6): array([-6.79345195, -7.72536948, -5.85198388, -5.85198405, -7.72537625]), (2, 6): array([-7.72546592, -8.64740855, -6.79345013, -6.79345002, -8.64699878]), (1, 6): array([ -8.64775785, -58.55967874,  -7.72545862,  -7.72539492,\n",
      "        -9.55868817]), (0, 6): array([-58.55973641, -58.55913972,  -8.64788002, -57.64779918,\n",
      "       -59.45181898]), (4, 6): array([-5.85198068, -6.79345935, -4.90099484, -4.90099498, -6.79345104]), (3, 7): array([-7.72539269, -8.64740222, -6.79345692, -6.79344668, -8.64765488]), (2, 7): array([-8.64613524, -9.55864627, -7.72522625, -7.72546321, -9.55964242]), (2, 8): array([ -9.559936  , -10.45614122,  -8.64791341,  -8.64713862,\n",
      "       -59.45186039]), (1, 8): array([-10.45937924, -60.33142119,  -9.55975721,  -9.5575545 ,\n",
      "       -60.33941125]), (0, 8): array([-60.34330263, -60.34415758, -10.45697288, -59.4585528 ,\n",
      "       -50.        ]), (3, 8): array([ -8.64785935,  -9.55735345,  -7.72549608,  -7.72538355,\n",
      "       -49.99999997]), (1, 7): array([ -9.55932331, -59.45752413,  -8.64760369,  -8.64790205,\n",
      "       -10.45701512]), (0, 7): array([-59.45475861, -59.45762975,  -9.55890546, -58.55845339,\n",
      "       -60.34540133]), (4, 7): array([-6.79345894, -7.72544207, -5.85198386, -5.85198387, -7.7254986 ]), (4, 8): array([ -7.72537915,  -8.64673114,  -6.79346068,  -6.7934592 ,\n",
      "       -49.99999999]), (5, 7): array([-5.85198395, -6.79346057, -6.79344578, -4.90099492, -6.79345554]), (5, 6): array([-4.90099481, -5.85198413, -5.8519697 , -3.940399  , -5.85198293]), (6, 7): array([-6.79340442, -5.85198434, -7.72490976, -5.85197529, -7.72543229]), (6, 6): array([-5.85196993, -4.90099479, -6.79333945, -4.90099089, -6.79342669]), (6, 5): array([-4.90099099, -3.94039899, -5.85194267, -3.94039806, -5.85197322]), (7, 5): array([-5.85195436, -4.9009911 , -6.79311068, -4.90098184, -6.79336261]), (5, 8): array([ -6.79345654,  -7.72546327,  -7.72534306,  -5.85198414,\n",
      "       -56.7254964 ]), (6, 8): array([ -7.72532634,  -6.79344764,  -8.64691165,  -6.79342914,\n",
      "       -50.        ]), (5, 9): array([-56.72545349, -50.        , -50.        ,  -6.79345124,\n",
      "       -56.7254449 ]), (4, 9): array([-49.99999998, -50.        , -56.72534145,  -7.72549137,\n",
      "         0.        ]), (3, 9): array([-50.        , -59.44795897, -49.99999996,  -8.64791331,\n",
      "         0.        ]), (2, 9): array([-59.45702942, -60.3461599 , -49.99999999,  -9.55688916,\n",
      "       -59.4552336 ]), (1, 9): array([-60.33410066, -49.99999997, -59.45868256, -10.45847353,\n",
      "       -60.33755003]), (0, 9): array([-50.        ,   0.        , -60.34698878, -60.34052524,\n",
      "         0.        ]), (6, 9): array([-49.99999999, -56.72546113, -58.55902657,  -7.7253795 ,\n",
      "         0.        ]), (7, 8): array([ -8.64668747,  -7.72536249,  -9.55274822,  -7.72506212,\n",
      "       -58.55904005]), (8, 8): array([ -9.55282914,  -8.64707075, -50.        ,  -8.64463709,\n",
      "       -59.45124983]), (7, 7): array([-7.72492207, -6.79343942, -8.64458062, -6.79328256, -8.64637649]), (8, 7): array([ -8.64357689,  -7.72491683, -49.99999998,  -7.72352135,\n",
      "        -9.55433035]), (7, 6): array([-6.7933161 , -5.85197572, -7.72421257, -5.85194421, -7.72522784]), (8, 6): array([ -7.72419324,  -6.79335002, -49.99999999,  -6.79297464,\n",
      "        -8.6445672 ]), (8, 5): array([ -6.7929722 ,  -5.85193518, -49.99999999,  -5.85188705,\n",
      "        -7.72389313]), (7, 4): array([-4.90098823, -3.94039887, -5.85185956, -3.94039696, -5.85193485]), (6, 4): array([-3.94039872, -2.9701    , -4.90098488, -2.97009998, -4.90099023]), (5, 4): array([-2.9701    , -3.94039899, -3.94039893, -1.99      , -3.94039899]), (8, 4): array([ -5.85179207,  -4.90099014, -49.99999999,  -4.90097708,\n",
      "        -6.79297612]), (7, 3): array([-3.94039669, -2.97009999, -4.90098583, -2.97009964, -4.90098392]), (6, 3): array([-2.97009982, -1.99      , -3.94039672, -1.99      , -3.94039887]), (5, 3): array([-1.99      , -2.9701    , -2.97009998, -1.        , -2.9701    ]), (8, 3): array([ -4.90096188,  -3.94039747, -54.85191807,  -3.94039745,\n",
      "        -5.85189263]), (7, 2): array([-2.97009974, -1.98999997, -3.94039752, -1.98999995, -3.94039746]), (5, 2): array([-1.  , -1.99, -1.99,  0.  , -1.99]), (5, 1): array([0., 0., 0., 0., 0.]), (6, 2): array([-1.99      , -1.        , -2.97009985, -1.        , -2.97009999]), (6, 1): array([-1.        ,  0.        ,  0.        ,  0.        , -1.42796522]), (7, 9): array([-58.55955665, -49.99999999, -59.45010307,  -8.64699409,\n",
      "       -58.55896601]), (8, 9): array([-59.45180227, -58.55868886, -50.        ,  -9.55218548,\n",
      "       -49.99999999]), (9, 8): array([-50.        ,  -9.55152092,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 7): array([-50.        ,  -8.64402044,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 6): array([-50.        ,  -7.72390098,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 5): array([-50.        ,  -6.79268397, -49.99999999, -50.        ,\n",
      "         0.        ]), (9, 4): array([-49.99999999,  -5.85180802, -49.99999999,   0.        ,\n",
      "       -49.99999999]), (9, 3): array([-54.85186243,  -4.90098495, -54.85193253, -49.99999993,\n",
      "       -49.99999983]), (8, 2): array([ -3.9403983 ,  -2.97009983, -50.        ,  -2.97009988,\n",
      "        -4.90098432]), (9, 2): array([-50.        ,  -3.94039728, -49.99999989,   0.        ,\n",
      "       -54.85192368]), (8, 1): array([ -2.97009994,  -1.99      , -49.99999976, -52.94039868,\n",
      "        -3.9403979 ]), (7, 1): array([ -1.99      ,  -1.        ,  -2.97009984, -51.97009938,\n",
      "        -2.97009961]), (9, 9): array([-49.99999998, -59.45077001,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 1): array([-49.99999998,  -2.97009996,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 0): array([-52.94039881, -51.97009993, -50.        , -52.94039858,\n",
      "        -2.97009996]), (7, 0): array([-51.97009957, -49.99999999, -52.94039863, -51.97009968,\n",
      "        -1.98999999]), (6, 0): array([-49.99999998, -49.9999999 , -51.9700993 ,   0.        ,\n",
      "         0.        ]), (5, 0): array([-49.99999996, -50.9899999 , -49.99999998,   0.        ,\n",
      "         0.        ]), (9, 0): array([-49.99999992, -52.94039305,   0.        ,   0.        ,\n",
      "         0.        ])}\n",
      "Q-table for Agent 2: {(4, 5): array([-5.85198506, -4.90099501, -6.79346473, -4.90099501, -6.79346517]), (3, 5): array([-4.90099501, -3.940399  , -5.85198506, -3.940399  , -5.85198505]), (2, 5): array([-3.940399  , -2.9701    , -4.90099501, -2.9701    , -4.90099501]), (1, 5): array([ -2.9701  , -52.940399,  -3.940399,  -1.99    ,  -3.940399]), (0, 5): array([-52.940399  , -52.94039899,  -2.9701    , -51.9700999 ,\n",
      "       -53.900995  ]), (5, 5): array([-6.79346515, -5.85198506, -7.725523  , -5.85198506, -7.72552706]), (4, 4): array([-4.90099501, -3.940399  , -5.85198505, -3.940399  , -5.85198506]), (3, 4): array([-3.940399  , -2.9701    , -4.90099501, -2.9701    , -4.90099501]), (2, 4): array([-2.9701  , -1.99    , -3.940399, -1.99    , -3.940399]), (1, 4): array([ -1.99      , -51.97009999,  -2.9701    ,  -1.        ,\n",
      "        -2.9701    ]), (0, 4): array([-51.97009978, -51.97009954,  -1.99      , -50.98999895,\n",
      "       -52.940399  ]), (5, 4): array([-5.85198504, -4.90099501, -6.79346292, -4.90099501, -6.79346512]), (4, 3): array([-3.940399  , -2.9701    , -4.900995  , -2.9701    , -4.90099501]), (3, 3): array([-2.9701  , -1.99    , -3.940399, -1.99    , -3.940399]), (2, 3): array([-1.99  , -1.    , -2.9701, -1.    , -2.9701]), (1, 3): array([ -1.  , -50.99,  -1.99,   0.  ,  -1.99]), (0, 3): array([-50.99      , -50.99      ,  -1.        , -50.        ,\n",
      "       -51.97009999]), (5, 3): array([-4.90099501, -3.940399  , -5.85198483, -3.940399  , -5.85198503]), (4, 2): array([-2.9701    , -1.99      , -3.940399  , -3.94039897, -3.940399  ]), (3, 2): array([-1.99      , -1.        , -2.9701    , -2.97009999, -2.9701    ]), (2, 2): array([-1.  ,  0.  , -1.99, -1.99, -1.99]), (1, 2): array([0., 0., 0., 0., 0.]), (4, 6): array([-6.79346492, -5.85198505, -7.72552803, -5.85198506, -7.72552344]), (3, 6): array([-5.85198505, -4.90099501, -6.79346512, -4.90099501, -6.79346465]), (2, 6): array([-4.90099501, -3.940399  , -5.85198505, -3.940399  , -5.85198477]), (1, 6): array([ -3.940399  , -53.900995  ,  -4.90099501,  -2.9701    ,\n",
      "        -4.900995  ]), (0, 6): array([-53.900995  , -53.90099496,  -3.940399  , -52.940399  ,\n",
      "       -54.85198478]), (5, 6): array([-7.72552746, -6.79346483, -8.64824179, -6.79346417, -8.64824278]), (6, 6): array([-8.64823331, -7.72552542, -9.56150539, -7.72552496, -9.56166063]), (6, 5): array([-7.72552651, -6.79346513, -8.64822774, -6.79346425, -8.64822633]), (6, 4): array([-6.79346465, -5.85198504, -7.72550624, -5.85198493, -7.72552385]), (6, 3): array([-5.85198495, -4.900995  , -6.79346482, -4.900995  , -6.79346294]), (5, 7): array([-8.64825051, -7.72552468, -9.56166523, -7.72552366, -9.5615558 ]), (4, 7): array([-7.72552744, -6.79346446, -8.64824374, -6.79346512, -8.64820621]), (3, 7): array([-6.7934643 , -5.85198498, -7.72552453, -5.85198505, -7.72550925]), (2, 7): array([-5.85198491, -4.900995  , -6.79346427, -4.900995  , -6.79346332]), (1, 7): array([ -4.900995  , -54.85198499,  -5.85198478,  -3.940399  ,\n",
      "        -5.85198492]), (0, 7): array([-54.851985  , -54.85198498,  -4.90099501, -53.90099498,\n",
      "       -55.79346206]), (0, 8): array([-55.79346239, -55.7934632 ,  -5.85198462, -54.85198498,\n",
      "       -50.        ]), (1, 8): array([ -5.85198461, -55.79346305,  -6.79346147,  -4.90099498,\n",
      "       -50.        ]), (6, 7): array([ -9.56161551,  -8.64825378, -10.46563004,  -8.64823447,\n",
      "       -10.46568483]), (7, 5): array([-8.64824016, -7.72552586, -9.56155444, -7.72551948, -9.56156928]), (7, 4): array([-7.72552049, -6.79345992, -8.64821808, -6.79346496, -8.64822719]), (7, 3): array([-6.79346375, -5.85198501, -7.72552292, -5.85198503, -7.72551713]), (6, 2): array([-4.900995  , -3.940399  , -5.85198503, -5.85198356, -5.85198476]), (7, 6): array([ -9.56167273,  -8.64823081, -10.46537886,  -8.64822629,\n",
      "       -10.46578487]), (8, 5): array([ -9.56152209,  -8.64823217, -50.        ,  -8.64817969,\n",
      "       -10.46535211]), (8, 4): array([ -8.6482256 ,  -7.72551353, -50.99      ,  -7.72552243,\n",
      "        -9.56139006]), (8, 3): array([ -7.72552595,  -6.79346468, -57.64824723,  -6.79346446,\n",
      "        -8.6482095 ]), (7, 2): array([-5.85198502, -4.90099501, -6.79346481, -6.79343761, -6.79346492]), (5, 2): array([-3.940399  , -2.9701    , -4.900995  , -4.90099469, -4.900995  ]), (8, 2): array([ -6.79346477,  -5.85198501, -56.72552891,  -7.72537822,\n",
      "        -7.72552696]), (7, 1): array([ -6.79344266,  -5.85198205,  -7.72539879, -56.72533107,\n",
      "        -5.85198504]), (6, 1): array([ -5.85198149,  -4.90099448,  -6.79341639, -55.79344496,\n",
      "        -4.90099501]), (5, 1): array([ -4.90099421,  -3.94039896,  -5.85198212, -54.85197877,\n",
      "        -3.940399  ]), (4, 1): array([ -3.9403988 ,  -2.97009999,  -4.90099467, -53.90099456,\n",
      "        -2.9701    ]), (3, 1): array([ -2.97009998,  -1.99      ,  -3.94039895, -52.94039886,\n",
      "        -1.99      ]), (2, 1): array([ -1.99      ,  -1.        ,  -2.97009999, -51.97000153,\n",
      "        -1.        ]), (1, 1): array([ -1.  , -50.  ,  -1.99, -50.99,   0.  ]), (0, 1): array([-50., -50.,  -1.,   0.,   0.]), (7, 7): array([-10.46558627,  -9.56156046, -11.35883203,  -9.56170103,\n",
      "       -11.35988975]), (6, 8): array([-10.46570841,  -9.56158069, -11.35998787,  -9.56158693,\n",
      "       -60.35962653]), (5, 8): array([ -9.56149422,  -8.64820028, -10.46558155,  -8.64822907,\n",
      "       -50.        ]), (4, 8): array([ -8.64823292,  -7.72551082,  -9.5615072 ,  -7.72552684,\n",
      "       -58.56146156]), (3, 8): array([ -7.72551953,  -6.79346156,  -8.64821646,  -6.79346486,\n",
      "       -50.        ]), (2, 8): array([ -6.793463  ,  -5.85198434,  -7.72550723,  -5.85198493,\n",
      "       -56.72551392]), (7, 8): array([-11.36014608, -10.46556866, -12.2429029 , -10.46565266,\n",
      "       -61.2436456 ]), (7, 9): array([-61.2427586 , -60.3601869 , -50.        , -11.36032864,\n",
      "       -61.24219056]), (6, 9): array([-60.36019888, -50.        , -61.24338074, -10.46570199,\n",
      "       -60.3601397 ]), (5, 9): array([-50.        , -58.56159035, -60.35986351,  -9.56161739,\n",
      "         0.        ]), (4, 9): array([-58.56151864, -50.        , -50.        ,  -8.6482306 ,\n",
      "       -58.56164195]), (3, 9): array([-50.        , -56.7255188 , -58.56148407,  -7.72550674,\n",
      "         0.        ]), (2, 9): array([-56.72551148, -50.        , -50.        ,  -6.79346062,\n",
      "       -56.72551512]), (1, 9): array([-50.        , -50.        , -56.72551514,   0.        ,\n",
      "         0.        ]), (0, 9): array([-50.       , -50.       , -50.       , -55.7934633,   0.       ]), (8, 9): array([-50.        , -61.24316218,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 6): array([-10.46510838,  -9.56143141, -50.        ,  -9.56133655,\n",
      "       -11.35970912]), (8, 7): array([-11.35913535, -10.46558126, -50.        , -10.46508896,\n",
      "       -12.23947275]), (8, 8): array([-12.24065498, -11.35952105, -50.        , -11.35890123,\n",
      "       -50.        ]), (9, 8): array([-50.        , -12.23434178,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 6): array([-50.      , -10.465158,   0.      ,   0.      ,   0.      ]), (9, 5): array([-50.        ,  -9.56146046, -50.        , -50.98999996,\n",
      "         0.        ]), (9, 4): array([-50.98999999,  -1.        , -50.98999998, -57.64824396,\n",
      "       -49.99999998]), (4, 0): array([-53.90099444, -52.94039866, -54.85198069, -53.9009944 ,\n",
      "        -3.94039895]), (3, 0): array([-52.94039893, -51.96981759, -53.90099438, -52.94039887,\n",
      "        -2.97009999]), (2, 0): array([-51.96996492, -50.99      , -52.9403989 , -51.9252433 ,\n",
      "        -1.99      ]), (1, 0): array([-50.99      , -50.        , -51.97006566, -50.99      ,\n",
      "        -1.        ]), (0, 0): array([-50.  , -50.  , -50.99,   0.  ,   0.  ]), (5, 0): array([-54.85197898, -53.90099469, -55.79344661, -54.85197803,\n",
      "        -4.90099425]), (6, 0): array([-55.79344384, -54.85198016, -56.72543178, -55.79343748,\n",
      "        -5.85198089]), (9, 7): array([-50.       , -11.3585694,   0.       ,   0.       ,   0.       ]), (9, 3): array([-57.64823484,  -7.72552173, -57.64825432, -56.72552904,\n",
      "       -50.98999999]), (9, 2): array([-56.72552363,  -6.79346491, -56.72552834, -57.64776423,\n",
      "       -57.6482433 ]), (8, 1): array([ -7.72543307,  -6.79343043, -57.64786716, -57.64794288,\n",
      "        -6.79346496]), (9, 1): array([ -57.64790944,   -7.72535045,  -57.64780921, -107.07112742,\n",
      "        -56.72552694]), (8, 0): array([ -57.64777429,  -56.72540199, -107.07038009,  -57.64793156,\n",
      "         -7.72542175]), (7, 0): array([-56.72522472, -55.79345339, -57.6478447 , -56.72540277,\n",
      "        -6.79342766]), (9, 0): array([-107.07055654,  -57.64794489, -107.0703729 , -107.0710856 ,\n",
      "        -57.64800304]), (0, 2): array([-49.99999999,   0.        ,   0.        ,   0.        ,\n",
      "       -50.99      ])}\n",
      "Q-table for Agent 3: {(2, 3): array([-2.97009999, -3.94038425, -1.99      , -1.99      , -3.94037589]), (1, 3): array([ -3.94038397, -53.90053411,  -2.97009756,  -2.97009863,\n",
      "        -4.89991022]), (0, 3): array([-53.90043094, -53.90031192,  -3.94035881, -52.94032377,\n",
      "       -54.84985183]), (3, 3): array([-1.98999997, -2.97009996, -2.97005407, -1.        , -2.9700981 ]), (2, 2): array([-1.98999979, -2.97009828, -1.        , -1.        , -2.97009969]), (1, 2): array([ -2.97009224, -52.94034744,  -1.9899999 ,  -1.98999994,\n",
      "        -3.94032757]), (0, 2): array([-52.94012066, -52.9403295 ,  -2.97009736, -51.9700245 ,\n",
      "       -53.90076382]), (3, 2): array([-0.99999998, -1.98999998, -1.98999597,  0.        , -1.98999868]), (3, 1): array([0., 0., 0., 0., 0.]), (2, 4): array([-3.94027145, -4.90026087, -2.97009941, -2.97009969, -4.89889696]), (3, 4): array([-2.97009902, -3.9402644 , -3.94014189, -1.98999999, -3.9401731 ]), (1, 4): array([ -4.90038927, -54.84347448,  -3.94028325,  -3.94034998,\n",
      "        -5.84769165]), (0, 4): array([-54.84974704, -54.84945877,  -4.90057687, -53.90065976,\n",
      "       -55.76124763]), (2, 5): array([-4.89869383, -5.84121978, -3.94013907, -3.94029244, -5.82792937]), (1, 5): array([ -5.84095236, -55.75168811,  -4.8994234 ,  -4.9002275 ,\n",
      "        -6.70925084]), (0, 5): array([-55.74507412, -55.75636064,  -5.83655095, -54.85019395,\n",
      "       -49.99999859]), (0, 1): array([ -51.97009123,  -51.9700948 ,   -1.98999955, -101.45009877,\n",
      "        -52.94024195]), (1, 1): array([ -1.98999925, -51.97008677,  -1.        , -51.97003665,\n",
      "        -2.97009642]), (0, 0): array([-101.45008202, -101.44983004,  -51.9700598 , -101.44985204,\n",
      "        -51.97007015]), (1, 0): array([ -51.97007113, -101.4498742 ,  -50.98994959,  -51.97004185,\n",
      "         -1.98999732]), (3, 5): array([-3.93988855, -4.89881283, -4.89383592, -2.97006664, -4.89041055]), (4, 4): array([-3.94019246, -2.97009719, -4.89474576, -2.97005424, -4.89000972]), (4, 3): array([-2.97006257, -1.98999918, -3.93865924, -1.989996  , -3.93963784]), (2, 1): array([ -0.99999979,  -1.98999991,   0.        , -50.98991237,\n",
      "        -1.98999898]), (4, 5): array([-4.89786712, -3.94000646, -5.83602533, -3.94022641, -5.83839344]), (2, 6): array([-5.81614939, -6.69352252, -4.89423984, -4.89935592, -6.72746545]), (1, 6): array([ -6.69191347, -49.99999925,  -5.82157224,  -5.84605817,\n",
      "        -7.53995055]), (0, 6): array([-49.99999155, -49.99999945,  -6.69934816, -55.75198921,\n",
      "         0.        ]), (3, 6): array([-4.89194663, -5.83425167, -5.83742791, -3.93975532, -5.84250292]), (2, 7): array([-6.73689201, -7.59531614, -5.83337134, -5.82391133, -7.58153653]), (1, 7): array([ -7.53561531, -49.99998712,  -6.74608128,  -6.71810886,\n",
      "        -8.32273673]), (0, 7): array([-49.99999384, -49.99995896,  -7.56255739, -49.99999316,\n",
      "         0.        ]), (3, 7): array([-5.83838469, -6.70388847, -6.71740466, -4.89735199, -6.7548624 ]), (2, 8): array([ -7.62544387,  -8.33063951,  -6.7542838 ,  -6.73367937,\n",
      "       -49.99997307]), (1, 8): array([ -8.34699189, -49.99999155,  -7.55555623,  -7.51359136,\n",
      "       -49.99999501]), (0, 8): array([-49.99836041,   0.        ,   0.        , -49.99998569,\n",
      "         0.        ]), (2, 0): array([-50.98995362, -51.97008702, -49.99998037, -50.98990789,\n",
      "        -0.99999828]), (3, 8): array([ -6.73996589,  -7.53806646,  -7.51102253,  -5.83743587,\n",
      "       -49.99999384]), (4, 8): array([ -7.57611684,  -6.73066318,  -8.17666665,  -6.73466947,\n",
      "       -49.99998037]), (4, 7): array([-6.73092914, -5.83379811, -7.5229692 , -5.83405424, -7.53607904]), (4, 6): array([-5.83808894, -4.89764926, -6.70427208, -4.89756148, -6.70066997]), (5, 5): array([-5.82974906, -4.89844207, -6.73555756, -4.89459054, -6.74010273]), (1, 9): array([-49.99997008, -49.99999636, -49.99992277,  -8.23243294,\n",
      "         0.        ]), (0, 9): array([-49.99997819, -49.99999384, -49.99996306,   0.        ,\n",
      "         0.        ]), (2, 9): array([-49.99997577, -49.99997008, -49.99999807,  -7.60559758,\n",
      "         0.        ]), (3, 9): array([-49.99996676, -49.9999305 , -49.9999305 ,  -6.72522263,\n",
      "         0.        ]), (4, 9): array([-49.99997577, -49.99998957, -49.99999384,  -7.56413968,\n",
      "         0.        ]), (5, 8): array([ -8.26702971,  -7.54590393,  -8.79895458,  -7.51616688,\n",
      "       -49.99992277]), (5, 7): array([-7.52229683, -6.6819741 , -8.21110065, -6.72970919, -8.05417126]), (5, 6): array([-6.69608753, -5.83128062, -7.53388829, -5.8219773 , -7.49935023]), (5, 9): array([-49.99996306, -49.9999437 , -58.02462811,  -8.17982306,\n",
      "         0.        ]), (6, 8): array([ -8.82892932,  -8.26166299,  -9.25662582,  -8.24298454,\n",
      "       -58.15488816]), (6, 7): array([-8.23729306, -7.50845704, -8.78853709, -7.48754652, -8.82026695]), (6, 6): array([-7.51204096, -6.7273474 , -8.13520936, -6.7212118 , -8.20292912]), (5, 4): array([-4.89560106, -3.93992015, -5.81464324, -3.93896966, -5.82369397]), (6, 4): array([-5.81278337, -4.89351811, -6.68334323, -4.89312667, -6.71871533]), (5, 3): array([-3.93908466, -2.97006492, -4.89475998, -2.96978412, -4.8953998 ]), (4, 2): array([-1.98999009, -0.99999996, -2.96965239, -0.99999997, -2.97003154]), (5, 2): array([-2.96973066, -1.98999708, -3.93810932, -1.98989714, -3.93931118]), (4, 1): array([-0.99999961,  0.        ,  0.        ,  0.        , -1.9899959 ]), (6, 9): array([-58.26836909, -49.99992277, -49.99958324,  -8.77868871,\n",
      "       -58.22220324]), (7, 8): array([ -9.17706399,  -8.76871058,  -9.52094888,  -8.71859339,\n",
      "       -49.99997577]), (7, 7): array([-8.77074208, -8.17707868, -9.19467277, -8.26178199, -9.21720017]), (7, 6): array([-8.22955907, -7.55531691, -8.65724842, -7.50094153, -8.71099372]), (6, 5): array([-6.72098983, -5.83892485, -7.50439041, -5.83077621, -7.5403526 ]), (6, 3): array([-4.89397415, -3.93938112, -5.81434793, -3.93932324, -5.82425527]), (6, 2): array([-3.93859515, -2.96976792, -4.89346076, -2.96992584, -4.89306426]), (5, 1): array([ -1.98999117,  -0.99999909,  -2.96987117, -51.96955798,\n",
      "        -2.96931707]), (7, 9): array([-49.99991419, -58.09829932, -58.69615803,  -9.23456271,\n",
      "         0.        ]), (8, 9): array([-58.63951397, -49.99989406, -49.99998712,  -9.31813579,\n",
      "       -58.77665602]), (9, 9): array([-49.99986922, -58.58719893,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 8): array([ -9.59959419,  -9.10814104, -49.9999437 ,  -8.97382288,\n",
      "       -58.86188741]), (8, 7): array([ -9.23388204,  -8.83192354, -58.27167992,  -8.68405865,\n",
      "        -9.30046916]), (9, 7): array([-58.41343357,  -8.99604118, -58.27706838, -58.0842092 ,\n",
      "       -49.9998206 ]), (8, 6): array([ -8.83092177,  -8.29929098, -49.99997833,  -8.17515726,\n",
      "        -9.14080317]), (8, 5): array([ -8.1498775 ,  -7.57189637, -57.69280422,  -7.45222185,\n",
      "        -8.83460507]), (9, 6): array([-58.4909722 ,  -8.83526114, -58.41442794, -57.58802315,\n",
      "       -58.47183938]), (7, 5): array([-7.53374204, -6.68706484, -8.18295934, -6.72253943, -8.15040248]), (7, 4): array([-6.65932194, -5.82656772, -7.34958824, -5.81354926, -7.49968846]), (8, 4): array([ -7.45068059,  -6.70659498, -57.11695518,  -6.67264355,\n",
      "        -8.16657939]), (7, 3): array([-5.82743986, -4.88857427, -6.70574252, -4.89447555, -6.7080658 ]), (7, 2): array([-4.89481777, -3.93908973, -5.83341148, -3.93951603, -5.81854177]), (6, 1): array([ -2.96996945,  -1.98996376,  -3.93940706, -52.93824824,\n",
      "        -3.93762755]), (7, 1): array([ -3.93927385,  -2.96977075,  -4.89657598, -53.88986191,\n",
      "        -4.89393387]), (3, 0): array([-49.99980066, -50.98976119, -49.99936479,   0.        ,\n",
      "         0.        ]), (9, 8): array([-49.99972656,  -9.48396544, -49.99942831, -58.14692335,\n",
      "         0.        ]), (9, 5): array([-57.4789554 ,  -8.14578803, -57.68125929, -56.94849901,\n",
      "       -58.17634408]), (9, 4): array([-57.17454032,  -7.43348415, -57.17387703, -49.99985468,\n",
      "       -57.56918039]), (9, 3): array([-49.99998712,  -6.64639639, -49.99989406,   0.        ,\n",
      "       -57.13075491]), (8, 3): array([ -6.6768619 ,  -5.81149525, -49.99962491,  -5.8253568 ,\n",
      "        -7.42302538]), (8, 2): array([ -5.83303189,  -4.89245453, -49.99991419,  -4.89666641,\n",
      "        -6.68897773]), (9, 2): array([-49.99942831,  -5.82175367,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 1): array([ -4.89837842,  -3.93945737, -49.99977852, -54.80708274,\n",
      "        -5.82986063]), (6, 0): array([-52.93870798, -51.96929889, -53.89010761, -52.93914238,\n",
      "        -2.96994326]), (5, 0): array([-51.9695286 , -49.997501  , -52.93953202, -51.96728673,\n",
      "        -1.98993288]), (4, 0): array([-49.99975391, -49.99969618, -51.96901912,   0.        ,\n",
      "         0.        ]), (8, 0): array([-54.83431023, -53.8842683 , -49.99903184, -54.82908922,\n",
      "        -4.89580212]), (7, 0): array([-53.87936732, -52.93769273, -54.83456195, -53.87690231,\n",
      "        -3.93706247]), (9, 0): array([-49.9998206, -54.8338684,   0.       ,   0.       ,   0.       ]), (9, 1): array([-49.99989406,  -4.89684076,   0.        ,   0.        ,\n",
      "         0.        ])}\n",
      "Q-table for Agent 4: {(4, 3): array([-3.94039889, -2.9701    , -4.90099003, -4.90094739, -2.9701    ]), (3, 3): array([-2.9701    , -1.99      , -3.94039897, -3.94038472, -1.99      ]), (2, 3): array([-1.98999999, -1.        , -2.97009998, -2.97009872, -1.        ]), (1, 3): array([ -1.        , -50.98999992,  -1.98999925,  -1.98999971,\n",
      "         0.        ]), (0, 3): array([-50.98999998, -50.98999956,  -1.        , -51.97009335,\n",
      "       -49.99999859]), (1, 4): array([0., 0., 0., 0., 0.]), (5, 3): array([-4.90098144, -3.94039816, -5.85192854, -5.85181657, -3.94039843]), (4, 2): array([-4.90090187, -3.94039475, -5.85186951, -5.85021718, -3.94039894]), (3, 2): array([-3.94038613, -2.97009964, -4.90094581, -4.90074618, -2.97009996]), (2, 2): array([-2.97009697, -1.98999993, -3.94039222, -3.94033718, -1.98999992]), (2, 1): array([ -3.94038133,  -2.97009823,  -4.90048195, -53.90083219,\n",
      "        -2.97009829]), (1, 1): array([ -2.97008543, -52.94035184,  -3.9403821 , -52.9402449 ,\n",
      "        -1.98999987]), (0, 1): array([ -52.94035729,  -52.94033487,   -2.97008909, -102.41006235,\n",
      "        -51.97008521]), (3, 1): array([ -4.90060962,  -3.94037774,  -5.84985715, -54.85087151,\n",
      "        -3.94037816]), (2, 0): array([-53.90092278, -52.9403785 , -54.85093202, -53.90084664,\n",
      "        -3.94037733]), (1, 0): array([ -52.94037653, -102.41020158,  -53.9008905 ,  -52.94035727,\n",
      "         -2.97009801]), (0, 0): array([-102.41040514, -102.41002533,  -52.94036785, -102.41003271,\n",
      "        -52.9401987 ]), (3, 0): array([-54.8492019 , -53.90089492, -55.78243939, -54.85062575,\n",
      "        -4.90063925]), (1, 2): array([ -1.98999973, -51.97008323,  -2.97009807,  -2.97009726,\n",
      "        -1.        ]), (0, 2): array([-51.97009232, -51.97005683,  -1.98999912, -52.94014847,\n",
      "       -50.98999628]), (5, 2): array([-5.85166147, -4.90096882, -6.79247868, -6.78597125, -4.90097268]), (4, 1): array([ -5.8506466 ,  -4.90044893,  -6.78779952, -55.78617718,\n",
      "        -4.90094871]), (4, 4): array([-2.97009995, -1.99      , -3.94039728, -3.94039873, -3.94037465]), (3, 4): array([-1.98999999, -1.        , -2.97009996, -2.97009993, -2.97009684]), (5, 4): array([-3.94039724, -2.97009997, -4.9009673 , -4.90098514, -4.90081029]), (4, 5): array([-3.94037905, -2.97009398, -4.90084296, -2.97009994, -4.90061955]), (3, 5): array([-2.97009673, -1.98999901, -3.94038566, -1.98999995, -3.940332  ]), (2, 5): array([-1.98999961, -1.        , -2.97009487, -1.        , -2.97009448]), (5, 5): array([-4.90092644, -3.9403843 , -5.85129688, -3.94039828, -5.85065516]), (2, 4): array([-1.        ,  0.        , -1.98999996, -1.98999996, -1.98999998]), (6, 4): array([-4.90092467, -3.94039764, -5.85190198, -5.85184886, -5.85171657]), (6, 5): array([-5.85114823, -4.90092117, -6.79064414, -4.90097297, -6.79061702]), (7, 4): array([-5.85191461, -4.90098083, -6.79310169, -6.79226956, -6.79125635]), (6, 3): array([-5.85174234, -4.90097954, -6.7913933 , -6.79219452, -4.9009763 ]), (7, 3): array([-6.79123255, -5.85173463, -7.721709  , -7.71786153, -5.85193558]), (8, 3): array([ -7.72189383,  -6.79245794, -49.99999981,  -8.62483935,\n",
      "        -6.7931997 ]), (6, 2): array([-6.7915808 , -5.85170613, -7.71881554, -7.71217745, -5.85175398]), (7, 2): array([-7.71578372, -6.7920772 , -8.6216176 , -8.61810245, -6.79108291]), (6, 1): array([ -7.71078967,  -6.78946063,  -8.62334057, -57.62021829,\n",
      "        -6.79164583]), (5, 1): array([ -6.78694279,  -5.85078605,  -7.71446459, -56.70830203,\n",
      "        -5.85167241]), (4, 0): array([-55.78656046, -54.84988568, -56.71178162, -55.78681414,\n",
      "        -5.84900518]), (5, 0): array([-56.70870622, -55.78493678, -57.61738216, -56.70348977,\n",
      "        -6.7872611 ]), (6, 0): array([-57.62078991, -56.70634401, -58.49637669, -57.61073085,\n",
      "        -7.71399587]), (7, 5): array([-6.79062662, -5.85163307, -7.71994021, -5.85188962, -7.71726662]), (8, 5): array([ -7.71808321,  -6.79177933, -49.99998841,  -6.79326828,\n",
      "        -8.62941962]), (8, 6): array([ -8.61691502,  -7.71707192, -49.99999239,  -7.71809327,\n",
      "        -9.46401622]), (7, 6): array([-7.71795376, -6.79045351, -8.62667337, -6.79012391, -8.59926058]), (6, 6): array([-6.79022709, -5.85074816, -7.71587455, -5.8516002 , -7.71355399]), (5, 6): array([-5.85067123, -4.90068381, -6.79000375, -4.90086757, -6.78942231]), (4, 6): array([-4.90072316, -3.9403302 , -5.85086754, -3.94036012, -5.85114525]), (3, 6): array([-3.94032027, -2.97008935, -4.90046642, -2.9700921 , -4.90082603]), (2, 6): array([-2.97009769, -1.98999959, -3.94034637, -1.98999961, -3.94035452]), (7, 7): array([-8.60497625, -7.70279835, -9.44022299, -7.71541597, -9.45464994]), (6, 7): array([-7.71219213, -6.78872521, -8.61107926, -6.79040585, -8.61825165]), (5, 7): array([-6.7873941 , -5.85030292, -7.7092601 , -5.85076227, -7.71375583]), (4, 7): array([-5.85097402, -4.90074497, -6.78913816, -4.90065085, -6.78947026]), (3, 7): array([-4.90075951, -3.94033681, -5.85072976, -3.94033419, -5.85085169]), (2, 7): array([-3.94037188, -2.97009828, -4.90076842, -2.97008387, -4.9008776 ]), (1, 7): array([ -2.97009918, -52.94039236,  -3.94034844,  -1.98999989,\n",
      "        -3.94035356]), (1, 8): array([ -3.94028618, -49.9999996 ,  -4.90087429,  -2.97009767,\n",
      "       -49.99999636]), (0, 8): array([-49.99999981, -49.99999859,  -3.94035452, -52.94029933,\n",
      "         0.        ]), (2, 8): array([ -4.90078388,  -3.94038702,  -5.8513021 ,  -3.94035934,\n",
      "       -49.99999897]), (0, 7): array([-52.94037412, -52.94038183,  -2.97009839, -51.97008555,\n",
      "       -49.99999239]), (8, 7): array([ -9.44543196,  -8.59210264, -49.99999843,  -8.62575792,\n",
      "       -10.26045715]), (9, 6): array([-49.99999735,  -8.62970822,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 4): array([ -6.79325396,  -5.85192357, -49.99999735,  -7.72173846,\n",
      "        -7.71292721]), (8, 2): array([ -8.63380338,  -7.7195319 , -49.99999983,  -9.49686085,\n",
      "        -7.71841011]), (7, 1): array([ -8.61425049,  -7.71171474,  -9.49960765, -58.49723444,\n",
      "        -7.71970253]), (8, 1): array([ -9.50083566,  -8.61484872, -49.99999933, -59.34491283,\n",
      "        -8.63034589]), (7, 0): array([-58.50414992, -57.6178891 , -59.35316953, -58.48073401,\n",
      "        -8.62171002]), (8, 0): array([-59.31488282, -58.50570858, -49.99999316, -59.3706559 ,\n",
      "        -9.50540157]), (9, 7): array([-49.99999843,  -9.43967698,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 8): array([-10.26619923,  -9.46960301, -49.99999785,  -9.44156076,\n",
      "       -49.99999859]), (7, 8): array([ -9.45996562,  -8.60674924, -10.31319099,  -8.60843141,\n",
      "       -59.30788524]), (6, 8): array([ -8.62601478,  -7.71432752,  -9.48036045,  -7.70952367,\n",
      "       -58.49934456]), (4, 8): array([ -6.78941074,  -5.85083098,  -7.71184631,  -5.85053714,\n",
      "       -56.71503995]), (3, 8): array([ -5.85134834,  -4.90083644,  -6.78945405,  -4.90080213,\n",
      "       -49.99999989]), (3, 9): array([-49.99999983, -49.99999985, -56.71456   ,  -5.85135662,\n",
      "         0.        ]), (2, 9): array([-49.99999981, -49.99999596, -49.99999551,  -4.90083843,\n",
      "         0.        ]), (1, 9): array([-49.99999859, -49.99999996, -49.99999673,  -3.94037053,\n",
      "         0.        ]), (0, 9): array([-49.99999945, -49.99999596, -49.99999873,   0.        ,\n",
      "         0.        ]), (4, 9): array([-56.71801685, -49.99999761, -49.99999705,  -6.78904067,\n",
      "       -56.71623243]), (5, 8): array([ -7.71528751,  -6.78817075,  -8.62137941,  -6.78983663,\n",
      "       -49.99999826]), (5, 9): array([-49.99999886, -56.71734158, -58.49887369,   0.        ,\n",
      "         0.        ]), (1, 6): array([ -1.98999988, -51.97009097,  -2.97009205,  -0.99999999,\n",
      "        -2.97009737]), (0, 6): array([-51.97009476, -51.97009221,  -1.98999977, -49.99997819,\n",
      "       -52.94035273]), (1, 5): array([ -0.99999993, -49.99999551,  -1.98999994,   0.        ,\n",
      "        -1.98992668]), (0, 5): array([-49.9999996 , -49.99999939,  -0.99999996, -49.99999551,\n",
      "         0.        ]), (9, 1): array([-49.99999951,  -9.5036024 , -49.99999551, -49.99999705,\n",
      "         0.        ]), (9, 0): array([-49.99999501, -59.34957815, -49.99999843,   0.        ,\n",
      "         0.        ]), (9, 2): array([-49.99999446,  -8.62594335,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 3): array([-49.99999636,  -7.72148365,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 4): array([-49.99999974,  -6.79327743,   0.        ,   0.        ,\n",
      "         0.        ]), (9, 5): array([-49.9999999 ,  -7.71466925,   0.        ,   0.        ,\n",
      "         0.        ]), (6, 9): array([-58.49526898, -49.99999917, -59.23411705,  -8.6256722 ,\n",
      "       -58.47154937]), (7, 9): array([-59.30679489, -58.51192976, -49.99999873,  -9.46179772,\n",
      "       -59.31054846]), (9, 8): array([-49.9999996 , -10.27457563,   0.        ,   0.        ,\n",
      "         0.        ]), (8, 9): array([-49.99999785, -59.2478518 , -49.99998569, -10.25166048,\n",
      "         0.        ]), (9, 9): array([-49.99999446, -49.99999945,   0.        ,   0.        ,\n",
      "       -49.99999446]), (0, 4): array([-49.99999061, -49.99999501,   0.        ,   0.        ,\n",
      "         0.        ])}\n",
      "Agent 1 Q-values for position (3, 4): [-4.90099501 -5.85198475 -3.940399   -3.940399   -5.85198466]\n",
      "Agent 2 Q-values for position (4, 5): [-5.85198506 -4.90099501 -6.79346473 -4.90099501 -6.79346517]\n",
      "Agent 3 Q-values for position (2, 3): [-2.97009999 -3.94038425 -1.99       -1.99       -3.94037589]\n",
      "Agent 4 Q-values for position (4, 3): [-3.94039889 -2.9701     -4.90099003 -4.90094739 -2.9701    ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Initial Test Environment =====\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "\"\"\"\n",
    "Creates and trains agents with cyclic goals (each agent's goal is another agent's start position),\n",
    "then tests them in a shared environment using the safety pipeline.\n",
    "\n",
    "Args:\n",
    "    num_agents: Number of agents to create\n",
    "    width: Width of the grid environment\n",
    "    height: Height of the grid environment\n",
    "    training_episodes: Number of episodes for training each agent\n",
    "    test_steps: Maximum number of steps to run the test\n",
    "\"\"\"\n",
    "# # # Define agent positions in a cycle (each agent's goal is the next agent's position)\n",
    "# agent_positions = [(3,2),(2,3)]\n",
    "#                 #    ,(5,6),(1,2)] # Rotate to create cycle with +1 to x coordinate\n",
    "# goal_positions = [(3,4),(4,3)]\n",
    "                # (6,6),(1,1),(5,3)] # Rotate to create cycle with +1 to x coordinate\n",
    "width =10\n",
    "height = 10\n",
    "# num_agents =4\n",
    "# agent_positions, goal_positions = generate_random_positions(num_agents=num_agents, width=width, height=height, min_distance=2)\n",
    "\n",
    "# Define some walls to make the environment more interesting\n",
    "walls = [(i, 5) for i in range(3, 7)]\n",
    "\n",
    "# Create training environment (separate environment for each agent)\n",
    "train_env = MultiAgentEnvWrapper(\n",
    "    is_testing=False,\n",
    "    width=height,\n",
    "    height=height,\n",
    "    agent_positions=agent_positions,\n",
    "    goal_positions=goal_positions,\n",
    "    walls=walls\n",
    ")\n",
    "\n",
    "# Create and train agents\n",
    "agents = []\n",
    "for i in range(len(agent_positions)):\n",
    "    print(f\"\\n===== Creating and training Agent {i+1} =====\")\n",
    "    agent = DynaQLearningAgent(state_dim=width*height, action_dim=5)\n",
    "    agent.env_wrapper = train_env\n",
    "    agent.agent_idx = i\n",
    "    agents.append(agent)\n",
    "\n",
    "# Train all agents independently\n",
    "print(\"\\n===== Training Agents =====\")\n",
    "training_episodes = 1500\n",
    "all_rewards = train_all_agents(train_env, agents, num_episodes=training_episodes, max_steps=200)\n",
    "\n",
    "# Plot training rewards\n",
    "plot_rewards_for_all_agents(all_rewards)\n",
    "Qstar = {}\n",
    "# Copy Q-tables from all agents to a shared Q-table\n",
    "# Create a structured dictionary to store Q-tables from all agents\n",
    "for i, agent in enumerate(agents):\n",
    "    Qstar[i] = agent.q_table.copy()  # Store each agent's Q-table separately\n",
    "    print(f\"Q-table for Agent {i+1}:\", Qstar[i])\n",
    "# Print sample Q-values from each agent's Q-table\n",
    "for i in range(len(agents)):\n",
    "    print(f\"Agent {i+1} Q-values for position {agent_positions[i]}: {Qstar[i].get(agent_positions[i], 'Not found')}\")\n",
    "# Print Q-tables for all agents\n",
    "\n",
    "# Create test environment (shared environment with all agents)\n",
    "test_env = MultiAgentEnvWrapper(\n",
    "    is_testing=True,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    agent_positions=agent_positions,\n",
    "    goal_positions=goal_positions,\n",
    "    walls=walls\n",
    ")\n",
    "test_env.visualize()\n",
    "# Update agents to use the test environment\n",
    "for i, agent in enumerate(agents):\n",
    "    agent.env_wrapper = test_env\n",
    "\n",
    "# Visualize initial state\n",
    "print(\"\\n===== Initial Test Environment =====\")\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aabfb524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned path for Agent 1: [(3, 1), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(2), np.int64(3)), (np.int64(3), np.int64(3))]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m video_path,Qsteps\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Run the simulation and save the video\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m video_path, Qstep \u001b[38;5;241m=\u001b[39m \u001b[43mrun_and_record_safety_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarl_simulation.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQstars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQstar\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m Qstep \u001b[38;5;241m=\u001b[39m flatten_qsteps(Qstep, k_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, L_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Save Qsteps to a file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 71\u001b[0m, in \u001b[0;36mrun_and_record_safety_pipeline\u001b[0;34m(test_env, agents, video_name, max_steps, k, L, safety_threshold, Qstars)\u001b[0m\n\u001b[1;32m     68\u001b[0m Qsteps[step] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Run one step of the safety pipeline\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m actions,safety_status,Qstep \u001b[38;5;241m=\u001b[39m\u001b[43msafety_aware_pipeline_with_safety_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate Qtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 83\u001b[0m, in \u001b[0;36msafety_aware_pipeline_with_safety_check\u001b[0;34m(env, agents, k, L)\u001b[0m\n\u001b[1;32m     80\u001b[0m     Qsteps[agent_idx] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     81\u001b[0m     figs[agent_idx] \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Initialize figure dictionary for this agent\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msafety_status\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     84\u001b[0m     qtable \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mq_table\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     85\u001b[0m     updated_q_table, Qstep, agent_figs \u001b[38;5;241m=\u001b[39m temp_Qlearning(k, L, qtable, agent_idx, safety_env, agent)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def run_and_record_safety_pipeline(test_env, agents, video_name=\"marl_simulation.mp4\", max_steps=30, \n",
    "                                  k=3, L=200, safety_threshold=-2.0, Qstars=None):\n",
    "    \"\"\"\n",
    "    Runs the safety pipeline for multi-agent reinforcement learning and saves the visualization as a video.\n",
    "    \n",
    "    Args:\n",
    "        test_env (MultiAgentEnvWrapper): The test environment with all agents\n",
    "        agents (list): List of agent objects\n",
    "        video_name (str): Name of the output video file\n",
    "        max_steps (int): Maximum number of steps to run the simulation\n",
    "        k (int): Safety lookahead parameter for safety_aware_pipeline\n",
    "        L (int): Learning iterations parameter for safety_aware_pipeline\n",
    "        safety_threshold (float): Safety threshold for determining safe actions\n",
    "        Qstars (dict): Dictionary of Q-tables for all agents\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved video file\n",
    "    \"\"\"\n",
    "    # Create videos directory if it doesn't exist\n",
    "    videos_dir = os.path.expanduser(\"~/Desktop/Projet_MARL/toy_exeemple_normalized_V3\")\n",
    "    os.makedirs(videos_dir, exist_ok=True)\n",
    "    \n",
    "    # Full path for video file\n",
    "    video_path = os.path.join(videos_dir, video_name)\n",
    "    Qstar = {}\n",
    "    # Copy Q-tables from all agents to a shared Q-table\n",
    "    # Create a structured dictionary to store Q-tables from all agents\n",
    "    # for i, agent in enumerate(agents):\n",
    "    #      Qstar[i]=normalize_Qtable(agent.q_table.copy())\n",
    "    #      agent.q_table=Qstar[i]  # Store each agent's Q-table separately\n",
    "    # Reset environment\n",
    "    obs, info = test_env.reset()\n",
    "    \n",
    "    # Initialize frame collection\n",
    "    frames = []\n",
    "    \n",
    "    for i, agent in enumerate(agents):\n",
    "        Qstar[i] = agent.q_table.copy() \n",
    "    # Add initial state to frames \n",
    "    mask=visualize_all_agents_planned_paths(test_env, agents, k_steps=k,Qstar=Qstar)\n",
    "    # Get the grid image using your visualize method with show=False\n",
    "    grid_img = test_env.visualize(show=False,highlight_masks=mask)\n",
    "    \n",
    "    # Create a figure with the initial state\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.title(\"Initial State\", fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    # Convert matplotlib figure to image\n",
    "    canvas = FigureCanvas(plt.gcf())\n",
    "    canvas.draw()\n",
    "    initial_frame = np.array(canvas.renderer.buffer_rgba())\n",
    "    plt.close()\n",
    "    frames.append(initial_frame)\n",
    "    Qsteps = {}\n",
    "    # Run simulation with safety pipeline\n",
    "    for step in range(max_steps):\n",
    "        Qsteps[step] = {}\n",
    " \n",
    "        # Run one step of the safety pipeline\n",
    "        actions,safety_status,Qstep =safety_aware_pipeline_with_safety_check(test_env, agents, k=k, L=L)\n",
    "        if k<=2:\n",
    "            print(\"update Qtable\")\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.q_table = Qstar[i]\n",
    "        mask=visualize_all_agents_planned_paths(test_env, agents, k_steps=k,Qstar=Qstar)\n",
    "        Qsteps[step] = Qstep\n",
    "\n",
    "        # Check if all agents have reached their goals\n",
    "        done = test_env.all_agents_at_goals()\n",
    "        \n",
    "        # Get the grid image using your visualize method with show=False\n",
    "        grid_img = test_env.visualize(highlight_masks=mask,show=False)\n",
    "        \n",
    "        # Create a figure for this step\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_img)\n",
    "        plt.title(f\"Step {step+1} | Actions: {actions} | Safety_status:{safety_status}\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Convert matplotlib figure to image\n",
    "        canvas = FigureCanvas(plt.gcf())\n",
    "        canvas.draw()\n",
    "        frame = np.array(canvas.renderer.buffer_rgba())\n",
    "        plt.close()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Report actions taken\n",
    "        print(f\"Actions taken: {actions}\")\n",
    "        \n",
    "        # Check if simulation is complete\n",
    "        if done:\n",
    "            print(f\"All agents reached their goals in {step+1} steps!\")\n",
    "            \n",
    "            # Get the grid image for the final state\n",
    "            grid_img = test_env.visualize(show=False)\n",
    "            \n",
    "            # Add a success message\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(grid_img)\n",
    "            plt.title(f\"Success! All agents reached goals in {step+1} steps\", \n",
    "                     fontsize=18, color='green')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Convert matplotlib figure to image\n",
    "            canvas = FigureCanvas(plt.gcf())\n",
    "            canvas.draw()\n",
    "            success_frame = np.array(canvas.renderer.buffer_rgba())\n",
    "            plt.close()\n",
    "            frames.append(success_frame)\n",
    "            break\n",
    "    \n",
    "    # Now create the video writer and write all frames\n",
    "    # Determine frame size from the first frame\n",
    "    height, width, _ = frames[0].shape\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_path, fourcc, 2, (width, height))  # 2 fps for easy viewing\n",
    "    \n",
    "    # Write frames to video\n",
    "    for frame in frames:\n",
    "        # Convert RGBA to BGR (OpenCV format)\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\n",
    "        video.write(frame_bgr)\n",
    "    \n",
    "    # Release the video writer\n",
    "    video.release()\n",
    "    \n",
    "    print(f\"Video saved to: {video_path}\")\n",
    "    return video_path,Qsteps\n",
    "# Run the simulation and save the video\n",
    "video_path, Qstep = run_and_record_safety_pipeline(\n",
    "    test_env, agents, video_name=\"marl_simulation.mp4\", max_steps=30, \n",
    "    k=2, L=500, safety_threshold=-2.0, Qstars=Qstar\n",
    ")\n",
    "\n",
    "Qstep = flatten_qsteps(Qstep, k_value=1, L_value=500)\n",
    "\n",
    "# Save Qsteps to a file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(Qstep)\n",
    "\n",
    "\n",
    "# Extract the directory from video_path\n",
    "videos_dir = os.path.dirname(video_path)\n",
    "\n",
    "# Save the dataframe to CSV\n",
    "csv_path = os.path.join(videos_dir, \"Qsteps.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1486d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_qtable_actions_with_unsafe_set(env, agent_idx, qtable, reachable_set, unsafe_set_dict, \n",
    "                                            show=True, title=None, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Creates a visualization that shows:\n",
    "    1. The preferred action for each state in the reachable set according to a Q-table\n",
    "    2. Unsafe states for the agent\n",
    "    3. Positions of other agents and agent goals\n",
    "    \n",
    "    Args:\n",
    "        env: The environment wrapper\n",
    "        agent_idx: Index of the agent to visualize\n",
    "        qtable: Q-table dictionary of the agent\n",
    "        reachable_set: Set of reachable states for the agent\n",
    "        unsafe_set_dict: Dictionary of unsafe states for the agent\n",
    "        show: Whether to display the plot (True) or just return the figure (False)\n",
    "        title: Optional title for the plot\n",
    "        figsize: Size of the figure as tuple (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Arrow, Rectangle, Circle\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Set up the grid\n",
    "    width = env.width\n",
    "    height = env.height\n",
    "    \n",
    "    # Create a grid\n",
    "    ax.set_xlim(-0.5, width-0.5)\n",
    "    ax.set_ylim(-0.5, height-0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, width, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, height, 1))\n",
    "    ax.grid(color='black', linestyle='-', linewidth=1)\n",
    "    \n",
    "    # Define colors\n",
    "    action_colors = {\n",
    "        0: 'gray',      # Stay - Gray\n",
    "        1: 'blue',      # Left - Blue\n",
    "        2: 'black',       # Right - Red\n",
    "        3: 'green',     # Up - Green\n",
    "        4: 'orange'     # Down - Orange\n",
    "    }\n",
    "    \n",
    "    # Create a custom colormap for unsafe areas with transparency\n",
    "    unsafe_cmap = LinearSegmentedColormap.from_list('unsafe', [(1, 0, 0, 0.1), (1, 0, 0, 0.5)], N=10)\n",
    "    \n",
    "    # Add background color to all grid cells\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.add_patch(Rectangle((x-0.5, y-0.5), 1, 1, fill=True, color='white', alpha=0.2))\n",
    "    \n",
    "    # Visualize unsafe set with varying transparency based on count\n",
    "    max_count = 1\n",
    "    all_unsafe_positions = set()\n",
    "    for step, positions_with_counts in unsafe_set_dict.items():\n",
    "        all_unsafe_positions.update(positions_with_counts.keys())\n",
    "        max_count = max(max_count, max(positions_with_counts.values()) if positions_with_counts else 1)\n",
    "    \n",
    "    for pos in all_unsafe_positions:\n",
    "        x, y = pos\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            # Find maximum count for this position across all steps\n",
    "            max_pos_count = max([unsafe_set_dict[step].get(pos, 0) for step in unsafe_set_dict])\n",
    "            alpha = 0.3 + 0.5 * (max_pos_count / max_count)\n",
    "            ax.add_patch(Rectangle((x-0.5, y-0.5), 1, 1, fill=True, color='red', alpha=alpha))\n",
    "            ax.text(x, y, f\"U:{max_pos_count}\", ha='center', va='center', fontsize=9, color='black')\n",
    "    \n",
    "    # Draw arrows for best actions at reachable states\n",
    "    arrow_props = dict(arrowstyle='->', linewidth=2)\n",
    "    for pos in reachable_set:\n",
    "        x, y = pos\n",
    "        if pos in qtable:\n",
    "            action = np.argmax(qtable[pos])\n",
    "            q_value = qtable[pos][action]\n",
    "            \n",
    "            # Calculate arrow direction\n",
    "            dx, dy = 0, 0\n",
    "            if action == 1:  # Left\n",
    "                dx = -0.3\n",
    "            elif action == 2:  # Right\n",
    "                dx = 0.3\n",
    "            elif action == 3:  # Up\n",
    "                dy = -0.3\n",
    "            elif action == 4:  # Down\n",
    "                dy = 0.3\n",
    "            \n",
    "            if action != 0:  # Draw arrow if not \"stay\" action\n",
    "                ax.add_patch(Arrow(x, y, dx, dy, width=0.2, color=action_colors[action]))\n",
    "            else:  # Draw circle for \"stay\" action\n",
    "                ax.add_patch(Circle((x, y), 0.2, color=action_colors[action], alpha=0.7))\n",
    "            \n",
    "            # Add the Q-value text\n",
    "            ax.text(x+0.25, y+0.25, f\"{q_value:.2f}\", ha='center', va='center', fontsize=7,\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # Add current agent position with a distinct marker\n",
    "    agent_pos = env.get_agent_position(idx=agent_idx)\n",
    "    agent_goal = env.get_agent_goal(idx=agent_idx)\n",
    "    \n",
    "    ax.plot(agent_pos[0], agent_pos[1], 'o', markersize=15, color='purple', alpha=0.7)\n",
    "    ax.text(agent_pos[0], agent_pos[1], f\"A{agent_idx}\", ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # Add agent goal with a star marker\n",
    "    ax.plot(agent_goal[0], agent_goal[1], '*', markersize=18, color='green', alpha=0.7)\n",
    "    ax.text(agent_goal[0], agent_goal[1], f\"G{agent_idx}\", ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # Add other agents positions with different markers\n",
    "    if env.is_testing:\n",
    "        num_agents = len(env.envs[0].agents)\n",
    "    else:\n",
    "        num_agents = len(env.envs)\n",
    "    \n",
    "    for other_idx in range(num_agents):\n",
    "        if other_idx != agent_idx:\n",
    "            other_pos = env.get_agent_position(idx=other_idx)\n",
    "            other_goal = env.get_agent_goal(idx=other_idx)\n",
    "            \n",
    "            ax.plot(other_pos[0], other_pos[1], 's', markersize=12, color='blue', alpha=0.7)\n",
    "            ax.text(other_pos[0], other_pos[1], f\"A{other_idx}\", ha='center', va='center', color='white')\n",
    "            \n",
    "            ax.plot(other_goal[0], other_goal[1], 'p', markersize=12, color='darkgreen', alpha=0.7)\n",
    "            ax.text(other_goal[0], other_goal[1], f\"G{other_idx}\", ha='center', va='center', color='white')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        Rectangle((0, 0), 1, 1, color='red', alpha=0.3, label='Unsafe States'),\n",
    "        Circle((0, 0), 0.2, color=action_colors[0], alpha=0.7, label='Stay'),\n",
    "        Arrow(0, 0, 0.3, 0, width=0.2, color=action_colors[2], label='Right'),\n",
    "        Arrow(0, 0, -0.3, 0, width=0.2, color=action_colors[1], label='Left'),\n",
    "        Arrow(0, 0, 0, 0.3, width=0.2, color=action_colors[3], label='Up'),\n",
    "        Arrow(0, 0, 0, -0.3, width=0.2, color=action_colors[4], label='Down'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label=f'Agent {agent_idx}'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', markersize=10, label='Other Agents'),\n",
    "        plt.Line2D([0], [0], marker='*', color='w', markerfacecolor='green', markersize=10, label=f'Goal {agent_idx}'),\n",
    "        plt.Line2D([0], [0], marker='p', color='w', markerfacecolor='darkgreen', markersize=10, label='Other Goals')\n",
    "    ]\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 1.05), \n",
    "              ncol=5, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Flip y-axis to match the environment's coordinates\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    # Set title\n",
    "    if title is None:\n",
    "        title = f\"Agent {agent_idx} - Q-Table Actions and Unsafe States\"\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Show plot if requested\n",
    "    if show:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned path for Agent 1: [(3, 4), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 5), (np.int64(3), np.int64(5)), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Planned path for Agent 3: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 4: [(4, 3), (np.int64(3), np.int64(3)), (np.int64(2), np.int64(3)), (np.int64(1), np.int64(3))]\n",
      "Planned path for Agent 1: [(3, 4), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 6), (np.int64(3), np.int64(6)), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5))]\n",
      "Planned path for Agent 3: [(2, 2), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 4: [(5, 3), (np.int64(4), np.int64(3)), (np.int64(3), np.int64(3)), (np.int64(2), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(4), np.int64(3), np.int64(2)]\n",
      "Planned path for Agent 1: [(2, 4), (np.int64(3), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Planned path for Agent 2: [(3, 6), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Planned path for Agent 3: [(3, 2), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 4: [(6, 3), (np.int64(6), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1), np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(1, 4), (np.int64(1), np.int64(3)), (np.int64(1), np.int64(2)), (np.int64(1), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 6), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(7, 3), (np.int64(6), np.int64(3)), (np.int64(6), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1), np.int64(3), np.int64(2)]\n",
      "Planned path for Agent 1: [(2, 4), (np.int64(3), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Planned path for Agent 2: [(3, 6), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(8, 3), (np.int64(7), np.int64(3)), (np.int64(6), np.int64(3)), (np.int64(6), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2), np.int64(0), np.int64(2)]\n",
      "Planned path for Agent 1: [(3, 4), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 6), (np.int64(3), np.int64(6)), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(8, 4), (np.int64(7), np.int64(4)), (np.int64(6), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2), np.int64(0), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 4), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3)), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 6), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(7, 4), (np.int64(6), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1), np.int64(0), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 4), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 6), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(6, 4), (np.int64(5), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "Planned path for Agent 1: [(2, 4), (np.int64(3), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Planned path for Agent 2: [(1, 6), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4)), (np.int64(1), np.int64(3))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(5, 4), (np.int64(4), np.int64(4)), (np.int64(3), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 4), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4)), (np.int64(5), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 6), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(6, 4), (np.int64(5), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2), np.int64(0), np.int64(2)]\n",
      "Planned path for Agent 1: [(2, 4), (np.int64(3), np.int64(4)), (np.int64(4), np.int64(4)), (np.int64(5), np.int64(4))]\n",
      "Planned path for Agent 2: [(3, 6), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Planned path for Agent 3: [(3, 1), (3, 1)]\n",
      "Planned path for Agent 4: [(6, 5), (np.int64(5), np.int64(5)), (np.int64(4), np.int64(5)), (np.int64(3), np.int64(5))]\n"
     ]
    }
   ],
   "source": [
    "def run_and_record_safety_pipeline(test_env, agents, video_name=\"marl_simulation.mp4\", max_steps=30, \n",
    "                                  k=3, L=200, safety_threshold=-2.0, Qstars=None):\n",
    "    # Create videos directory if it doesn't exist\n",
    "    videos_dir = os.path.expanduser(\"~/Desktop/Projet_MARL/toy_exeemple_normalized\")\n",
    "    os.makedirs(videos_dir, exist_ok=True)\n",
    "    \n",
    "    # Full paths for video files\n",
    "    main_video_path = os.path.join(videos_dir, video_name)\n",
    "    qtable_video_name = \"qtables_\" + video_name\n",
    "    qtable_video_path = os.path.join(videos_dir, qtable_video_name)\n",
    "    \n",
    "    # Reset environment\n",
    "    obs, info = test_env.reset()\n",
    "    \n",
    "    # Initialize frame collections\n",
    "    main_frames = []\n",
    "    \n",
    "    # Create a dictionary to store Q-table visualization frames for each agent\n",
    "    qtable_frames = {i: [] for i in range(len(agents))}\n",
    "    \n",
    "    # Create initial Q-table visualizations for all agents\n",
    "    for agent_idx in range(len(agents)):\n",
    "        reachable_dict, reachable_set = compute_reachable_states(\n",
    "            test_env, agent_idx=agent_idx, n_steps=k\n",
    "        )\n",
    "        unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "            test_env, agent_idx=agent_idx, n_steps=k, exclude_indices=None\n",
    "        )\n",
    "        \n",
    "        fig = visualize_qtable_actions_with_unsafe_set(\n",
    "            env=test_env,\n",
    "            agent_idx=agent_idx,\n",
    "            qtable=agents[agent_idx].q_table,\n",
    "            reachable_set=reachable_set,\n",
    "            unsafe_set_dict=unsafe_set_dict,\n",
    "            show=False,\n",
    "            title=f\"Initial State - Agent {agent_idx+1}\"\n",
    "        )\n",
    "        \n",
    "        canvas = FigureCanvas(fig)\n",
    "        canvas.draw()\n",
    "        frame = np.array(canvas.renderer.buffer_rgba())\n",
    "        qtable_frames[agent_idx].append(frame)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Add initial environment state to frames\n",
    "    if Qstars is None:\n",
    "        Qstar = {}\n",
    "        for i, agent in enumerate(agents):\n",
    "            Qstar[i] = agent.q_table.copy()\n",
    "    else:\n",
    "        Qstar = Qstars\n",
    "        \n",
    "    mask = visualize_all_agents_planned_paths(test_env, agents, k_steps=k, Qstar=Qstar)\n",
    "    grid_img = test_env.visualize(show=False, highlight_masks=mask)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.title(\"Initial State\", fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    canvas = FigureCanvas(plt.gcf())\n",
    "    canvas.draw()\n",
    "    initial_frame = np.array(canvas.renderer.buffer_rgba())\n",
    "    plt.close()\n",
    "    main_frames.append(initial_frame)\n",
    "    \n",
    "    Qsteps = {}\n",
    "    \n",
    "    # Run simulation with safety pipeline\n",
    "    for step in range(max_steps):\n",
    "        Qsteps[step] = {}\n",
    "        \n",
    "        # Run one step of the safety pipeline\n",
    "        actions, safety_status, Qstep, step_figs = safety_aware_pipeline_with_safety_check(test_env, agents, k=k, L=L)\n",
    "        # for i, agent in enumerate(agents):\n",
    "        #     agent.q_table = Qstar[i]\n",
    "        \n",
    "        mask = visualize_all_agents_planned_paths(test_env, agents, k_steps=k, Qstar=Qstar)\n",
    "        Qsteps[step] = Qstep\n",
    "        \n",
    "        # Process the figures for each agent\n",
    "        for agent_idx in range(len(agents)):\n",
    "            if agent_idx in step_figs and not safety_status[agent_idx]:\n",
    "                # The agent needed safety intervention - use the horizon figures\n",
    "                horizon_figs = step_figs[agent_idx]  # This should be a dict {k: fig, k-1: fig, ...}\n",
    "                \n",
    "                # Add each horizon's figure to the video\n",
    "                sorted_horizons = sorted(horizon_figs.keys(), reverse=True)  # From k down to 1\n",
    "                for horizon in sorted_horizons:\n",
    "                    fig = horizon_figs[horizon]\n",
    "                    canvas = FigureCanvas(fig)\n",
    "                    canvas.draw()\n",
    "                    frame = np.array(canvas.renderer.buffer_rgba())\n",
    "                    qtable_frames[agent_idx].append(frame)\n",
    "                    plt.close(fig)\n",
    "            else:\n",
    "                # The agent was already safe - create a new visualization\n",
    "                reachable_dict, reachable_set = compute_reachable_states(\n",
    "                    test_env, agent_idx=agent_idx, n_steps=k\n",
    "                )\n",
    "                unsafe_set_dict = K_agents_n_unsafe_set(\n",
    "                    test_env, agent_idx=agent_idx, n_steps=k, exclude_indices=None\n",
    "                )\n",
    "                \n",
    "                fig = visualize_qtable_actions_with_unsafe_set(\n",
    "                    env=test_env,\n",
    "                    agent_idx=agent_idx,\n",
    "                    qtable=Qstar[agent_idx],\n",
    "                    reachable_set=reachable_set,\n",
    "                    unsafe_set_dict=unsafe_set_dict,\n",
    "                    show=False,\n",
    "                    title=f\"Step {step+1} - Agent {agent_idx+1} (Safe) - Action: {actions[agent_idx]}\"\n",
    "                )\n",
    "                \n",
    "                canvas = FigureCanvas(fig)\n",
    "                canvas.draw()\n",
    "                frame = np.array(canvas.renderer.buffer_rgba())\n",
    "                qtable_frames[agent_idx].append(frame)\n",
    "                plt.close(fig)\n",
    "        \n",
    "        # Create the main environment visualization\n",
    "        grid_img = test_env.visualize(highlight_masks=mask, show=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_img)\n",
    "        plt.title(f\"Step {step+1} | Actions: {actions} | Safety_status: {safety_status}\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        canvas = FigureCanvas(plt.gcf())\n",
    "        canvas.draw()\n",
    "        frame = np.array(canvas.renderer.buffer_rgba())\n",
    "        plt.close()\n",
    "        main_frames.append(frame)\n",
    "        \n",
    "        print(f\"Actions taken: {actions}\")\n",
    "        \n",
    "        # Check if simulation is complete\n",
    "        done = test_env.all_agents_at_goals()\n",
    "        if done:\n",
    "            print(f\"All agents reached their goals in {step+1} steps!\")\n",
    "            \n",
    "            grid_img = test_env.visualize(show=False)\n",
    "            \n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(grid_img)\n",
    "            plt.title(f\"Success! All agents reached goals in {step+1} steps\", \n",
    "                     fontsize=18, color='green')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            canvas = FigureCanvas(plt.gcf())\n",
    "            canvas.draw()\n",
    "            success_frame = np.array(canvas.renderer.buffer_rgba())\n",
    "            plt.close()\n",
    "            main_frames.append(success_frame)\n",
    "            break\n",
    "    \n",
    "    # Create the main video\n",
    "    height, width, _ = main_frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    main_video = cv2.VideoWriter(main_video_path, fourcc, 2, (width, height))\n",
    "    \n",
    "    for frame in main_frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\n",
    "        main_video.write(frame_bgr)\n",
    "    \n",
    "    main_video.release()\n",
    "    print(f\"Main video saved to: {main_video_path}\")\n",
    "    \n",
    "    # Create the Q-table video with agent-by-agent progression\n",
    "    if any(frames for frames in qtable_frames.values()):\n",
    "        sample_frame = next(frame for frames in qtable_frames.values() for frame in frames if len(frames) > 0)\n",
    "        q_height, q_width, _ = sample_frame.shape\n",
    "        \n",
    "        qtable_video = cv2.VideoWriter(qtable_video_path, fourcc, 1, (q_width, q_height))\n",
    "        \n",
    "        # Create title frame\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.text(0.5, 0.5, \"Q-Table Evolution by Agent\", \n",
    "                ha=\"center\", va=\"center\", fontsize=24, fontweight=\"bold\")\n",
    "        plt.axis(\"off\")\n",
    "        canvas = FigureCanvas(plt.gcf())\n",
    "        canvas.draw()\n",
    "        title_frame = np.array(canvas.renderer.buffer_rgba())\n",
    "        title_frame_bgr = cv2.cvtColor(title_frame, cv2.COLOR_RGBA2BGR)\n",
    "        title_frame_resized = cv2.resize(title_frame_bgr, (q_width, q_height))\n",
    "        plt.close()\n",
    "        \n",
    "        # Add title frame to video (2 seconds)\n",
    "        for _ in range(2):\n",
    "            qtable_video.write(title_frame_resized)\n",
    "        \n",
    "        # For each agent, add their Q-table frames\n",
    "        for agent_idx in range(len(agents)):\n",
    "            if len(qtable_frames[agent_idx]) > 0:\n",
    "                # Add agent title slide\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                plt.text(0.5, 0.5, f\"Agent {agent_idx+1} Q-Table Evolution\", \n",
    "                        ha=\"center\", va=\"center\", fontsize=24, fontweight=\"bold\")\n",
    "                plt.axis(\"off\")\n",
    "                canvas = FigureCanvas(plt.gcf())\n",
    "                canvas.draw()\n",
    "                agent_title = np.array(canvas.renderer.buffer_rgba())\n",
    "                agent_title_bgr = cv2.cvtColor(agent_title, cv2.COLOR_RGBA2BGR)\n",
    "                agent_title_resized = cv2.resize(agent_title_bgr, (q_width, q_height))\n",
    "                plt.close()\n",
    "                \n",
    "                # Add agent title to video (2 seconds)\n",
    "                for _ in range(2):\n",
    "                    qtable_video.write(agent_title_resized)\n",
    "                \n",
    "                # Add all frames for this agent\n",
    "                for frame in qtable_frames[agent_idx]:\n",
    "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\n",
    "                    qtable_video.write(frame_bgr)\n",
    "                \n",
    "                # Add a separator frame if not the last agent\n",
    "                if agent_idx < len(agents) - 1:\n",
    "                    separator = np.ones((q_height, q_width, 3), dtype=np.uint8) * 255\n",
    "                    qtable_video.write(separator)\n",
    "        \n",
    "        qtable_video.release()\n",
    "        print(f\"Q-table video saved to: {qtable_video_path}\")\n",
    "    \n",
    "    return main_video_path, qtable_video_path, Qsteps\n",
    "\n",
    "main_video_path,qtable_video_path,Qsteps=run_and_record_safety_pipeline(\n",
    "    test_env, agents, video_name=\"marl_simulation4.mp4\", max_steps=30, \n",
    "    k=3, L=500, safety_threshold=-2.0, Qstars=Qstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_with_qtable(qtable, state, action_dim=5):\n",
    "    \"\"\"\n",
    "    Select the best action based on the provided Q-table.\n",
    "    \n",
    "    Args:\n",
    "        qtable: The Q-table dictionary\n",
    "        state: The current state (position)\n",
    "        action_dim: Number of possible actions\n",
    "        \n",
    "    Returns:\n",
    "        int: The best action according to the Q-table\n",
    "    \"\"\"\n",
    "    state_key = tuple(state) if isinstance(state, np.ndarray) else state\n",
    "    \n",
    "    if state_key in qtable:\n",
    "        return np.argmax(qtable[state_key])\n",
    "    else:\n",
    "        # If state not in Q-table, return default action\n",
    "        return 0  # Default: stay in place\n",
    "\n",
    "\n",
    "\n",
    "def visualize_agent_planned_path_qtable(env, agent_idx, qtable, k_steps=5, fixed_color=None):\n",
    "    \"\"\"\n",
    "    Creates a highlight mask showing the planned path for an agent over k steps using Q-table.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment wrapper\n",
    "        agent_idx: Index of the agent\n",
    "        qtable: Q-table to use for action selection\n",
    "        k_steps: Number of steps to look ahead\n",
    "        fixed_color: Optional color index to use for path visualization\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A highlight mask showing the planned path\n",
    "    \"\"\"\n",
    "    # Create a 2D array of lists for the highlight mask\n",
    "    mask = np.empty((env.width, env.height), dtype=object)\n",
    "    for i in range(env.width):\n",
    "        for j in range(env.height):\n",
    "            mask[i, j] = []\n",
    "    \n",
    "    # Get the agent's current position\n",
    "    current_pos = env.get_agent_position(idx=agent_idx)\n",
    "    \n",
    "    # Get reference to the world object for colors\n",
    "    env_world = env.envs[0].world\n",
    "    color_count = len(env_world.IDX_TO_COLOR)\n",
    "    \n",
    "    # Add the current position to the path with step 0\n",
    "    x, y = current_pos\n",
    "    step_color = 0 if fixed_color is None else fixed_color\n",
    "    mask[x, y].append(step_color)\n",
    "    \n",
    "    # Simulate the agent's path for k steps\n",
    "    simulated_pos = current_pos\n",
    "    path = [current_pos]\n",
    "    \n",
    "    for step in range(k_steps):\n",
    "        # Select action according to the provided Q-table\n",
    "        action = select_action_with_qtable(qtable, simulated_pos)\n",
    "        \n",
    "        # Simulate the action\n",
    "        next_pos = model_state(simulated_pos, action)\n",
    "        \n",
    "        # Check if next position is within bounds\n",
    "        x, y = next_pos\n",
    "        if 0 <= x < env.width and 0 <= y < env.height:\n",
    "            step_color = (step+1) % color_count if fixed_color is None else fixed_color\n",
    "            mask[x, y].append(step_color)\n",
    "            \n",
    "            # Update for next iteration\n",
    "            simulated_pos = next_pos\n",
    "            path.append(next_pos)\n",
    "        else:\n",
    "            # Position out of bounds, stop simulating\n",
    "            print(f\"Agent {agent_idx+1} would go out of bounds at step {step+1}.\")\n",
    "            break\n",
    "        \n",
    "        # Break if the agent reaches its goal\n",
    "        if next_pos == env.get_agent_goal(idx=agent_idx):\n",
    "            break\n",
    "    \n",
    "    print(f\"Planned path for Agent {agent_idx+1}: {path}\")\n",
    "    return mask\n",
    "\n",
    "def visualize_all_agents_planned_paths(env, agents, k_steps=5, Qstar=None):\n",
    "    \"\"\"\n",
    "    Creates a merged highlight mask showing the planned paths for all agents.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment wrapper\n",
    "        agents: List of agent objects with Q-tables\n",
    "        k_steps: Number of steps to look ahead\n",
    "        Qstar: Optional list of Q-tables to use instead of agents' Q-tables\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A highlight mask showing all planned paths\n",
    "    \"\"\"\n",
    "    # Create a 2D array of lists for the combined highlight mask\n",
    "    combined_mask = np.empty((env.width, env.height), dtype=object)\n",
    "    for i in range(env.width):\n",
    "        for j in range(env.height):\n",
    "            combined_mask[i, j] = []\n",
    "    \n",
    "    # Generate path for each agent and add to the combined mask\n",
    "    for agent_idx in range(len(agents)):\n",
    "        # Use visualize_agent_planned_path_qtable for each agent\n",
    "        if Qstar is not None and agent_idx < len(Qstar) and Qstar[agent_idx] is not None:\n",
    "            qtable = Qstar[agent_idx]\n",
    "        else:\n",
    "            qtable = agents[agent_idx].q_table\n",
    "                    \n",
    "        # Get agent's mask with a fixed color (the agent's index)\n",
    "        agent_mask = visualize_agent_planned_path_qtable(\n",
    "            env, \n",
    "            agent_idx, \n",
    "            qtable, \n",
    "            k_steps=k_steps, \n",
    "            fixed_color=agent_idx % len(env.envs[0].world.IDX_TO_COLOR)\n",
    "        )\n",
    "        \n",
    "        # Merge the agent's mask with the combined mask\n",
    "        for i in range(env.width):\n",
    "            for j in range(env.height):\n",
    "                if agent_mask[i, j]:\n",
    "                    combined_mask[i, j].extend(agent_mask[i, j])\n",
    "    \n",
    "    return combined_mask\n",
    "def flatten_qsteps(qsteps, k_value=None, L_value=None):\n",
    "    \"\"\"\n",
    "    Aplatit la structure hiérarchique de Qsteps en lignes pour DataFrame,\n",
    "    en déterminant automatiquement quelle position est le centre (agent) \n",
    "    et quelles positions sont up, down, left, right.\n",
    "    \n",
    "    Args:\n",
    "        qsteps: Le dictionnaire hiérarchique à aplatir\n",
    "        k_value: La valeur de k (horizon maximum) utilisée\n",
    "        L_value: La valeur de L (itérations) utilisée\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste de dictionnaires, chaque dictionnaire représentant une ligne\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for step, agents_dict in qsteps.items():\n",
    "        if not agents_dict:  # Skip empty dictionaries\n",
    "            continue\n",
    "            \n",
    "        for agent_idx, horizons_dict in agents_dict.items():\n",
    "            if not horizons_dict:  # Skip empty dictionaries\n",
    "                continue\n",
    "                \n",
    "            # Trier les horizons dans l'ordre croissant\n",
    "            sorted_horizons = sorted(horizons_dict.keys(), reverse=False)\n",
    "            \n",
    "            for horizon in sorted_horizons:\n",
    "                qtable = horizons_dict[horizon]\n",
    "                if not qtable or len(qtable) == 0:  # Skip empty dictionaries\n",
    "                    continue\n",
    "                \n",
    "                # Convertir tous les états en tuples d'entiers simples\n",
    "                states = []\n",
    "                for state in qtable.keys():\n",
    "                    clean_state = tuple(int(x) for x in state)\n",
    "                    states.append(clean_state)\n",
    "                \n",
    "                if len(states) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Trouver la position de l'agent (centre) et les positions adjacentes\n",
    "                # On suppose que le centre est la position qui a des voisins dans les 4 directions\n",
    "                positions = {'center': None, 'up': None, 'down': None, 'left': None, 'right': None}\n",
    "                \n",
    "                # Si nous avons exactement 5 états, nous pouvons déterminer directement le centre\n",
    "                if len(states) == 5:\n",
    "                    # Pour chaque état potentiel au centre\n",
    "                    for state in states:\n",
    "                        # Vérifier si les 4 directions adjacentes sont présentes\n",
    "                        up = (state[0], state[1] - 1)\n",
    "                        down = (state[0], state[1] + 1)\n",
    "                        left = (state[0] - 1, state[1])\n",
    "                        right = (state[0] + 1, state[1])\n",
    "                        \n",
    "                        if up in states and down in states and left in states and right in states:\n",
    "                            positions['center'] = state\n",
    "                            positions['up'] = up\n",
    "                            positions['down'] = down\n",
    "                            positions['left'] = left\n",
    "                            positions['right'] = right\n",
    "                            break\n",
    "                \n",
    "                # Si nous n'avons pas trouvé le centre avec la méthode précédente\n",
    "                # Cela peut arriver si certains états sont bloqués par des murs\n",
    "                if positions['center'] is None and len(states) > 0:\n",
    "                    # Alternative: trouver le \"centre\" comme étant l'état avec le plus de voisins\n",
    "                    neighbor_counts = {}\n",
    "                    for state in states:\n",
    "                        neighbors = [\n",
    "                            (state[0], state[1] - 1),  # up\n",
    "                            (state[0], state[1] + 1),  # down\n",
    "                            (state[0] - 1, state[1]),  # left\n",
    "                            (state[0] + 1, state[1])   # right\n",
    "                        ]\n",
    "                        count = sum(1 for n in neighbors if n in states)\n",
    "                        neighbor_counts[state] = count\n",
    "                    \n",
    "                    # L'état avec le plus de voisins est probablement le centre\n",
    "                    center = max(neighbor_counts.items(), key=lambda x: x[1])[0]\n",
    "                    positions['center'] = center\n",
    "                    \n",
    "                    # Déterminer les positions adjacentes présentes\n",
    "                    up = (center[0], center[1] - 1)\n",
    "                    down = (center[0], center[1] + 1)\n",
    "                    left = (center[0] - 1, center[1])\n",
    "                    right = (center[0] + 1, center[1])\n",
    "                    \n",
    "                    positions['up'] = up if up in states else None\n",
    "                    positions['down'] = down if down in states else None\n",
    "                    positions['left'] = left if left in states else None\n",
    "                    positions['right'] = right if right in states else None\n",
    "                \n",
    "                # Créer une ligne pour cette configuration\n",
    "                row = {\n",
    "                    'k': k_value,\n",
    "                    'L': L_value,\n",
    "                    'Step': step,\n",
    "                    'Agent': agent_idx,\n",
    "                    'Horizon': \"original qtable\" if horizon == k_value + 1 else horizon,\n",
    "                }\n",
    "                \n",
    "                # Ajouter le s états et leurs valeurs Q\n",
    "                for pos_name, pos in positions.items():\n",
    "                    if pos is not None:\n",
    "                        row[f'{pos_name}_Position'] = pos\n",
    "                        q_values = qtable[pos]\n",
    "                        row[f'{pos_name}_Q'] = q_values\n",
    "                        row[f'{pos_name}_BestAction'] = np.argmax(q_values)\n",
    "                \n",
    "                rows.append(row)\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Running with k=1, L=1 =====\n",
      "Running simulation with k=1, L=1...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L1.mp4\n",
      "CSV saved for k=1, L=1\n",
      "\n",
      "\n",
      "===== Running with k=1, L=5 =====\n",
      "Running simulation with k=1, L=5...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(5, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(3)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(4), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L5.mp4\n",
      "CSV saved for k=1, L=5\n",
      "\n",
      "\n",
      "===== Running with k=1, L=10 =====\n",
      "Running simulation with k=1, L=10...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L10.mp4\n",
      "CSV saved for k=1, L=10\n",
      "\n",
      "\n",
      "===== Running with k=1, L=15 =====\n",
      "Running simulation with k=1, L=15...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L15.mp4\n",
      "CSV saved for k=1, L=15\n",
      "\n",
      "\n",
      "===== Running with k=1, L=20 =====\n",
      "Running simulation with k=1, L=20...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L20.mp4\n",
      "CSV saved for k=1, L=20\n",
      "\n",
      "\n",
      "===== Running with k=1, L=50 =====\n",
      "Running simulation with k=1, L=50...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L50.mp4\n",
      "CSV saved for k=1, L=50\n",
      "\n",
      "\n",
      "===== Running with k=1, L=100 =====\n",
      "Running simulation with k=1, L=100...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(4), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L100.mp4\n",
      "CSV saved for k=1, L=100\n",
      "\n",
      "\n",
      "===== Running with k=1, L=200 =====\n",
      "Running simulation with k=1, L=200...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(3), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(3, 4), (np.int64(4), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 4), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(3)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(3), np.int64(2))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(4), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k1_L200.mp4\n",
      "CSV saved for k=1, L=200\n",
      "\n",
      "\n",
      "===== Running with k=2, L=1 =====\n",
      "Running simulation with k=2, L=1...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2)), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2)), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(3), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L1.mp4\n",
      "CSV saved for k=2, L=1\n",
      "\n",
      "\n",
      "===== Running with k=2, L=5 =====\n",
      "Running simulation with k=2, L=5...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2)), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2)), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(3), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 1), (np.int64(4), np.int64(1)), (np.int64(3), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(3), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L5.mp4\n",
      "CSV saved for k=2, L=5\n",
      "\n",
      "\n",
      "===== Running with k=2, L=10 =====\n",
      "Running simulation with k=2, L=10...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2)), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2)), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(4)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(3), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(2, 4), (np.int64(1), np.int64(4)), (np.int64(2), np.int64(4))]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "Planned path for Agent 1: [(4, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(2), np.int64(4)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(1)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L10.mp4\n",
      "CSV saved for k=2, L=10\n",
      "\n",
      "\n",
      "===== Running with k=2, L=15 =====\n",
      "Running simulation with k=2, L=15...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(5), np.int64(2)), (np.int64(6), np.int64(2))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(5, 2), (np.int64(6), np.int64(2)), (np.int64(6), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(6, 2), (np.int64(6), np.int64(1)), (np.int64(5), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(0)]\n",
      "Planned path for Agent 1: [(6, 1), (np.int64(5), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(3), np.int64(0)]\n",
      "Planned path for Agent 1: [(5, 1), (np.int64(4), np.int64(1)), (np.int64(4), np.int64(1))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(4, 1), (4, 1), (4, 1)]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L15.mp4\n",
      "CSV saved for k=2, L=15\n",
      "\n",
      "\n",
      "===== Running with k=2, L=20 =====\n",
      "Running simulation with k=2, L=20...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(1), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 3), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(1, 3), (1, 3), (1, 3)]\n",
      "Actions taken: [np.int64(4), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(5))]\n",
      "Actions taken: [np.int64(4), np.int64(4)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(4)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 5), (1, 5), (1, 5)]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L20.mp4\n",
      "CSV saved for k=2, L=20\n",
      "\n",
      "\n",
      "===== Running with k=2, L=50 =====\n",
      "Running simulation with k=2, L=50...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(1), np.int64(4)), (np.int64(1), np.int64(5))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(1), np.int64(4)]\n",
      "Planned path for Agent 1: [(3, 3), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(1, 5), (np.int64(1), np.int64(4)), (np.int64(1), np.int64(5))]\n",
      "Actions taken: [np.int64(4), np.int64(4)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 4), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(4))]\n",
      "Actions taken: [np.int64(4), np.int64(3)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(2), np.int64(3)), (np.int64(3), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(3)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(2)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(0), np.int64(0)]\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L50.mp4\n",
      "CSV saved for k=2, L=50\n",
      "\n",
      "\n",
      "===== Running with k=2, L=100 =====\n",
      "Running simulation with k=2, L=100...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(4), np.int64(3)), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(2), np.int64(3)), (np.int64(3), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(4, 3), (np.int64(4), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(4), np.int64(2)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 4), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(4), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(3, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 4), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(4, 2), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(3)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(4)]\n",
      "All agents reached their goals in 7 steps!\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L100.mp4\n",
      "CSV saved for k=2, L=100\n",
      "\n",
      "\n",
      "===== Running with k=2, L=200 =====\n",
      "Running simulation with k=2, L=200...\n",
      "Planned path for Agent 1: [(3, 2), (np.int64(3), np.int64(3)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 2), (np.int64(3), np.int64(2)), (np.int64(3), np.int64(3))]\n",
      "Planned path for Agent 2: [(1, 3), (np.int64(2), np.int64(3)), (np.int64(3), np.int64(3))]\n",
      "Actions taken: [np.int64(2), np.int64(1)]\n",
      "Planned path for Agent 1: [(4, 3), (np.int64(4), np.int64(4)), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(4), np.int64(2)]\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 4), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(2, 3), (np.int64(3), np.int64(3)), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(4), np.int64(0)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(3, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "reward 100\n",
      "Planned path for Agent 1: [(4, 4), (np.int64(3), np.int64(4))]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(2), np.int64(2)]\n",
      "reward -100\n",
      "reward 100\n",
      "reward 100\n",
      "reward 100\n",
      "reward -100\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(5, 3), (np.int64(4), np.int64(3))]\n",
      "Actions taken: [np.int64(1), np.int64(2)]\n",
      "Planned path for Agent 1: [(3, 4), (3, 4)]\n",
      "Planned path for Agent 2: [(4, 3), (4, 3)]\n",
      "Actions taken: [np.int64(0), np.int64(1)]\n",
      "All agents reached their goals in 7 steps!\n",
      "Video saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/easy_ex_k2_L200.mp4\n",
      "CSV saved for k=2, L=200\n",
      "\n",
      "Combined CSV with all parameters saved to: /home/yacine/Desktop/Projet_MARL/toy_exeemple_normalized/all_parameters_qsteps.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from itertools import product\n",
    "def flatten_qsteps(qsteps, k_value=None, L_value=None):\n",
    "    \"\"\"\n",
    "    Aplatit la structure hiérarchique de Qsteps en lignes pour DataFrame,\n",
    "    en déterminant automatiquement quelle position est le centre (agent) \n",
    "    et quelles positions sont up, down, left, right.\n",
    "    \n",
    "    Args:\n",
    "        qsteps: Le dictionnaire hiérarchique à aplatir\n",
    "        k_value: La valeur de k (horizon maximum) utilisée\n",
    "        L_value: La valeur de L (itérations) utilisée\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste de dictionnaires, chaque dictionnaire représentant une ligne\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for step, agents_dict in qsteps.items():\n",
    "        if not agents_dict:  # Skip empty dictionaries\n",
    "            continue\n",
    "            \n",
    "        for agent_idx, horizons_dict in agents_dict.items():\n",
    "            if not horizons_dict:  # Skip empty dictionaries\n",
    "                continue\n",
    "                \n",
    "            # Trier les horizons dans l'ordre croissant\n",
    "            sorted_horizons = sorted(horizons_dict.keys(), reverse=False)\n",
    "            \n",
    "            for horizon in sorted_horizons:\n",
    "                qtable = horizons_dict[horizon]\n",
    "                if not qtable or len(qtable) == 0:  # Skip empty dictionaries\n",
    "                    continue\n",
    "                \n",
    "                # Convertir tous les états en tuples d'entiers simples\n",
    "                states = []\n",
    "                for state in qtable.keys():\n",
    "                    clean_state = tuple(int(x) for x in state)\n",
    "                    states.append(clean_state)\n",
    "                \n",
    "                if len(states) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Trouver la position de l'agent (centre) et les positions adjacentes\n",
    "                # On suppose que le centre est la position qui a des voisins dans les 4 directions\n",
    "                positions = {'center': None, 'up': None, 'down': None, 'left': None, 'right': None}\n",
    "                \n",
    "                # Si nous avons exactement 5 états, nous pouvons déterminer directement le centre\n",
    "                if len(states) == 5:\n",
    "                    # Pour chaque état potentiel au centre\n",
    "                    for state in states:\n",
    "                        # Vérifier si les 4 directions adjacentes sont présentes\n",
    "                        up = (state[0], state[1] - 1)\n",
    "                        down = (state[0], state[1] + 1)\n",
    "                        left = (state[0] - 1, state[1])\n",
    "                        right = (state[0] + 1, state[1])\n",
    "                        \n",
    "                        if up in states and down in states and left in states and right in states:\n",
    "                            positions['center'] = state\n",
    "                            positions['up'] = up\n",
    "                            positions['down'] = down\n",
    "                            positions['left'] = left\n",
    "                            positions['right'] = right\n",
    "                            break\n",
    "                \n",
    "                # Si nous n'avons pas trouvé le centre avec la méthode précédente\n",
    "                # Cela peut arriver si certains états sont bloqués par des murs\n",
    "                if positions['center'] is None and len(states) > 0:\n",
    "                    # Alternative: trouver le \"centre\" comme étant l'état avec le plus de voisins\n",
    "                    neighbor_counts = {}\n",
    "                    for state in states:\n",
    "                        neighbors = [\n",
    "                            (state[0], state[1] - 1),  # up\n",
    "                            (state[0], state[1] + 1),  # down\n",
    "                            (state[0] - 1, state[1]),  # left\n",
    "                            (state[0] + 1, state[1])   # right\n",
    "                        ]\n",
    "                        count = sum(1 for n in neighbors if n in states)\n",
    "                        neighbor_counts[state] = count\n",
    "                    \n",
    "                    # L'état avec le plus de voisins est probablement le centre\n",
    "                    center = max(neighbor_counts.items(), key=lambda x: x[1])[0]\n",
    "                    positions['center'] = center\n",
    "                    \n",
    "                    # Déterminer les positions adjacentes présentes\n",
    "                    up = (center[0], center[1] - 1)\n",
    "                    down = (center[0], center[1] + 1)\n",
    "                    left = (center[0] - 1, center[1])\n",
    "                    right = (center[0] + 1, center[1])\n",
    "                    \n",
    "                    positions['up'] = up if up in states else None\n",
    "                    positions['down'] = down if down in states else None\n",
    "                    positions['left'] = left if left in states else None\n",
    "                    positions['right'] = right if right in states else None\n",
    "                \n",
    "                # Créer une ligne pour cette configuration\n",
    "                row = {\n",
    "                    'k': k_value,\n",
    "                    'L': L_value,\n",
    "                    'Step': step,\n",
    "                    'Agent': agent_idx,\n",
    "                    'Horizon': \"original qtable\" if horizon == k_value + 1 else horizon,\n",
    "                }\n",
    "                \n",
    "                # Ajouter le s états et leurs valeurs Q\n",
    "                for pos_name, pos in positions.items():\n",
    "                    if pos is not None:\n",
    "                        row[f'{pos_name}_Position'] = pos\n",
    "                        q_values = qtable[pos]\n",
    "                        row[f'{pos_name}_Q'] = q_values\n",
    "                        row[f'{pos_name}_BestAction'] = np.argmax(q_values)\n",
    "                \n",
    "                rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "# Définir les plages de valeurs pour L et k\n",
    "L_values = [1,5,10 ,15,20,50,100,200]\n",
    "k_values = [1,2]\n",
    "\n",
    "# Créer un dossier pour les résultats si nécessaire\n",
    "results_dir = os.path.expanduser(\"~/Desktop/Projet_MARL/toy_exeemple_normalized\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Liste pour collecter toutes les données pour CSV\n",
    "all_rows = []\n",
    "\n",
    "# Exécuter la simulation pour chaque combinaison de paramètres\n",
    "for k, L in product(k_values, L_values):\n",
    "    print(f\"\\n\\n===== Running with k={k}, L={L} =====\")\n",
    "    \n",
    "    # Créer un nom de fichier vidéo basé sur les paramètres\n",
    "    video_name = f\"easy_ex_k{k}_L{L}.mp4\"\n",
    "    \n",
    "    # Réinitialiser l'environnement de test et les agents pour une nouvelle exécution\n",
    "    test_env = MultiAgentEnvWrapper(\n",
    "        is_testing=True,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        agent_positions=agent_positions,\n",
    "        goal_positions=goal_positions,\n",
    "        walls=walls\n",
    "    )\n",
    "    \n",
    "    # Mettre à jour les agents avec l'environnement de test réinitialisé\n",
    "    for i, agent in enumerate(agents):\n",
    "        agent.env_wrapper = test_env\n",
    "        # Restaurer la Q-table originale pour chaque agent\n",
    "        agent.q_table = Qstar[i].copy()\n",
    "    \n",
    "    # Lancer la simulation avec cette combinaison de paramètres\n",
    "    print(f\"Running simulation with k={k}, L={L}...\")\n",
    "    video_path, Qsteps = run_and_record_safety_pipeline(\n",
    "        test_env, agents, \n",
    "        video_name=video_name, \n",
    "        max_steps=40,\n",
    "        k=k, \n",
    "        L=L, \n",
    "        safety_threshold=-2.0, \n",
    "        Qstars=Qstar\n",
    "    )\n",
    "    \n",
    "    # Aplatir les données de Qsteps avec les valeurs de k et L\n",
    "    flattened_data = flatten_qsteps(Qsteps, k_value=k, L_value=L)\n",
    "    \n",
    "    # Ajouter les données à la liste complète\n",
    "    all_rows.extend(flattened_data)\n",
    "    \n",
    "    # Sauvegarder les données pour cette combinaison de paramètres\n",
    "    df_current = pd.DataFrame(flattened_data)\n",
    "    if not df_current.empty:\n",
    "        df_current.to_csv(os.path.join(results_dir, f\"qsteps_k{k}_L{L}.csv\"), index=False)\n",
    "        print(f\"CSV saved for k={k}, L={L}\")\n",
    "\n",
    "# Créer un DataFrame avec toutes les données\n",
    "df_all = pd.DataFrame(all_rows)\n",
    "\n",
    "# Sauvegarder le CSV combiné\n",
    "combined_csv_path = os.path.join(results_dir, \"all_parameters_qsteps.csv\")\n",
    "df_all.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nCombined CSV with all parameters saved to: {combined_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
